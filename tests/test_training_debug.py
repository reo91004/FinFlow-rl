# tests/debug_training.py - í•™ìŠµ ì‹ í˜¸ ë””ë²„ê¹… (Loss, Gradient, TD-error)

import os
import sys
import warnings

warnings.filterwarnings("ignore")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

from config import *
from agents.bcell import BCell
from data import DataLoader
from core.environment import PortfolioEnvironment
from core.system import ImmunePortfolioSystem
from data.features import FeatureExtractor


def debug_training_signals():
    """í•™ìŠµ ì‹ í˜¸ (Loss, Gradient, TD-error) ë””ë²„ê¹…"""

    print("=" * 80)
    print("í•™ìŠµ ì‹ í˜¸ ë””ë²„ê¹… ì‹œì‘")
    print("=" * 80)

    # 1. í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì„±
    print("[1] í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì„±...")

    # ì‘ì€ ë°ì´í„°ì…‹ ìƒì„±
    dates = pd.date_range("2020-01-01", periods=50, freq="D")
    symbols = SYMBOLS[:5]
    np.random.seed(42)
    returns = np.random.normal(0.001, 0.02, (50, 5))
    prices = pd.DataFrame(
        np.cumprod(1 + returns, axis=0) * 100, index=dates, columns=symbols
    )

    # í™˜ê²½ ë° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    feature_extractor = FeatureExtractor(lookback_window=20)
    env = PortfolioEnvironment(prices, feature_extractor, initial_capital=100000)
    immune_system = ImmunePortfolioSystem(n_assets=len(symbols), state_dim=18)

    # ë”ë¯¸ í•™ìŠµ ë°ì´í„°ë¡œ T-Cell í›ˆë ¨
    dummy_features = np.random.randn(100, 12)
    immune_system.fit_tcell(dummy_features)

    print(f"í™˜ê²½ ì„¤ì • ì™„ë£Œ: {len(symbols)}ê°œ ìì‚°, {env.max_steps} ìŠ¤í…")

    # 2. ì§§ì€ ì—í”¼ì†Œë“œ ì‹¤í–‰í•˜ì—¬ ê²½í—˜ ìˆ˜ì§‘
    print(f"\n[2] ê²½í—˜ ìˆ˜ì§‘ (20 ìŠ¤í…)...")

    state = env.reset()
    experiences = []

    for step in range(20):
        # ì˜ì‚¬ê²°ì •
        weights, decision_info = immune_system.decide(state, training=True)

        # í™˜ê²½ ìŠ¤í…
        next_state, reward, done, info = env.step(weights)

        # ê²½í—˜ ì €ì¥
        experiences.append(
            {
                "state": state.copy(),
                "action": weights.copy(),
                "reward": reward,
                "next_state": next_state.copy(),
                "done": done,
                "step": step,
            }
        )

        # ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ (ê²½í—˜ ì €ì¥)
        immune_system.update(state, weights, reward, next_state, done)

        print(
            f"  ìŠ¤í… {step+1:2d}: ë³´ìƒ={reward:+7.4f}, ì„ íƒ={decision_info['selected_bcell'][:4]}"
        )

        state = next_state
        if done:
            break

    print(f"ê²½í—˜ ìˆ˜ì§‘ ì™„ë£Œ: {len(experiences)} ìŠ¤í…")

    # 3. B-Cellë³„ í•™ìŠµ ì‹ í˜¸ ë¶„ì„
    print(f"\n[3] B-Cell í•™ìŠµ ì‹ í˜¸ ë¶„ì„...")

    training_stats = {}

    for bcell_name, bcell in immune_system.bcells.items():
        print(f"\n  === {bcell_name} B-Cell ===")

        # ì¶©ë¶„í•œ ê²½í—˜ì´ ìˆëŠ”ì§€ í™•ì¸
        buffer_size = len(bcell.replay_buffer)
        print(f"    ë²„í¼ í¬ê¸°: {buffer_size}")

        if buffer_size < bcell.batch_size:
            print(f"    âš ï¸  ê²½í—˜ ë¶€ì¡± (í•„ìš”: {bcell.batch_size}, ë³´ìœ : {buffer_size})")
            continue

        # í•™ìŠµ ì „ ìƒíƒœ ê¸°ë¡
        pre_losses = {
            "actor": bcell.actor_losses[-5:] if bcell.actor_losses else [],
            "critic": bcell.critic_losses[-5:] if bcell.critic_losses else [],
        }

        # í•™ìŠµ ì‹¤í–‰ (5íšŒ)
        print(f"    5íšŒ ì—…ë°ì´íŠ¸ ì‹¤í–‰...")
        for update_i in range(5):
            bcell.update()

        # í•™ìŠµ í›„ ìƒíƒœ ê¸°ë¡
        post_losses = {
            "actor": bcell.actor_losses[-5:],
            "critic": bcell.critic_losses[-5:],
        }

        # í†µê³„ ê³„ì‚°
        if post_losses["actor"] and post_losses["critic"]:
            avg_actor_loss = np.mean(post_losses["actor"])
            avg_critic_loss = np.mean(post_losses["critic"])

            print(f"    í‰ê·  Actor Loss: {avg_actor_loss:.6f}")
            print(f"    í‰ê·  Critic Loss: {avg_critic_loss:.6f}")
            print(f"    Alpha ê°’: {bcell.alpha.item():.4f}")
            print(f"    ì—…ë°ì´íŠ¸ íšŸìˆ˜: {bcell.update_count}")

            # ì†ì‹¤ ë¶„ì„
            if avg_actor_loss > 1000:
                print(f"    âš ï¸  Actor ì†ì‹¤ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!")
            elif avg_actor_loss < 1e-6:
                print(f"    âš ï¸  Actor ì†ì‹¤ì´ ë„ˆë¬´ ë‚®ì•„ í•™ìŠµì´ ì•ˆ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            else:
                print(f"    âœ… Actor ì†ì‹¤ì´ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤.")

            if avg_critic_loss > 1000:
                print(f"    âš ï¸  Critic ì†ì‹¤ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!")
            elif np.isnan(avg_critic_loss):
                print(f"    âŒ Critic ì†ì‹¤ì— NaNì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤!")
            else:
                print(f"    âœ… Critic ì†ì‹¤ì´ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤.")

            training_stats[bcell_name] = {
                "avg_actor_loss": avg_actor_loss,
                "avg_critic_loss": avg_critic_loss,
                "alpha": bcell.alpha.item(),
                "update_count": bcell.update_count,
            }
        else:
            print(f"    âš ï¸  ì†ì‹¤ ë°ì´í„° ì—†ìŒ")

    # 4. ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„
    print(f"\n[4] ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„...")

    # í•˜ë‚˜ì˜ B-Cellì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ìƒì„¸ ë¶„ì„
    test_bcell = immune_system.bcells["volatility"]

    if len(test_bcell.replay_buffer) >= test_bcell.batch_size:
        print(f"  Volatility B-Cell ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì„...")

        # ë°°ì¹˜ ìƒ˜í”Œë§
        batch, is_weights, indices = test_bcell.replay_buffer.sample(
            test_bcell.batch_size
        )
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.tensor(np.array(states, dtype=np.float32)).to(DEVICE)
        actions = torch.tensor(np.array(actions, dtype=np.float32)).to(DEVICE)
        rewards = torch.tensor([float(r) for r in rewards]).to(DEVICE)
        next_states = torch.tensor(np.array(next_states, dtype=np.float32)).to(DEVICE)
        dones = torch.tensor([bool(d) for d in dones]).to(DEVICE)

        # Forward pass
        _, current_actions, current_log_probs = test_bcell.actor(states)
        q1_current = test_bcell.critic1(states, current_actions).squeeze()
        q2_current = test_bcell.critic2(states, current_actions).squeeze()

        # Actor ì†ì‹¤ ê³„ì‚°
        q_current = torch.min(q1_current, q2_current)
        actor_loss = (test_bcell.alpha * current_log_probs - q_current).mean()

        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        test_bcell.actor_optimizer.zero_grad()
        actor_loss.backward(retain_graph=True)

        # ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„
        actor_grad_norms = []
        for param in test_bcell.actor.parameters():
            if param.grad is not None:
                grad_norm = param.grad.data.norm().item()
                actor_grad_norms.append(grad_norm)

        if actor_grad_norms:
            avg_grad_norm = np.mean(actor_grad_norms)
            max_grad_norm = np.max(actor_grad_norms)

            print(f"    Actor ê·¸ë˜ë””ì–¸íŠ¸:")
            print(f"      í‰ê·  norm: {avg_grad_norm:.6f}")
            print(f"      ìµœëŒ€ norm: {max_grad_norm:.6f}")
            print(f"      ë ˆì´ì–´ ìˆ˜: {len(actor_grad_norms)}")

            if avg_grad_norm < 1e-6:
                print(f"      âš ï¸  ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ (Vanishing)!")
            elif avg_grad_norm > 10:
                print(f"      âš ï¸  ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ (Exploding)!")
            else:
                print(f"      âœ… ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì ì ˆí•©ë‹ˆë‹¤.")

    # 5. TD-error ë¶„í¬ ë¶„ì„
    print(f"\n[5] TD-error ë¶„í¬ ë¶„ì„...")

    for bcell_name in ["volatility", "correlation"]:
        bcell = immune_system.bcells[bcell_name]

        if (
            hasattr(bcell, "monitoring_stats")
            and bcell.monitoring_stats["td_error_stats"]["mean"]
        ):
            td_means = bcell.monitoring_stats["td_error_stats"]["mean"]
            td_maxs = bcell.monitoring_stats["td_error_stats"]["max"]

            print(f"  {bcell_name} B-Cell TD-error:")
            print(f"    í‰ê·  TD-error: {np.mean(td_means):.6f}")
            print(f"    ìµœëŒ€ TD-error: {np.mean(td_maxs):.6f}")
            print(f"    TD-error ê¸°ë¡ ìˆ˜: {len(td_means)}")

            if np.mean(td_means) > 1.0:
                print(f"    âš ï¸  TD-errorê°€ ë†’ì•„ ê°€ì¹˜ ì¶”ì •ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            else:
                print(f"    âœ… TD-errorê°€ ì ì ˆí•©ë‹ˆë‹¤.")

    print("\n" + "=" * 80)
    print("í•™ìŠµ ì‹ í˜¸ ë””ë²„ê¹… ì™„ë£Œ")
    print("=" * 80)

    return training_stats


def analyze_learning_convergence(training_stats):
    """í•™ìŠµ ìˆ˜ë ´ì„± ë¶„ì„"""

    print("\n" + "=" * 60)
    print("í•™ìŠµ ìˆ˜ë ´ì„± ë¶„ì„")
    print("=" * 60)

    convergence_issues = []

    for bcell_name, stats in training_stats.items():
        print(f"\n{bcell_name} B-Cell:")

        actor_loss = stats["avg_actor_loss"]
        critic_loss = stats["avg_critic_loss"]
        alpha = stats["alpha"]

        print(f"  Actor Loss: {actor_loss:.6f}")
        print(f"  Critic Loss: {critic_loss:.6f}")
        print(f"  Alpha: {alpha:.4f}")

        # ìˆ˜ë ´ì„± í‰ê°€
        if np.isnan(actor_loss) or np.isnan(critic_loss):
            convergence_issues.append(f"{bcell_name}: NaN ì†ì‹¤ ë°œìƒ")
        elif actor_loss > 100 or critic_loss > 100:
            convergence_issues.append(f"{bcell_name}: ì†ì‹¤ì´ ë„ˆë¬´ í¼")
        elif actor_loss < 1e-8 and critic_loss < 1e-8:
            convergence_issues.append(f"{bcell_name}: ì†ì‹¤ì´ ë„ˆë¬´ ì‘ìŒ (í•™ìŠµ ì •ì²´)")
        else:
            print(f"  âœ… í•™ìŠµ ì‹ í˜¸ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤.")

    if convergence_issues:
        print(f"\në°œê²¬ëœ ìˆ˜ë ´ ë¬¸ì œ:")
        for i, issue in enumerate(convergence_issues, 1):
            print(f"  {i}. {issue}")

        print(f"\nê¶Œì¥ í•´ê²°ì±…:")
        print(f"  - í•™ìŠµë¥  ì¡°ì • (Actor/Critic: 3e-4)")
        print(f"  - ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°•í™” (1.0 â†’ 0.5)")
        print(f"  - Target ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¡°ì •")
        print(f"  - ë³´ìƒ ì •ê·œí™” ì¶”ê°€")
    else:
        print(f"\nâœ… ëª¨ë“  B-Cellì´ ì •ìƒì ìœ¼ë¡œ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    try:
        training_stats = debug_training_signals()

        if training_stats:
            analyze_learning_convergence(training_stats)

        print(f"\nğŸ¯ í•µì‹¬ ë°œê²¬ì‚¬í•­:")
        print(f"   1. í•™ìŠµ ì‹ í˜¸ (Loss, Gradient) ìƒíƒœ ì ê²€ ì™„ë£Œ")
        print(f"   2. TD-error ë¶„í¬ í™•ì¸ë¨")
        print(f"   3. ìˆ˜ë ´ì„± ë¬¸ì œ ì—¬ë¶€ íŒŒì•…ë¨")

    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì‹ í˜¸ ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
