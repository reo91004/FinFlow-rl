# tests/performance_benchmark.py

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import time
from typing import Dict, List
from core import ImmunePortfolioBacktester
from constant import *


class PerformanceBenchmark:
    """BIPD ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.symbols = STOCK_SYMBOLS
        self.train_start = TRAIN_START_DATE
        self.train_end = TRAIN_END_DATE
        self.test_start = TEST_START_DATE
        self.test_end = TEST_END_DATE
        
    def run_comprehensive_benchmark(self, n_runs=10) -> Dict:
        """í¬ê´„ì  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"=== BIPD ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì‹¤í–‰ íšŸìˆ˜: {n_runs}) ===")
        
        results = {
            'sharpe_ratios': [],
            'total_returns': [],
            'max_drawdowns': [],
            'volatilities': [],
            'execution_times': [],
            'convergence_episodes': [],
            'final_rewards': []
        }
        
        for run in range(n_runs):
            print(f"\nì‹¤í–‰ {run + 1}/{n_runs}")
            start_time = time.time()
            
            try:
                # ë°±í…ŒìŠ¤í„° ì´ˆê¸°í™”
                backtester = ImmunePortfolioBacktester(
                    self.symbols, self.train_start, self.train_end, 
                    self.test_start, self.test_end
                )
                
                # ë‹¨ì¼ ì‹¤í–‰
                portfolio_returns, immune_system = backtester.backtest_single_run(
                    seed=42 + run,
                    return_model=True,
                    use_learning_bcells=True,
                    use_hierarchical=True,
                    use_curriculum=True,
                    logging_level="minimal"  # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´
                )
                
                # ì„±ê³¼ ê³„ì‚°
                metrics = backtester.calculate_metrics(portfolio_returns)
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ì €ì¥
                results['sharpe_ratios'].append(metrics['Sharpe Ratio'])
                results['total_returns'].append(metrics['Total Return'])
                results['max_drawdowns'].append(metrics['Max Drawdown'])
                results['volatilities'].append(metrics['Volatility'])
                results['execution_times'].append(execution_time)
                
                # RL íŠ¹ì • ë©”íŠ¸ë¦­
                if hasattr(immune_system, 'bcells') and immune_system.bcells:
                    avg_final_reward = np.mean([
                        bcell.last_critic_loss if hasattr(bcell, 'last_critic_loss') else 0
                        for bcell in immune_system.bcells
                    ])
                    results['final_rewards'].append(avg_final_reward)
                
                print(f"  ìƒ¤í”„ ë¹„ìœ¨: {metrics['Sharpe Ratio']:.3f}")
                print(f"  ì´ ìˆ˜ìµë¥ : {metrics['Total Return']:.2%}")
                print(f"  ìµœëŒ€ ë‚™í­: {metrics['Max Drawdown']:.2%}")
                print(f"  ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ")
                
            except Exception as e:
                print(f"  ì‹¤í–‰ {run + 1} ì‹¤íŒ¨: {e}")
                continue
        
        return self._analyze_benchmark_results(results)
    
    def _analyze_benchmark_results(self, results: Dict) -> Dict:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„"""
        analysis = {}
        
        for metric, values in results.items():
            if values:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°
                analysis[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values),
                    'count': len(values)
                }
        
        return analysis
    
    def run_stability_test(self, target_sharpe=0.5, n_runs=20) -> Dict:
        """ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ - ëª©í‘œ ì„±ê³¼ ë‹¬ì„± í™•ë¥ """
        print(f"=== ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (ëª©í‘œ ìƒ¤í”„ ë¹„ìœ¨: {target_sharpe}) ===")
        
        success_count = 0
        sharpe_ratios = []
        
        for run in range(n_runs):
            try:
                backtester = ImmunePortfolioBacktester(
                    self.symbols, self.train_start, self.train_end,
                    self.test_start, self.test_end
                )
                
                portfolio_returns, _ = backtester.backtest_single_run(
                    seed=100 + run,
                    return_model=False,
                    use_learning_bcells=True,
                    use_hierarchical=True,
                    use_curriculum=True,
                    logging_level="minimal"
                )
                
                metrics = backtester.calculate_metrics(portfolio_returns)
                sharpe = metrics['Sharpe Ratio']
                sharpe_ratios.append(sharpe)
                
                if sharpe >= target_sharpe:
                    success_count += 1
                
                print(f"ì‹¤í–‰ {run + 1}: ìƒ¤í”„ ë¹„ìœ¨ {sharpe:.3f}")
                
            except Exception as e:
                print(f"ì‹¤í–‰ {run + 1} ì‹¤íŒ¨: {e}")
                sharpe_ratios.append(0.0)
        
        success_rate = success_count / n_runs
        
        return {
            'success_rate': success_rate,
            'target_sharpe': target_sharpe,
            'avg_sharpe': np.mean(sharpe_ratios),
            'std_sharpe': np.std(sharpe_ratios),
            'success_count': success_count,
            'total_runs': n_runs
        }
    
    def compare_with_baseline(self) -> Dict:
        """Buy & Hold ê¸°ì¤€ì„ ê³¼ ë¹„êµ"""
        print("=== Buy & Hold ê¸°ì¤€ì„ ê³¼ ë¹„êµ ===")
        
        # BIPD ì„±ê³¼
        backtester = ImmunePortfolioBacktester(
            self.symbols, self.train_start, self.train_end,
            self.test_start, self.test_end
        )
        
        bipd_returns, _ = backtester.backtest_single_run(
            seed=42,
            return_model=False,
            use_learning_bcells=True,
            use_hierarchical=True,
            use_curriculum=True,
            logging_level="minimal"
        )
        bipd_metrics = backtester.calculate_metrics(bipd_returns)
        
        # Buy & Hold ê¸°ì¤€ì„  (ê· ë“± ê°€ì¤‘)
        baseline_returns = backtester.calculate_baseline_performance()
        baseline_metrics = backtester.calculate_metrics(baseline_returns)
        
        comparison = {
            'bipd': bipd_metrics,
            'baseline': baseline_metrics,
            'improvement': {
                'sharpe_ratio': bipd_metrics['Sharpe Ratio'] - baseline_metrics['Sharpe Ratio'],
                'total_return': bipd_metrics['Total Return'] - baseline_metrics['Total Return'],
                'max_drawdown': bipd_metrics['Max Drawdown'] - baseline_metrics['Max Drawdown'],
            }
        }
        
        print(f"BIPD ìƒ¤í”„ ë¹„ìœ¨: {bipd_metrics['Sharpe Ratio']:.3f}")
        print(f"ê¸°ì¤€ì„  ìƒ¤í”„ ë¹„ìœ¨: {baseline_metrics['Sharpe Ratio']:.3f}")
        print(f"ê°œì„ ë„: {comparison['improvement']['sharpe_ratio']:.3f}")
        
        return comparison
    
    def test_convergence_speed(self) -> Dict:
        """í•™ìŠµ ìˆ˜ë ´ ì†ë„ í…ŒìŠ¤íŠ¸"""
        print("=== í•™ìŠµ ìˆ˜ë ´ ì†ë„ í…ŒìŠ¤íŠ¸ ===")
        
        backtester = ImmunePortfolioBacktester(
            self.symbols, self.train_start, self.train_end,
            self.test_start, self.test_end
        )
        
        # ì§§ì€ í•™ìŠµìœ¼ë¡œ ìˆ˜ë ´ í…ŒìŠ¤íŠ¸
        start_time = time.time()
        portfolio_returns, immune_system = backtester.backtest_single_run(
            seed=42,
            return_model=True,
            use_learning_bcells=True,
            use_hierarchical=True,
            use_curriculum=True,
            logging_level="sample"
        )
        training_time = time.time() - start_time
        
        # B-Cell í•™ìŠµ ìƒíƒœ í™•ì¸
        learning_stats = {}
        if hasattr(immune_system, 'bcells'):
            learning_stats = {
                'avg_update_counter': np.mean([
                    bcell.update_counter for bcell in immune_system.bcells
                ]),
                'avg_experience_buffer_size': np.mean([
                    len(bcell.experience_buffer) for bcell in immune_system.bcells
                ]),
                'avg_epsilon': np.mean([
                    bcell.epsilon for bcell in immune_system.bcells
                ])
            }
        
        return {
            'training_time_seconds': training_time,
            'learning_stats': learning_stats,
            'final_metrics': backtester.calculate_metrics(portfolio_returns)
        }


def main():
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmark = PerformanceBenchmark()
    
    print("BIPD ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    # 1. ê¸°ë³¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    basic_results = benchmark.run_comprehensive_benchmark(n_runs=5)
    
    print("\n=== ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ===")
    for metric, stats in basic_results.items():
        print(f"{metric}:")
        print(f"  í‰ê· : {stats['mean']:.4f}")
        print(f"  í‘œì¤€í¸ì°¨: {stats['std']:.4f}")
        print(f"  ë²”ìœ„: [{stats['min']:.4f}, {stats['max']:.4f}]")
    
    # 2. ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
    stability_results = benchmark.run_stability_test(target_sharpe=0.3, n_runs=10)
    
    print(f"\n=== ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ì„±ê³µë¥ : {stability_results['success_rate']:.1%}")
    print(f"í‰ê·  ìƒ¤í”„ ë¹„ìœ¨: {stability_results['avg_sharpe']:.3f}")
    
    # 3. ê¸°ì¤€ì„  ë¹„êµ
    comparison_results = benchmark.compare_with_baseline()
    
    print(f"\n=== ê¸°ì¤€ì„  ëŒ€ë¹„ ê°œì„ ë„ ===")
    print(f"ìƒ¤í”„ ë¹„ìœ¨ ê°œì„ : {comparison_results['improvement']['sharpe_ratio']:.3f}")
    print(f"ìˆ˜ìµë¥  ê°œì„ : {comparison_results['improvement']['total_return']:.2%}")
    
    # 4. ìˆ˜ë ´ ì†ë„ í…ŒìŠ¤íŠ¸
    convergence_results = benchmark.test_convergence_speed()
    
    print(f"\n=== í•™ìŠµ ìˆ˜ë ´ ì†ë„ ===")
    print(f"í›ˆë ¨ ì‹œê°„: {convergence_results['training_time_seconds']:.1f}ì´ˆ")
    if convergence_results['learning_stats']:
        print(f"í‰ê·  ì—…ë°ì´íŠ¸ íšŸìˆ˜: {convergence_results['learning_stats']['avg_update_counter']:.0f}")
        print(f"í‰ê·  ê²½í—˜ ë²„í¼ í¬ê¸°: {convergence_results['learning_stats']['avg_experience_buffer_size']:.0f}")
    
    print("\nğŸ¯ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")


if __name__ == "__main__":
    main()