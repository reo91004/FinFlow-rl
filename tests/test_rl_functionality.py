# tests/test_rl_functionality.py

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import torch
from agents import BCell, TCell, MemoryCell
from core.system import ImmunePortfolioSystem
from core.reward import RewardCalculator
from constant import *


class TestRLFunctionality:
    """ê°•í™”í•™ìŠµ í•µì‹¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""

    def test_experience_replay_buffer(self):
        """Experience Replay Buffer í…ŒìŠ¤íŠ¸"""
        from agents.bcell import ExperienceReplayBuffer

        buffer = ExperienceReplayBuffer(capacity=100)

        # ë°ì´í„° ì¶”ê°€
        state = np.random.random(23)
        action = np.random.random(10)
        reward = 0.1
        next_state = np.random.random(23)
        done = False

        buffer.push(state, action, reward, next_state, done)

        assert len(buffer) == 1
        assert buffer.buffer[0] == (state, action, reward, next_state, done)

    def test_target_network_initialization(self):
        """Target Network ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        bcell = BCell("test", "volatility", 23, 10)

        # Target networkì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        assert hasattr(bcell, "target_critic_network")
        assert bcell.target_critic_network is not None

        # ì´ˆê¸°ì—ëŠ” ê°™ì€ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì•¼ í•¨
        for target_param, param in zip(
            bcell.target_critic_network.parameters(), bcell.critic_network.parameters()
        ):
            assert torch.allclose(target_param.data, param.data)

    def test_target_network_update(self):
        """Target Network ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""
        bcell = BCell("test", "volatility", 23, 10)

        # ë©”ì¸ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ë³€ê²½
        with torch.no_grad():
            for param in bcell.critic_network.parameters():
                param.data += 0.1

        # Target network ì—…ë°ì´íŠ¸
        bcell.update_target_network()

        # ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸ (tau=0.005)
        for target_param, param in zip(
            bcell.target_critic_network.parameters(), bcell.critic_network.parameters()
        ):
            assert not torch.allclose(target_param.data, param.data)

    def test_td_learning_computation(self):
        """TD Learning ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        bcell = BCell("test", "volatility", 23, 10)

        # ê²½í—˜ ë°ì´í„° ì¶”ê°€
        for _ in range(300):  # batch_size(256)ë³´ë‹¤ ë§ì´
            state = np.random.random(23)
            action = np.random.random(10)
            reward = np.random.random() * 0.1
            next_state = np.random.random(23)
            done = np.random.random() > 0.95

            bcell.add_experience(state, action, reward, next_state, done)

        # TD Learning ì‹¤í–‰
        loss = bcell.learn_from_batch()

        assert loss is not None
        assert isinstance(loss, float)
        assert loss >= 0  # MSE lossëŠ” í•­ìƒ ì–‘ìˆ˜
        
    def test_learning_convergence(self):
        """í•™ìŠµ ìˆ˜ë ´ í…ŒìŠ¤íŠ¸ - TD Lossê°€ ê°ì†Œí•˜ëŠ”ì§€ ê²€ì¦"""
        bcell = BCell("test", "volatility", 23, 10)
        
        losses = []
        
        # ì¼ê´€ëœ ê²½í—˜ìœ¼ë¡œ í•™ìŠµ (ìˆ˜ë ´ ìœ ë„)
        for episode in range(20):
            for _ in range(30):
                # ì¼ê´€ëœ íŒ¨í„´ì˜ ê²½í—˜ ìƒì„±
                state = np.random.random(23) * 0.1 + 0.5  # ë¹„ìŠ·í•œ ìƒíƒœë“¤
                action = np.ones(10) / 10  # ê· ë“± ì•¡ì…˜
                reward = 0.1 if np.mean(state) > 0.5 else -0.1  # ì¼ê´€ëœ ë³´ìƒ
                next_state = state + np.random.random(23) * 0.05  # ì•½ê°„ì˜ ë³€í™”
                done = False
                
                bcell.add_experience(state, action, reward, next_state, done)
                
            # ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ í•™ìŠµ
            loss = bcell.learn_from_batch()
            if loss is not None:
                losses.append(loss)
        
        # í•™ìŠµì´ ì§„í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert len(losses) > 5, "ì¶©ë¶„í•œ í•™ìŠµì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŒ"
        
        # ìˆ˜ë ´ ì—¬ë¶€ í™•ì¸ (í›„ë°˜ë¶€ ì†ì‹¤ì´ ì „ë°˜ë¶€ë³´ë‹¤ ë‚®ê±°ë‚˜ ì•ˆì •ì )
        early_losses = np.mean(losses[:5])
        late_losses = np.mean(losses[-5:])
        
        # ì†ì‹¤ì´ ê°ì†Œí•˜ê±°ë‚˜ ì•ˆì •í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
        improvement = early_losses - late_losses
        assert improvement >= 0 or late_losses < early_losses * 1.1, f"í•™ìŠµ ìˆ˜ë ´ ì‹¤íŒ¨: ì´ˆê¸° ì†ì‹¤ {early_losses:.4f} -> í›„ê¸° ì†ì‹¤ {late_losses:.4f}"

    def test_activation_threshold(self):
        """í™œì„±í™” ì„ê³„ê°’ í…ŒìŠ¤íŠ¸"""
        bcell = BCell("test", "volatility", 23, 10)

        # ë‚®ì€ ìê·¹ - í™œì„±í™” ì•ˆë¨
        low_stimulus = 0.3
        assert not bcell.should_activate(low_stimulus)

        # ë†’ì€ ìê·¹ - í™œì„±í™”ë¨
        high_stimulus = 0.8
        assert bcell.should_activate(high_stimulus)

    def test_reward_single_clipping(self):
        """ë³´ìƒ ì‹œìŠ¤í…œ ë‹¨ì¼ í´ë¦¬í•‘ í…ŒìŠ¤íŠ¸"""
        reward_calc = RewardCalculator()

        # ê·¹ë‹¨ì  ì…ë ¥ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        extreme_return = 0.5  # 50% ì¼ì¼ ìˆ˜ìµë¥ 
        weights_prev = np.ones(10) / 10
        weights_curr = np.random.random(10)
        weights_curr /= weights_curr.sum()
        market_features = np.random.random(12)
        crisis_level = 0.8

        reward_details = reward_calc.calculate_comprehensive_reward(
            extreme_return, weights_prev, weights_curr, market_features, crisis_level
        )

        # í´ë¦¬í•‘ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        final_reward = reward_details["total_reward"]
        assert REWARD_CLIPPING_RANGE[0] <= final_reward <= REWARD_CLIPPING_RANGE[1]

    def test_state_transition_chain(self):
        """State Transition ì²´ì¸ í…ŒìŠ¤íŠ¸"""
        system = ImmunePortfolioSystem(n_assets=10, n_tcells=3, n_bcells=5)

        # ê°€ìƒ ì‹œì¥ ë°ì´í„° (pandas DataFrameìœ¼ë¡œ)
        import pandas as pd
        market_data = pd.DataFrame(np.random.random((100, 10)))

        states = []
        for i in range(5):
            if len(market_data[:i+20]) > 20:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
                state = system.extract_market_features(market_data[:i+20])
                states.append(state)

        # ì—°ì†ëœ stateë“¤ì´ ë‹¤ë¥¸ì§€ í™•ì¸ (ì‹œì¥ì´ ë³€í™”í•˜ë¯€ë¡œ)
        assert not np.array_equal(states[0], states[1])
        assert not np.array_equal(states[1], states[2])

        # ëª¨ë“  stateê°€ ì˜¬ë°”ë¥¸ ì°¨ì›ì¸ì§€ í™•ì¸
        for state in states:
            assert len(state) == FEATURE_SIZE

    def test_gradient_flow(self):
        """Gradient Flow í…ŒìŠ¤íŠ¸"""
        bcell = BCell("test", "volatility", 23, 10)

        # ë”ë¯¸ ì…ë ¥
        market_features = torch.randn(12, requires_grad=True)
        crisis_level = 0.5
        tcell_contributions = {"volatility": 0.8, "correlation": 0.3}

        # Forward pass
        attended_features, attention_weights = bcell.attention_mechanism(
            market_features, tcell_contributions
        )

        # Backward pass
        loss = attended_features.sum()
        loss.backward()

        # Gradientê°€ íë¥´ëŠ”ì§€ í™•ì¸
        assert market_features.grad is not None
        assert not torch.allclose(
            market_features.grad, torch.zeros_like(market_features.grad)
        )


def test_realistic_parameters():
    """í˜„ì‹¤ì  íŒŒë¼ë¯¸í„° ì„¤ì • í…ŒìŠ¤íŠ¸"""
    assert TOTAL_EPISODES == 50000  # 5ë§Œ ì—í”¼ì†Œë“œ
    assert DEFAULT_BATCH_SIZE == 256  # 256 ë°°ì¹˜ í¬ê¸°
    assert EPISODE_LENGTH == 252  # 1ë…„ ê±°ë˜ì¼
    assert DEFAULT_GAMMA == 0.99  # í˜„ì‹¤ì  í• ì¸ìœ¨


if __name__ == "__main__":
    print("ê°•í™”í•™ìŠµ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")

    test_suite = TestRLFunctionality()

    try:
        test_suite.test_experience_replay_buffer()
        print("âœ… Experience Replay Buffer í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_suite.test_target_network_initialization()
        print("âœ… Target Network ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_suite.test_target_network_update()
        print("âœ… Target Network ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_suite.test_td_learning_computation()
        print("âœ… TD Learning ê³„ì‚° í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_suite.test_activation_threshold()
        print("âœ… í™œì„±í™” ì„ê³„ê°’ í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_suite.test_reward_single_clipping()
        print("âœ… ë³´ìƒ ë‹¨ì¼ í´ë¦¬í•‘ í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_suite.test_state_transition_chain()
        print("âœ… State Transition ì²´ì¸ í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_suite.test_gradient_flow()
        print("âœ… Gradient Flow í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        test_suite.test_learning_convergence()
        print("âœ… í•™ìŠµ ìˆ˜ë ´ í…ŒìŠ¤íŠ¸ í†µê³¼")

        test_realistic_parameters()
        print("âœ… í˜„ì‹¤ì  íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸ í†µê³¼")

        print("\nğŸ‰ ëª¨ë“  ê°•í™”í•™ìŠµ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼!")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise
