# tests/debug_agent.py - SAC ì—ì´ì „íŠ¸ íƒí—˜ ë° ì´ˆê¸°í™” ë””ë²„ê¹…

import os
import sys
import warnings

warnings.filterwarnings("ignore")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import torch
import pandas as pd
from datetime import datetime

from config import *
from agents.bcell import BCell
from data import DataLoader
from core.environment import PortfolioEnvironment
from data.features import FeatureExtractor


def debug_sac_agent():
    """SAC ì—ì´ì „íŠ¸ì˜ íƒí—˜ ì „ëµ ë° ì´ˆê¸°í™” ë””ë²„ê¹…"""

    print("=" * 80)
    print("SAC ì—ì´ì „íŠ¸ ë””ë²„ê¹… ì‹œì‘")
    print("=" * 80)

    # 1. í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
    print("[1] í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •...")
    n_assets = 5
    state_dim = 12 + 1 + n_assets  # features + crisis + weights

    # B-Cell ì´ˆê¸°í™”
    bcell = BCell("volatility", state_dim, n_assets)

    print(f"B-Cell ì´ˆê¸°í™” ì™„ë£Œ:")
    print(f"  - ìœ„í—˜ ìœ í˜•: {bcell.risk_type}")
    print(f"  - ìƒíƒœ ì°¨ì›: {bcell.state_dim}")
    print(f"  - í–‰ë™ ì°¨ì›: {bcell.action_dim}")
    print(f"  - ì´ˆê¸° Alpha: {bcell.alpha.item():.4f}")
    print(f"  - Target Entropy: {bcell.target_entropy:.4f}")
    print(f"  - ì—…ë°ì´íŠ¸ ë¹ˆë„: {bcell.update_frequency}")

    # 2. íƒí—˜ í–‰ë™ í…ŒìŠ¤íŠ¸
    print(f"\n[2] íƒí—˜ í–‰ë™ ë¶„ì„ (100íšŒ ìƒ˜í”Œë§)...")

    # ë”ë¯¸ ìƒíƒœ ìƒì„±
    test_state = np.concatenate(
        [
            np.random.randn(12) * 0.1,  # ì •ê·œí™”ëœ ì‹œì¥ íŠ¹ì„±
            [0.3],  # ìœ„ê¸° ìˆ˜ì¤€
            np.ones(n_assets) / n_assets,  # ê· ë“± ê°€ì¤‘ì¹˜
        ]
    )

    # 100ë²ˆ í–‰ë™ ìƒ˜í”Œë§
    actions_training = []
    actions_eval = []

    bcell.actor.train()  # í›ˆë ¨ ëª¨ë“œ
    for _ in range(100):
        action = bcell.get_action(test_state, deterministic=False)
        actions_training.append(action)

    bcell.actor.eval()  # í‰ê°€ ëª¨ë“œ
    for _ in range(100):
        action = bcell.get_action(test_state, deterministic=True)
        actions_eval.append(action)

    actions_training = np.array(actions_training)
    actions_eval = np.array(actions_eval)

    # íƒí—˜ ì •ë„ ë¶„ì„
    training_entropy = -np.sum(
        actions_training * np.log(actions_training + 1e-8), axis=1
    ).mean()
    eval_entropy = -np.sum(actions_eval * np.log(actions_eval + 1e-8), axis=1).mean()

    print(f"  í›ˆë ¨ ëª¨ë“œ:")
    print(f"    í‰ê·  ì—”íŠ¸ë¡œí”¼: {training_entropy:.4f}")
    print(f"    ê°€ì¤‘ì¹˜ í‘œì¤€í¸ì°¨: {actions_training.std(axis=0).mean():.4f}")
    print(
        f"    ìµœëŒ€ ê°€ì¤‘ì¹˜ ë²”ìœ„: [{actions_training.max():.3f}, {actions_training.min():.3f}]"
    )

    print(f"  í‰ê°€ ëª¨ë“œ:")
    print(f"    í‰ê·  ì—”íŠ¸ë¡œí”¼: {eval_entropy:.4f}")
    print(f"    ê°€ì¤‘ì¹˜ í‘œì¤€í¸ì°¨: {actions_eval.std(axis=0).mean():.4f}")
    print(f"    ìµœëŒ€ ê°€ì¤‘ì¹˜ ë²”ìœ„: [{actions_eval.max():.3f}, {actions_eval.min():.3f}]")

    # íƒí—˜ ì ì ˆì„± í‰ê°€
    exploration_ratio = (
        training_entropy / eval_entropy if eval_entropy > 0 else float("inf")
    )
    print(f"    íƒí—˜ ë¹„ìœ¨: {exploration_ratio:.2f}")

    if exploration_ratio < 1.5:
        print("    âš ï¸  íƒí—˜ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    elif exploration_ratio > 10:
        print("    âš ï¸  íƒí—˜ì´ ê³¼ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("    âœ… íƒí—˜ ì •ë„ê°€ ì ì ˆí•©ë‹ˆë‹¤.")

    # 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ì ˆì„± ê²€ì‚¬
    print(f"\n[3] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ì ˆì„± ê²€ì‚¬...")

    # í•™ìŠµë¥  ì²´í¬
    print(f"  í•™ìŠµë¥ :")
    print(f"    Actor LR: {ACTOR_LR:.0e} (ê¶Œì¥: 1e-4~3e-4)")
    print(f"    Critic LR: {CRITIC_LR:.0e} (ê¶Œì¥: 1e-4~3e-4)")
    print(f"    Alpha LR: {ALPHA_LR:.0e} (ê¶Œì¥: 1e-4~3e-4)")

    if ACTOR_LR < 1e-5:
        print("    âš ï¸  Actor í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    elif ACTOR_LR > 1e-3:
        print("    âš ï¸  Actor í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("    âœ… Actor í•™ìŠµë¥ ì´ ì ì ˆí•©ë‹ˆë‹¤.")

    # í• ì¸ìœ¨ ì²´í¬
    print(f"\n  ê°•í™”í•™ìŠµ íŒŒë¼ë¯¸í„°:")
    print(f"    Gamma (í• ì¸ìœ¨): {GAMMA} (ê¶Œì¥: 0.95~0.99)")
    print(f"    Tau (íƒ€ê²Ÿ ì—…ë°ì´íŠ¸): {TAU} (ê¶Œì¥: 0.005~0.01)")
    print(f"    ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE} (ê¶Œì¥: 64~256)")
    print(f"    ë²„í¼ í¬ê¸°: {BUFFER_SIZE} (ê¶Œì¥: 10K~1M)")

    if GAMMA < 0.9:
        print("    âš ï¸  í• ì¸ìœ¨ì´ ë„ˆë¬´ ë‚®ì•„ ì¥ê¸° í•™ìŠµì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    elif GAMMA > 0.999:
        print("    âš ï¸  í• ì¸ìœ¨ì´ ë„ˆë¬´ ë†’ì•„ ìˆ˜ë ´ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("    âœ… í• ì¸ìœ¨ì´ ì ì ˆí•©ë‹ˆë‹¤.")

    # 4. ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ìƒíƒœ ê²€ì‚¬
    print(f"\n[4] ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ìƒíƒœ ê²€ì‚¬...")

    # Actor ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¶„í¬
    actor_weights = []
    for param in bcell.actor.parameters():
        actor_weights.extend(param.data.flatten().cpu().numpy())

    actor_weights = np.array(actor_weights)

    print(f"  Actor ë„¤íŠ¸ì›Œí¬:")
    print(f"    ê°€ì¤‘ì¹˜ í‰ê· : {actor_weights.mean():.6f}")
    print(f"    ê°€ì¤‘ì¹˜ í‘œì¤€í¸ì°¨: {actor_weights.std():.6f}")
    print(f"    ê°€ì¤‘ì¹˜ ë²”ìœ„: [{actor_weights.min():.6f}, {actor_weights.max():.6f}]")

    if actor_weights.std() < 1e-3:
        print("    âš ï¸  ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ê°€ ë„ˆë¬´ ì‘ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    elif actor_weights.std() > 1:
        print("    âš ï¸  ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ê°€ ë„ˆë¬´ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("    âœ… ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ê°€ ì ì ˆí•©ë‹ˆë‹¤.")

    # Critic ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ ë¶„í¬
    critic_weights = []
    for param in bcell.critic1.parameters():
        critic_weights.extend(param.data.flatten().cpu().numpy())

    critic_weights = np.array(critic_weights)

    print(f"  Critic ë„¤íŠ¸ì›Œí¬:")
    print(f"    ê°€ì¤‘ì¹˜ í‰ê· : {critic_weights.mean():.6f}")
    print(f"    ê°€ì¤‘ì¹˜ í‘œì¤€í¸ì°¨: {critic_weights.std():.6f}")
    print(f"    ê°€ì¤‘ì¹˜ ë²”ìœ„: [{critic_weights.min():.6f}, {critic_weights.max():.6f}]")

    # 5. Dirichlet ë¶„í¬ íŒŒë¼ë¯¸í„° ê²€ì‚¬
    print(f"\n[5] Dirichlet ë¶„í¬ íŒŒë¼ë¯¸í„° ê²€ì‚¬...")

    with torch.no_grad():
        state_tensor = (
            torch.tensor(test_state, dtype=torch.float32).unsqueeze(0).to(DEVICE)
        )
        concentration, weights, log_prob = bcell.actor(state_tensor)

        concentration_np = concentration.cpu().numpy().flatten()
        weights_np = weights.cpu().numpy().flatten()

    print(f"  Concentration íŒŒë¼ë¯¸í„°:")
    print(f"    í‰ê· : {concentration_np.mean():.4f}")
    print(f"    í‘œì¤€í¸ì°¨: {concentration_np.std():.4f}")
    print(f"    ë²”ìœ„: [{concentration_np.min():.4f}, {concentration_np.max():.4f}]")

    if concentration_np.mean() < 1.0:
        print("    âš ï¸  Concentrationì´ ë„ˆë¬´ ë‚®ì•„ íƒí—˜ì´ ê³¼ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    elif concentration_np.mean() > 10.0:
        print("    âš ï¸  Concentrationì´ ë„ˆë¬´ ë†’ì•„ íƒí—˜ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("    âœ… Concentrationì´ ì ì ˆí•©ë‹ˆë‹¤.")

    print(f"  ìƒì„±ëœ ê°€ì¤‘ì¹˜:")
    print(f"    ê°€ì¤‘ì¹˜: {weights_np}")
    print(f"    ê°€ì¤‘ì¹˜ í•©: {weights_np.sum():.6f}")
    print(f"    ì—”íŠ¸ë¡œí”¼: {-np.sum(weights_np * np.log(weights_np + 1e-8)):.4f}")

    print("\n" + "=" * 80)
    print("SAC ì—ì´ì „íŠ¸ ë””ë²„ê¹… ì™„ë£Œ")
    print("=" * 80)

    return {
        "training_entropy": training_entropy,
        "eval_entropy": eval_entropy,
        "exploration_ratio": exploration_ratio,
        "actor_weights_std": actor_weights.std(),
        "concentration_mean": concentration_np.mean(),
    }


def analyze_learning_issues(debug_results):
    """í•™ìŠµ ë¬¸ì œì  ì¢…í•© ë¶„ì„"""

    print("\n" + "=" * 60)
    print("í•™ìŠµ ë¬¸ì œì  ì¢…í•© ë¶„ì„")
    print("=" * 60)

    issues = []

    # 1. íƒí—˜ ë¬¸ì œ
    if debug_results["exploration_ratio"] < 1.5:
        issues.append("íƒí—˜ ë¶€ì¡±: ë‹¤ì–‘í•œ ì „ëµì„ ì‹œë„í•˜ì§€ ì•ŠìŒ")
    elif debug_results["exploration_ratio"] > 10:
        issues.append("ê³¼ë„í•œ íƒí—˜: í•™ìŠµëœ ì§€ì‹ì„ í™œìš©í•˜ì§€ ì•ŠìŒ")

    # 2. ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ë¬¸ì œ
    if debug_results["actor_weights_std"] < 1e-3:
        issues.append("ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ë¶ˆëŸ‰: ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì‘ìŒ")
    elif debug_results["actor_weights_std"] > 1:
        issues.append("ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ë¶ˆëŸ‰: ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ í¼")

    # 3. Concentration ë¬¸ì œ
    if debug_results["concentration_mean"] < 1.0:
        issues.append("Dirichlet concentration ë„ˆë¬´ ë‚®ìŒ: ë¶ˆì•ˆì •í•œ ì •ì±…")
    elif debug_results["concentration_mean"] > 10.0:
        issues.append("Dirichlet concentration ë„ˆë¬´ ë†’ìŒ: ê²½ì§ëœ ì •ì±…")

    # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¬¸ì œ
    if ACTOR_LR < 1e-5:
        issues.append("Actor í•™ìŠµë¥  ë„ˆë¬´ ë‚®ìŒ: í•™ìŠµ ì†ë„ ì €í•˜")
    elif ACTOR_LR > 1e-3:
        issues.append("Actor í•™ìŠµë¥  ë„ˆë¬´ ë†’ìŒ: ë¶ˆì•ˆì •í•œ í•™ìŠµ")

    print(f"ë°œê²¬ëœ ë¬¸ì œì : {len(issues)}ê°œ")
    for i, issue in enumerate(issues, 1):
        print(f"  {i}. {issue}")

    if not issues:
        print("âœ… ì—ì´ì „íŠ¸ ì´ˆê¸° ì¡°ê±´ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")

    # ê¶Œì¥ ê°œì„ ì‚¬í•­
    print(f"\nê¶Œì¥ ê°œì„ ì‚¬í•­:")
    if debug_results["exploration_ratio"] < 1.5:
        print("  - Target entropy ì¦ê°€: -15 â†’ -7.5")
        print("  - Alpha í•™ìŠµë¥  ì¦ê°€: 1e-4 â†’ 3e-4")

    if debug_results["concentration_mean"] < 1.0:
        print("  - Concentration ìµœì†Œê°’ ì¦ê°€: 1.0 â†’ 2.0")

    if ACTOR_LR < 3e-4:
        print("  - Actor í•™ìŠµë¥  ì¦ê°€: 1e-4 â†’ 3e-4")


if __name__ == "__main__":
    try:
        debug_results = debug_sac_agent()
        analyze_learning_issues(debug_results)

        print(f"\nğŸ¯ í•µì‹¬ ë°œê²¬ì‚¬í•­:")
        print(f"   1. SAC ì—ì´ì „íŠ¸ì˜ íƒí—˜/í™œìš© ê· í˜• ì ê²€ ì™„ë£Œ")
        print(f"   2. ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ë¨")
        print(f"   3. ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‹¤ì œ í•™ìŠµ ì‹ í˜¸ë¥¼ ë¶„ì„í•´ì•¼ í•¨")

    except Exception as e:
        print(f"\nâŒ ì—ì´ì „íŠ¸ ë””ë²„ê¹… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
