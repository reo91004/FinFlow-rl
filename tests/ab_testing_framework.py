# tests/ab_testing_framework.py

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass, asdict
from collections import defaultdict

from utils.logger import BIPDLogger
from core.environment import PortfolioEnvironment
from core.system import ImmunePortfolioSystem
from core.trainer import BIPDTrainer
from data.features import FeatureExtractor
from data.loader import DataLoader
from config import SYMBOLS, TRAIN_START, TRAIN_END, TEST_START, TEST_END


@dataclass
class ExperimentConfig:
    """ì‹¤í—˜ ì„¤ì •"""

    name: str
    description: str
    adaptive_entropy: bool = True
    adaptive_no_trade_band: bool = True
    use_simplex_projection: bool = True
    n_episodes: int = 100  # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ ì¶•ì†Œ
    random_seed: int = 42

    def to_dict(self):
        return asdict(self)


@dataclass
class ExperimentResult:
    """ì‹¤í—˜ ê²°ê³¼"""

    config_name: str
    final_portfolio_value: float
    avg_sharpe_ratio: float
    max_drawdown: float
    avg_turnover: float
    success_rate: float
    training_stability: float
    regime_stats: Optional[Dict] = None
    entropy_gap_stats: Optional[Dict] = None
    execution_time: float = 0.0

    def to_dict(self):
        return asdict(self)


class ABTestingFramework:
    """Phase 3 ê°œì„ ì‚¬í•­ A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""

    def __init__(
        self,
        market_data: Optional[pd.DataFrame] = None,
        save_dir: str = "experiments/results",
        logger_name: str = "ABTesting",
    ):

        self.logger = BIPDLogger(logger_name)
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

        # ì‹œì¥ ë°ì´í„° ë¡œë“œ
        if market_data is None:
            self.logger.info("ì‹œì¥ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            data_loader = DataLoader()
            market_data = data_loader.get_market_data(
                symbols=SYMBOLS[:10],  # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ 10ê°œ ì¢…ëª©ë§Œ
                train_start=TRAIN_START,
                train_end=TRAIN_END,
                test_start=TEST_START,
                test_end=TEST_END,
            )

        self.train_data = market_data["train_data"]
        self.test_data = market_data["test_data"]

        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        self.experiment_results = {}

        self.logger.info(f"A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"  í›ˆë ¨ ë°ì´í„°: {len(self.train_data)} ê±°ë˜ì¼")
        self.logger.info(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(self.test_data)} ê±°ë˜ì¼")
        self.logger.info(f"  ì¢…ëª© ìˆ˜: {len(self.train_data.columns)}ê°œ")

    def run_experiment(self, config: ExperimentConfig) -> ExperimentResult:
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        self.logger.info(f"ì‹¤í—˜ ì‹œì‘: {config.name}")
        self.logger.info(f"  ì„¤ì •: {config.description}")

        start_time = datetime.now()

        try:
            # ì‹¤í—˜ë³„ ì‹œë“œ ì„¤ì •
            np.random.seed(config.random_seed)

            # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° ì„¤ì • ì ìš©
            trainer = self._create_trainer_with_config(config)

            # í›ˆë ¨ ì‹¤í–‰
            training_results = trainer.train(
                n_episodes=config.n_episodes,
                save_interval=max(config.n_episodes // 4, 10),
            )

            # í‰ê°€ ì‹¤í–‰
            evaluation_results = trainer.evaluate(n_episodes=5)

            # ë ˆì§ í†µê³„ ìˆ˜ì§‘ (ì ì‘í˜• ì—”íŠ¸ë¡œí”¼ê°€ í™œì„±í™”ëœ ê²½ìš°)
            regime_stats = None
            entropy_gap_stats = None

            if config.adaptive_entropy:
                regime_stats = self._collect_regime_statistics(trainer)
                entropy_gap_stats = self._collect_entropy_gap_statistics(trainer)

            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = (datetime.now() - start_time).total_seconds()

            # ê²°ê³¼ êµ¬ì„±
            result = ExperimentResult(
                config_name=config.name,
                final_portfolio_value=evaluation_results["avg_final_value"],
                avg_sharpe_ratio=evaluation_results["avg_sharpe_ratio"],
                max_drawdown=evaluation_results["avg_max_drawdown"],
                avg_turnover=evaluation_results.get("avg_turnover", 0.0),
                success_rate=evaluation_results["success_rate"],
                training_stability=training_results["training_stability"],
                regime_stats=regime_stats,
                entropy_gap_stats=entropy_gap_stats,
                execution_time=execution_time,
            )

            self.logger.info(f"ì‹¤í—˜ ì™„ë£Œ: {config.name} ({execution_time:.1f}ì´ˆ)")
            self.logger.info(f"  ìµœì¢… ê°€ì¹˜: {result.final_portfolio_value:,.0f}")
            self.logger.info(f"  ìƒ¤í”„ ë¹„ìœ¨: {result.avg_sharpe_ratio:.3f}")
            self.logger.info(f"  ì„±ê³µë¥ : {result.success_rate:.1%}")

            return result

        except Exception as e:
            self.logger.error(f"ì‹¤í—˜ ì‹¤íŒ¨: {config.name} - {str(e)}")
            import traceback

            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            raise

    def _create_trainer_with_config(self, config: ExperimentConfig) -> BIPDTrainer:
        """ì‹¤í—˜ ì„¤ì •ì— ë§ëŠ” íŠ¸ë ˆì´ë„ˆ ìƒì„±"""

        # ì„ì‹œë¡œ ì„¤ì •ì„ ì ìš©í•˜ê¸° ìœ„í•´ config ëª¨ë“ˆì„ ìˆ˜ì •
        # (ì‹¤ì œë¡œëŠ” configë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì¢‹ì§€ë§Œ, ê¸°ì¡´ ì½”ë“œ êµ¬ì¡°ìƒ ì´ë ‡ê²Œ í•¨)

        # B-Cell ì„¤ì • ìˆ˜ì • (ì ì‘í˜• ì—”íŠ¸ë¡œí”¼)
        if hasattr(config, "adaptive_entropy"):
            # ì´ëŠ” BCell ì´ˆê¸°í™” ì‹œ ì ìš©ë¨ (ì´ë¯¸ êµ¬í˜„ë¨)
            pass

        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = BIPDTrainer(train_data=self.train_data, test_data=self.test_data)

        return trainer

    def _collect_regime_statistics(self, trainer) -> Optional[Dict]:
        """ë ˆì§ í†µê³„ ìˆ˜ì§‘"""
        try:
            # ì‹œìŠ¤í…œì—ì„œ B-Cellë“¤ì˜ ì—”íŠ¸ë¡œí”¼ ìŠ¤ì¼€ì¤„ëŸ¬ í†µê³„ ìˆ˜ì§‘
            regime_stats = {}

            for bcell_name, bcell in trainer.immune_system.bcells.items():
                if (
                    hasattr(bcell, "entropy_scheduler")
                    and bcell.entropy_scheduler is not None
                ):
                    stats = bcell.entropy_scheduler.get_regime_statistics()
                    regime_stats[bcell_name] = stats

            return regime_stats if regime_stats else None

        except Exception as e:
            self.logger.warning(f"ë ˆì§ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return None

    def _collect_entropy_gap_statistics(self, trainer) -> Optional[Dict]:
        """ì—”íŠ¸ë¡œí”¼ ê°­ í†µê³„ ìˆ˜ì§‘"""
        try:
            entropy_stats = {}

            for bcell_name, bcell in trainer.immune_system.bcells.items():
                if (
                    hasattr(bcell, "entropy_scheduler")
                    and bcell.entropy_scheduler is not None
                ):
                    stats = bcell.entropy_scheduler.get_regime_statistics()
                    if "entropy_gap_stats" in stats:
                        entropy_stats[bcell_name] = stats["entropy_gap_stats"]

            return entropy_stats if entropy_stats else None

        except Exception as e:
            self.logger.warning(f"ì—”íŠ¸ë¡œí”¼ ê°­ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return None

    def run_comparative_study(
        self, experiment_configs: List[ExperimentConfig], n_runs: int = 3
    ) -> Dict[str, List[ExperimentResult]]:
        """ë¹„êµ ì—°êµ¬ ì‹¤í–‰ (ì—¬ëŸ¬ ì‹¤í–‰ìœ¼ë¡œ í†µê³„ì  ìœ ì˜ì„± í™•ë³´)"""

        self.logger.info(
            f"ë¹„êµ ì—°êµ¬ ì‹œì‘: {len(experiment_configs)}ê°œ ì„¤ì •, ê°ê° {n_runs}íšŒ ì‹¤í–‰"
        )

        all_results = defaultdict(list)

        for config in experiment_configs:
            self.logger.info(f"\nì„¤ì • '{config.name}' ì‹¤í—˜ ì‹œì‘...")

            for run_idx in range(n_runs):
                self.logger.info(f"  ì‹¤í–‰ {run_idx + 1}/{n_runs}")

                # ì‹¤í–‰ë³„ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš©
                run_config = ExperimentConfig(
                    name=f"{config.name}_run_{run_idx + 1}",
                    description=config.description,
                    adaptive_entropy=config.adaptive_entropy,
                    adaptive_no_trade_band=config.adaptive_no_trade_band,
                    use_simplex_projection=config.use_simplex_projection,
                    n_episodes=config.n_episodes,
                    random_seed=config.random_seed + run_idx * 100,
                )

                result = self.run_experiment(run_config)
                all_results[config.name].append(result)

        # ê²°ê³¼ ì €ì¥
        self._save_comparative_results(all_results)

        return dict(all_results)

    def _save_comparative_results(self, results: Dict[str, List[ExperimentResult]]):
        """ë¹„êµ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ì›ì‹œ ê²°ê³¼ ì €ì¥
        raw_results = {
            config_name: [result.to_dict() for result in result_list]
            for config_name, result_list in results.items()
        }

        raw_file = os.path.join(self.save_dir, f"ab_test_raw_results_{timestamp}.json")
        with open(raw_file, "w") as f:
            json.dump(raw_results, f, indent=2, ensure_ascii=False)

        # í†µê³„ ìš”ì•½ ìƒì„±
        summary = self._generate_summary_statistics(results)

        summary_file = os.path.join(self.save_dir, f"ab_test_summary_{timestamp}.json")
        with open(summary_file, "w") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        self.logger.info(f"  ì›ì‹œ ë°ì´í„°: {raw_file}")
        self.logger.info(f"  í†µê³„ ìš”ì•½: {summary_file}")

    def _generate_summary_statistics(
        self, results: Dict[str, List[ExperimentResult]]
    ) -> Dict:
        """í†µê³„ ìš”ì•½ ìƒì„±"""
        summary = {
            "experiment_info": {
                "timestamp": datetime.now().isoformat(),
                "n_configs": len(results),
                "n_runs_per_config": len(list(results.values())[0]) if results else 0,
            },
            "config_summaries": {},
        }

        for config_name, result_list in results.items():
            # ë©”íŠ¸ë¦­ë³„ í†µê³„
            metrics = [
                "final_portfolio_value",
                "avg_sharpe_ratio",
                "max_drawdown",
                "success_rate",
                "training_stability",
                "execution_time",
            ]

            config_summary = {}

            for metric in metrics:
                values = [getattr(result, metric, 0.0) for result in result_list]
                if values and not all(v == 0.0 for v in values):
                    config_summary[metric] = {
                        "mean": np.mean(values),
                        "std": np.std(values),
                        "min": np.min(values),
                        "max": np.max(values),
                        "values": values,
                    }

            summary["config_summaries"][config_name] = config_summary

        return summary

    def print_comparison_report(self, results: Dict[str, List[ExperimentResult]]):
        """ë¹„êµ ë³´ê³ ì„œ ì¶œë ¥"""
        self.logger.info("\n" + "=" * 80)
        self.logger.info("Phase 3 ê°œì„ ì‚¬í•­ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        self.logger.info("=" * 80)

        # ì„¤ì •ë³„ í‰ê·  ì„±ëŠ¥
        for config_name, result_list in results.items():
            n_runs = len(result_list)

            # ë©”íŠ¸ë¦­ í‰ê·  ê³„ì‚°
            avg_value = np.mean([r.final_portfolio_value for r in result_list])
            avg_sharpe = np.mean([r.avg_sharpe_ratio for r in result_list])
            avg_success = np.mean([r.success_rate for r in result_list])
            avg_time = np.mean([r.execution_time for r in result_list])

            std_value = np.std([r.final_portfolio_value for r in result_list])
            std_sharpe = np.std([r.avg_sharpe_ratio for r in result_list])

            self.logger.info(f"\n--- {config_name} ({n_runs}íšŒ ì‹¤í–‰) ---")
            self.logger.info(f"  ìµœì¢… ê°€ì¹˜: {avg_value:,.0f} (Â±{std_value:,.0f})")
            self.logger.info(f"  ìƒ¤í”„ ë¹„ìœ¨: {avg_sharpe:.3f} (Â±{std_sharpe:.3f})")
            self.logger.info(f"  ì„±ê³µë¥ : {avg_success:.1%}")
            self.logger.info(f"  ì‹¤í–‰ ì‹œê°„: {avg_time:.1f}ì´ˆ")

        # ìµœê³  ì„±ëŠ¥ êµ¬ì„± ì‹ë³„
        best_config = max(
            results.keys(),
            key=lambda k: np.mean([r.avg_sharpe_ratio for r in results[k]]),
        )

        self.logger.info(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_config}")

        # í†µê³„ì  ìœ ì˜ì„± ê°„ë‹¨ ì²´í¬
        if len(results) >= 2:
            self._simple_significance_test(results)

    def _simple_significance_test(self, results: Dict[str, List[ExperimentResult]]):
        """ê°„ë‹¨í•œ í†µê³„ì  ìœ ì˜ì„± í…ŒìŠ¤íŠ¸"""
        config_names = list(results.keys())

        if len(config_names) == 2:
            config_a, config_b = config_names
            values_a = [r.avg_sharpe_ratio for r in results[config_a]]
            values_b = [r.avg_sharpe_ratio for r in results[config_b]]

            # ë‹¨ìˆœ t-í…ŒìŠ¤íŠ¸ (ì •ê·œë¶„í¬ ê°€ì •)
            from scipy import stats

            try:
                t_stat, p_value = stats.ttest_ind(values_a, values_b)

                self.logger.info(f"\nğŸ“Š í†µê³„ì  ìœ ì˜ì„± í…ŒìŠ¤íŠ¸ (ìƒ¤í”„ ë¹„ìœ¨ ê¸°ì¤€):")
                self.logger.info(f"  {config_a} vs {config_b}")
                self.logger.info(f"  t-í†µê³„ëŸ‰: {t_stat:.3f}")
                self.logger.info(f"  p-ê°’: {p_value:.3f}")

                if p_value < 0.05:
                    winner = (
                        config_a if np.mean(values_a) > np.mean(values_b) else config_b
                    )
                    self.logger.info(
                        f"  ê²°ë¡ : {winner}ì´(ê°€) í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê²Œ ìš°ìˆ˜ (p<0.05)"
                    )
                else:
                    self.logger.info(f"  ê²°ë¡ : í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (pâ‰¥0.05)")

            except ImportError:
                self.logger.info(
                    "scipyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í†µê³„ì  ìœ ì˜ì„± í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
                )
            except Exception as e:
                self.logger.warning(f"í†µê³„ì  ìœ ì˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")


# ì‚¬ì „ ì •ì˜ëœ ì‹¤í—˜ ì„¤ì •ë“¤
def get_phase3_experiment_configs() -> List[ExperimentConfig]:
    """Phase 3 ê°œì„ ì‚¬í•­ ì‹¤í—˜ ì„¤ì •ë“¤"""

    return [
        ExperimentConfig(
            name="baseline",
            description="ê¸°ì¡´ ì‹œìŠ¤í…œ (Phase 1,2 ê°œì„ ì‚¬í•­ë§Œ)",
            adaptive_entropy=False,
            adaptive_no_trade_band=True,
            use_simplex_projection=True,
            n_episodes=100,
        ),
        ExperimentConfig(
            name="adaptive_entropy_only",
            description="ì ì‘í˜• ì—”íŠ¸ë¡œí”¼ë§Œ í™œì„±í™”",
            adaptive_entropy=True,
            adaptive_no_trade_band=False,
            use_simplex_projection=True,
            n_episodes=100,
        ),
        ExperimentConfig(
            name="full_phase3",
            description="ëª¨ë“  Phase 3 ê°œì„ ì‚¬í•­ í™œì„±í™”",
            adaptive_entropy=True,
            adaptive_no_trade_band=True,
            use_simplex_projection=True,
            n_episodes=100,
        ),
        ExperimentConfig(
            name="legacy_system",
            description="ì›ë³¸ ì‹œìŠ¤í…œ (ê°œì„ ì‚¬í•­ ì—†ìŒ)",
            adaptive_entropy=False,
            adaptive_no_trade_band=False,
            use_simplex_projection=False,
            n_episodes=100,
        ),
    ]


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
def run_phase3_ab_test():
    """Phase 3 A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    # í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
    framework = ABTestingFramework()

    # ì‹¤í—˜ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    configs = get_phase3_experiment_configs()

    # ë¹„êµ ì—°êµ¬ ì‹¤í–‰ (ê° ì„¤ì •ë§ˆë‹¤ 3íšŒ ì‹¤í–‰)
    results = framework.run_comparative_study(configs, n_runs=3)

    # ê²°ê³¼ ë³´ê³ ì„œ ì¶œë ¥
    framework.print_comparison_report(results)

    return results


if __name__ == "__main__":
    results = run_phase3_ab_test()
