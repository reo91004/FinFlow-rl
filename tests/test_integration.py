#!/usr/bin/env python3
# tests/test_integration.py

"""
FinFlow-RL í†µí•© í…ŒìŠ¤íŠ¸
ì‹œìŠ¤í…œì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦
"""

import sys
import traceback
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))


def test_imports():
    """ëª¨ë“  í•µì‹¬ ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸"""
    print("1. Import í…ŒìŠ¤íŠ¸...")
    try:
        # Core modules
        from src.environments.portfolio_env import PortfolioEnv
        from src.environments.reward_functions import DifferentialSharpe, CVaRConstraint
        from src.data.replay_buffer import PrioritizedReplayBuffer
        from src.data.offline_dataset import OfflineDataset

        # Agents
        from src.algorithms.online.t_cell import TCell
        from src.algorithms.online.b_cell import BCell
        from src.algorithms.online.memory import MemoryCell
        from src.algorithms.offline.iql import IQLAgent

        # Data
        from src.data.market_loader import DataLoader
        from src.data.feature_extractor import FeatureExtractor

        # Utils
        from src.utils.logger import FinFlowLogger

        print("âœ“ ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ")
        return True
    except ImportError as e:
        print(f"âœ— Import ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False


def test_environment():
    """í™˜ê²½ ìƒì„± ë° ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("\n2. í™˜ê²½ í…ŒìŠ¤íŠ¸...")
    try:
        import numpy as np
        import pandas as pd
        from src.environments.portfolio_env import PortfolioEnv

        # Create dummy price data
        prices = pd.DataFrame(
            np.random.randn(100, 5).cumsum(axis=0) + 100,
            columns=["Asset1", "Asset2", "Asset3", "Asset4", "Asset5"],
        )

        # Create environment
        env = PortfolioEnv(price_data=prices, initial_capital=1000000, transaction_cost=0.001)

        # Reset
        state, info = env.reset()
        print(f"  - ìƒíƒœ ì°¨ì›: {state.shape}")

        # Step
        action = np.ones(5) / 5  # Equal weights
        next_state, reward, terminated, truncated, info = env.step(action)
        print(f"  - ë³´ìƒ: {reward:.4f}")
        print(f"  - í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜: ${info['portfolio_value']:,.2f}")

        print("âœ“ í™˜ê²½ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âœ— í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False


def test_agents():
    """ì—ì´ì „íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n3. ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸...")
    try:
        import torch
        import numpy as np
        from src.algorithms.online.t_cell import TCell
        from src.algorithms.online.b_cell import BCell
        from src.algorithms.online.memory import MemoryCell
        from src.algorithms.offline.iql import IQLAgent

        device = torch.device("cpu")
        state_dim = 43
        action_dim = 5

        # T-Cell
        t_cell = TCell(feature_dim=12)
        print("  - T-Cell ìƒì„± ì™„ë£Œ")

        # B-Cell
        b_cell = BCell(
            state_dim=state_dim,
            action_dim=action_dim,
            config={"gamma": 0.99},
            device=device,
        )
        print("  - B-Cell ìƒì„± ì™„ë£Œ")

        # Memory Cell
        memory_cell = MemoryCell(capacity=100)
        print("  - Memory Cell ìƒì„± ì™„ë£Œ")

        # Gating Network - ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
        # gating = GatingNetwork(state_dim=state_dim, num_experts=5)
        # print("  - Gating Network ìƒì„± ì™„ë£Œ")

        # IQL Agent
        iql = IQLAgent(state_dim=state_dim, action_dim=action_dim, device=device)
        print("  - IQL Agent ìƒì„± ì™„ë£Œ")

        # Test forward pass
        dummy_state = torch.randn(1, state_dim)
        dummy_crisis = 0.3
        dummy_guidance = {"has_guidance": False}

        # Gating decision - ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
        # decision = gating(dummy_state, dummy_guidance, dummy_crisis)
        # print(f"  - ì„ íƒëœ B-Cell: {decision.selected_bcell}")

        # B-Cell action
        action = b_cell.select_action(dummy_state, deterministic=True)
        print(f"  - ì•¡ì…˜ ì°¨ì›: {action.shape}")

        print("âœ“ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âœ— ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False


def test_mini_training():
    """ë¯¸ë‹ˆ í•™ìŠµ ë£¨í”„ í…ŒìŠ¤íŠ¸"""
    print("\n4. ë¯¸ë‹ˆ í•™ìŠµ í…ŒìŠ¤íŠ¸...")
    try:
        import torch
        import numpy as np
        import pandas as pd
        from src.environments.portfolio_env import PortfolioEnv
        from src.algorithms.offline.iql import IQLAgent

        # Setup
        device = torch.device("cpu")
        prices = pd.DataFrame(
            np.random.randn(100, 5).cumsum(axis=0) + 100, columns=[f"Asset{i+1}" for i in range(5)]
        )

        env = PortfolioEnv(price_data=prices)
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.shape[0]

        agent = IQLAgent(state_dim=state_dim, action_dim=action_dim, device=device)

        # Collect some data
        experiences = []
        state, _ = env.reset()

        for _ in range(10):
            action = np.random.dirichlet(np.ones(action_dim))
            next_state, reward, terminated, truncated, _ = env.step(action)

            experiences.append(
                {
                    "state": state,
                    "action": action,
                    "reward": reward,
                    "next_state": next_state,
                    "done": terminated or truncated,
                }
            )

            state = next_state
            if terminated or truncated:
                state, _ = env.reset()

        # Batch update
        if experiences:
            states = torch.FloatTensor([e["state"] for e in experiences])
            actions = torch.FloatTensor([e["action"] for e in experiences])
            rewards = torch.FloatTensor([e["reward"] for e in experiences]).unsqueeze(1)
            next_states = torch.FloatTensor([e["next_state"] for e in experiences])
            dones = torch.FloatTensor([e["done"] for e in experiences]).unsqueeze(1)

            losses = agent.update(states, actions, rewards, next_states, dones)
            print(f"  - Value Loss: {losses['value_loss']:.4f}")
            print(f"  - Q Loss: {losses['q_loss']:.4f}")
            print(f"  - Actor Loss: {losses['actor_loss']:.4f}")

        print("âœ“ ë¯¸ë‹ˆ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âœ— ë¯¸ë‹ˆ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 60)
    print("FinFlow-RL í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    results = []

    # Run tests
    results.append(("Imports", test_imports()))
    results.append(("Environment", test_environment()))
    results.append(("Agents", test_agents()))
    results.append(("Training", test_mini_training()))

    # Summary
    print("\n" + "=" * 60)
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    for test_name, passed in results:
        status = "âœ“ PASS" if passed else "âœ— FAIL"
        print(f"{test_name:20s}: {status}")

    total_passed = sum(1 for _, passed in results if passed)
    total_tests = len(results)

    print("-" * 60)
    print(f"ì´ {total_tests}ê°œ ì¤‘ {total_passed}ê°œ í†µê³¼")

    if total_passed == total_tests:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        return 0
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìœ„ ì˜¤ë¥˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
