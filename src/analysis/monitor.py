# src/analysis/monitor.py

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
import json
import datetime
import threading
import time
from collections import deque
from pathlib import Path

from src.utils.logger import FinFlowLogger
from src.analysis.metrics import MetricsCalculator
from sklearn.ensemble import IsolationForest
import torch
import torch.nn as nn

@dataclass
class Alert:
    """ì•Œë¦¼ ì •ë³´"""
    level: str  # 'info', 'warning', 'critical', 'emergency'
    metric: str
    value: float
    threshold: float
    message: str
    timestamp: str
    action_required: bool = False
    notification_sent: bool = False

class PerformanceMonitor:
    """
    ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•ˆì •ì„± ì¶”ì 
    
    - ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ
    - ê°•í™”ëœ ì•Œë¦¼ ì‹œìŠ¤í…œ
    - ìƒì„¸ ë©”íŠ¸ë¦­ ì¶”ì 
    - ê±°ë˜ ë¹„ìš© ë¶„ì„
    """
    
    def __init__(self, 
                 log_dir: str = "logs",
                 use_wandb: bool = False,
                 use_tensorboard: bool = True,
                 use_dashboard: bool = False,
                 dashboard_port: int = 8050,
                 wandb_config: Optional[Dict] = None,
                 alert_thresholds: Optional[Dict] = None,
                 notification_config: Optional[Dict] = None):
        """
        Args:
            log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬
            use_wandb: Wandb ì‚¬ìš© ì—¬ë¶€
            use_tensorboard: TensorBoard ì‚¬ìš© ì—¬ë¶€
            wandb_config: Wandb ì„¤ì • (project, entity ë“±)
            alert_thresholds: ì•Œë¦¼ ì„ê³„ê°’
        """
        self.logger = FinFlowLogger("PerformanceMonitor")
        self.metrics_calc = MetricsCalculator()
        
        # ëª¨ë‹ˆí„°ë§ ë°±ì—”ë“œ ì¡°ê±´ë¶€ í™œì„±í™”
        self.use_wandb = use_wandb
        self.use_tensorboard = use_tensorboard
        self.use_dashboard = use_dashboard
        self.dashboard_port = dashboard_port
        
        # ì•Œë¦¼ ì„¤ì •
        self.notification_config = notification_config or {}
        self.notification_handlers = self._setup_notification_handlers()
        
        if self.use_wandb:
            import wandb
            self.wandb = wandb
            wandb_config = wandb_config or {}
            wandb.init(
                project=wandb_config.get('wandb_project', 'finflow-rl'),
                entity=wandb_config.get('wandb_entity'),
                tags=wandb_config.get('wandb_tags', []),
                dir=log_dir
            )
            self.logger.info("Wandb ì´ˆê¸°í™” ì™„ë£Œ")
        
        if self.use_tensorboard:
            from tensorboardX import SummaryWriter
            self.writer = SummaryWriter(log_dir)
            self.logger.info(f"TensorBoard ì´ˆê¸°í™” ì™„ë£Œ: {log_dir}")
        
        # ì•Œë¦¼ ì„ê³„ê°’ (4ë‹¨ê³„)
        self.alert_thresholds = alert_thresholds or {
            'sharpe_ratio': {'info': 2.0, 'warning': 1.0, 'critical': 0.5, 'emergency': 0},
            'max_drawdown': {'info': -0.10, 'warning': -0.15, 'critical': -0.25, 'emergency': -0.40},
            'cvar_95': {'info': -0.02, 'warning': -0.03, 'critical': -0.05, 'emergency': -0.10},
            'volatility': {'info': 0.15, 'warning': 0.20, 'critical': 0.30, 'emergency': 0.50},
            'turnover': {'info': 0.1, 'warning': 0.3, 'critical': 0.5, 'emergency': 0.8},
            'concentration': {'info': 0.3, 'warning': 0.5, 'critical': 0.7, 'emergency': 0.9}
        }
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬
        self.history = []
        self.alerts = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ì•Œë¦¼ë§Œ ìœ ì§€
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ë²„í¼
        self.realtime_buffer = {
            'timestamps': deque(maxlen=1000),
            'portfolio_value': deque(maxlen=1000),
            'returns': deque(maxlen=1000),
            'positions': deque(maxlen=1000),
            'trades': deque(maxlen=100)
        }
        
        # ê±°ë˜ ë¹„ìš© ì¶”ì 
        self.cost_tracker = {
            'transaction_costs': [],
            'slippage_costs': [],
            'market_impact_costs': [],
            'total_costs': 0
        }
        
        # ì´ìƒ ê°ì§€ ëª¨ë¸
        self.anomaly_detector = IsolationForest(
            contamination=0.1,
            n_estimators=100,
            random_state=42
        )
        self.anomaly_buffer = deque(maxlen=1000)
        self.anomaly_fitted = False
        
        # ìë™ ê°œì… ì„¤ì •
        self.auto_intervention = {
            'enabled': True,
            'intervention_count': 0,
            'last_intervention': None,
            'cooldown_steps': 100
        }
        
        # ëŒ€ì‹œë³´ë“œ ì‹œì‘
        if self.use_dashboard:
            self._start_dashboard()
        
    def log_metrics(self, metrics: Dict[str, float], step: int):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history.append({'step': step, **metrics})
        
        # Wandb ë¡œê¹…
        if self.use_wandb:
            self.wandb.log(metrics, step=step)
        
        # TensorBoard ë¡œê¹…
        if self.use_tensorboard:
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    self.writer.add_scalar(key, value, step)
        
        # ì´ìƒ ê°ì§€ ë° ìë™ ê°œì…
        anomalies = self._detect_anomalies(metrics)
        if anomalies:
            interventions = self._auto_intervene(metrics, anomalies)
            if interventions:
                metrics['interventions'] = interventions
        
        # ì•Œë¦¼ ì²´í¬
        self._check_alerts(metrics, step)
    
    def log_portfolio(self, weights: np.ndarray, asset_names: List[str], step: int):
        """í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ë¡œê¹…"""
        portfolio_dict = {f"portfolio/{name}": w for name, w in zip(asset_names, weights)}
        
        if self.use_wandb:
            self.wandb.log(portfolio_dict, step=step)
        
        if self.use_tensorboard:
            for name, weight in portfolio_dict.items():
                self.writer.add_scalar(name, weight, step)
    
    def log_gradients(self, model: Any, step: int):
        """ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„ ë¡œê¹…"""
        grad_stats = self._compute_gradient_stats(model)
        
        if self.use_wandb:
            self.wandb.log({f"gradients/{k}": v for k, v in grad_stats.items()}, step=step)
        
        if self.use_tensorboard:
            for key, value in grad_stats.items():
                self.writer.add_scalar(f"gradients/{key}", value, step)
    
    def _compute_gradient_stats(self, model: Any) -> Dict[str, float]:
        """ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„ ê³„ì‚°"""
        total_norm = 0
        grad_norms = []
        
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2).item()
                total_norm += param_norm ** 2
                grad_norms.append(param_norm)
        
        total_norm = total_norm ** 0.5
        
        return {
            'norm': total_norm,
            'max': max(grad_norms) if grad_norms else 0,
            'min': min(grad_norms) if grad_norms else 0,
            'mean': np.mean(grad_norms) if grad_norms else 0
        }
    
    def _detect_anomalies(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ ê°ì§€"""
        anomalies = {}
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        feature_keys = ['sharpe_ratio', 'volatility', 'max_drawdown', 'turnover']
        features = []
        for key in feature_keys:
            if key in metrics:
                features.append(metrics[key])
            else:
                features.append(0.0)
        
        # ë²„í¼ì— ì¶”ê°€
        self.anomaly_buffer.append(features)
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ ëª¨ë¸ í•™ìŠµ/ì˜ˆì¸¡
        if len(self.anomaly_buffer) >= 100:
            if not self.anomaly_fitted:
                # ì´ˆê¸° í•™ìŠµ
                self.anomaly_detector.fit(list(self.anomaly_buffer))
                self.anomaly_fitted = True
            
            # ì´ìƒì¹˜ ì˜ˆì¸¡
            score = self.anomaly_detector.decision_function([features])[0]
            is_anomaly = self.anomaly_detector.predict([features])[0] == -1
            
            if is_anomaly:
                anomalies['ml_detection'] = {
                    'score': score,
                    'features': dict(zip(feature_keys, features)),
                    'severity': 'high' if score < -0.5 else 'medium'
                }
        
        # í†µê³„ì  ì´ìƒì¹˜ ê²€ì¶œ
        for key, value in metrics.items():
            if key in self.history and len(self.history) > 30:
                recent_values = [h.get(key, 0) for h in self.history[-30:]]
                mean = np.mean(recent_values)
                std = np.std(recent_values)
                
                if std > 1e-8:
                    z_score = abs(value - mean) / std
                    if z_score > 3:
                        anomalies[f'{key}_statistical'] = {
                            'z_score': z_score,
                            'value': value,
                            'mean': mean,
                            'std': std
                        }
        
        return anomalies
    
    def _auto_intervene(self, metrics: Dict[str, float], anomalies: Dict[str, Any]) -> List[str]:
        """ìë™ ê°œì… ì‹œìŠ¤í…œ"""
        interventions = []
        
        if not self.auto_intervention['enabled']:
            return interventions
        
        # ì¿¨ë‹¤ìš´ ì²´í¬
        if self.auto_intervention['last_intervention']:
            steps_since = metrics.get('step', 0) - self.auto_intervention['last_intervention']
            if steps_since < self.auto_intervention['cooldown_steps']:
                return interventions
        
        # Q-value í­ë°œ ì²´í¬
        if 'q_value' in metrics:
            q_value = metrics['q_value']
            if q_value > 100:
                interventions.append('reduce_learning_rate')
                self.logger.warning(f"Q-value í­ë°œ ê°ì§€: {q_value:.2f}, í•™ìŠµë¥  ê°ì†Œ")
            elif q_value < -100:
                interventions.append('reset_q_network')
                self.logger.warning(f"Q-value ë¶•ê´´ ê°ì§€: {q_value:.2f}, Q-network ì¬ì´ˆê¸°í™”")
        
        # ì—”íŠ¸ë¡œí”¼ ë¶•ê´´ ì²´í¬
        if 'entropy' in metrics and metrics['entropy'] < 0.1:
            interventions.append('increase_exploration')
            self.logger.warning(f"ì—”íŠ¸ë¡œí”¼ ë¶•ê´´: {metrics['entropy']:.3f}, íƒìƒ‰ ì¦ê°€")
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ì²´í¬
        if 'gradient_norm' in metrics and metrics['gradient_norm'] > 10:
            interventions.append('clip_gradients')
            self.logger.warning(f"ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ: {metrics['gradient_norm']:.2f}, í´ë¦¬í•‘ ì ìš©")
        
        # ë³´ìƒ í´ë¦¬í”„ ì²´í¬
        if 'reward' in metrics and len(self.history) > 10:
            recent_rewards = [h.get('reward', 0) for h in self.history[-10:]]
            if metrics['reward'] < np.mean(recent_rewards) * 0.5:
                interventions.append('adjust_reward_scale')
                self.logger.warning(f"ë³´ìƒ í´ë¦¬í”„ ê°ì§€, ë³´ìƒ ìŠ¤ì¼€ì¼ ì¡°ì •")
        
        # ML ê¸°ë°˜ ì´ìƒì¹˜ì— ëŒ€í•œ ê°œì…
        if 'ml_detection' in anomalies and anomalies['ml_detection']['severity'] == 'high':
            interventions.append('reduce_update_frequency')
            self.logger.warning("ML ëª¨ë¸ì´ ì‹¬ê°í•œ ì´ìƒ íŒ¨í„´ ê°ì§€, ì—…ë°ì´íŠ¸ ë¹ˆë„ ê°ì†Œ")
        
        # ë‹¤ì¤‘ ë¬¸ì œ ë°œìƒ ì‹œ ì²´í¬í¬ì¸íŠ¸ ë¡¤ë°±
        if len(interventions) >= 3:
            interventions = ['rollback_checkpoint']
            self.logger.error("ë‹¤ì¤‘ ë¬¸ì œ ê°ì§€, ì²´í¬í¬ì¸íŠ¸ ë¡¤ë°±")
        
        # ê°œì… ê¸°ë¡
        if interventions:
            self.auto_intervention['intervention_count'] += 1
            self.auto_intervention['last_intervention'] = metrics.get('step', 0)
            self.logger.info(f"ìë™ ê°œì… ìˆ˜í–‰: {interventions}")
        
        return interventions
    
    def _check_alerts(self, metrics: Dict[str, float], step: int):
        """ê°•í™”ëœ ë‹¤ë‹¨ê³„ ì•Œë¦¼ ì²´í¬"""
        
        for metric_name, thresholds in self.alert_thresholds.items():
            if metric_name not in metrics:
                continue
                
            value = metrics[metric_name]
            alert_level = None
            threshold_value = None
            
            # ë©”íŠ¸ë¦­ íƒ€ì…ë³„ ë¹„êµ ë°©í–¥ ê²°ì •
            is_lower_bad = metric_name in ['sharpe_ratio', 'cvar_95']
            is_higher_bad = metric_name in ['volatility', 'turnover', 'concentration', 'max_drawdown']
            
            # ë ˆë²¨ë³„ ì²´í¬ (emergency -> critical -> warning -> info)
            for level in ['emergency', 'critical', 'warning', 'info']:
                if level not in thresholds:
                    continue
                    
                threshold = thresholds[level]
                
                if is_lower_bad and value < threshold:
                    alert_level = level
                    threshold_value = threshold
                    break
                elif is_higher_bad:
                    # max_drawdownì€ ìŒìˆ˜ì´ë¯€ë¡œ íŠ¹ë³„ ì²˜ë¦¬
                    if metric_name == 'max_drawdown' and value < threshold:
                        alert_level = level
                        threshold_value = threshold
                        break
                    elif metric_name != 'max_drawdown' and value > threshold:
                        alert_level = level
                        threshold_value = threshold
                        break
            
            if alert_level:
                # ì•Œë¦¼ ìƒì„±
                alert = Alert(
                    level=alert_level,
                    metric=metric_name,
                    value=value,
                    threshold=threshold_value,
                    message=self._format_alert_message(metric_name, value, alert_level),
                    timestamp=datetime.datetime.now().isoformat(),
                    action_required=(alert_level in ['critical', 'emergency'])
                )
                
                self.alerts.append(alert)
                
                # ë¡œê¹…
                if alert_level == 'emergency':
                    self.logger.critical(f"[EMERGENCY] {alert.message}")
                elif alert_level == 'critical':
                    self.logger.critical(alert.message)
                elif alert_level == 'warning':
                    self.logger.warning(alert.message)
                else:
                    self.logger.info(alert.message)
                
                # ì•Œë¦¼ ì „ì†¡
                self._send_notification(alert)
    
    def get_stability_report(self) -> Dict:
        """ì•ˆì •ì„± ë¦¬í¬íŠ¸ ìƒì„±"""
        if len(self.history) < 10:
            return {'status': 'insufficient_data'}
        
        recent_metrics = pd.DataFrame(self.history[-100:])
        
        # ì•ˆì •ì„± ë©”íŠ¸ë¦­ ê³„ì‚°
        stability_metrics = {}
        
        for col in recent_metrics.columns:
            if col != 'step' and recent_metrics[col].dtype in [np.float64, np.float32, np.int64]:
                values = recent_metrics[col].values
                stability_metrics[col] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'trend': float(np.polyfit(range(len(values)), values, 1)[0]),
                    'volatility': float(np.std(values) / (np.mean(values) + 1e-8))
                }
        
        # ì „ì²´ ì•ˆì •ì„± ì ìˆ˜
        volatilities = [m['volatility'] for m in stability_metrics.values()]
        avg_volatility = np.mean(volatilities)
        stability_score = max(0, min(100, 100 * (1 - avg_volatility)))
        
        return {
            'stability_score': stability_score,
            'metrics': stability_metrics,
            'recent_alerts': [vars(a) for a in self.alerts[-10:]],
            'status': 'stable' if stability_score > 70 else 'unstable'
        }
    
    def save_history(self, path: str):
        """íˆìŠ¤í† ë¦¬ ì €ì¥"""
        pd.DataFrame(self.history).to_csv(path, index=False)
        self.logger.info(f"íˆìŠ¤í† ë¦¬ ì €ì¥: {path}")
    
    def _setup_notification_handlers(self) -> Dict[str, Callable]:
        """ì•Œë¦¼ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        handlers = {}
        
        # ì´ë©”ì¼ í•¸ë“¤ëŸ¬
        if self.notification_config.get('email_enabled'):
            handlers['email'] = self._send_email_notification
        
        # Slack í•¸ë“¤ëŸ¬
        if self.notification_config.get('slack_enabled'):
            handlers['slack'] = self._send_slack_notification
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬ (ê¸°ë³¸)
        handlers['console'] = self._send_console_notification
        
        return handlers
    
    def _format_alert_message(self, metric: str, value: float, level: str) -> str:
        """ì•Œë¦¼ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        emoji_map = {
            'info': 'â„¹ï¸',
            'warning': 'âš ï¸',
            'critical': 'ğŸš¨',
            'emergency': 'ğŸ†˜'
        }
        
        # ì—°êµ¬ìš© ì½”ë“œì´ë¯€ë¡œ ì´ëª¨ì§€ ì œê±°
        return f"[{level.upper()}] {metric}: {value:.4f}"
    
    def _send_notification(self, alert: Alert):
        """ì•Œë¦¼ ì „ì†¡"""
        # ë ˆë²¨ë³„ í•„í„°ë§
        min_level = self.notification_config.get('min_notification_level', 'warning')
        level_priority = {'info': 0, 'warning': 1, 'critical': 2, 'emergency': 3}
        
        if level_priority.get(alert.level, 0) < level_priority.get(min_level, 1):
            return
        
        # ê° í•¸ë“¤ëŸ¬ë¡œ ì „ì†¡
        for handler_name, handler_func in self.notification_handlers.items():
            handler_func(alert)
        
        alert.notification_sent = True
    
    def _send_console_notification(self, alert: Alert):
        """ì½˜ì†” ì•Œë¦¼"""
        print(f"\n{'='*60}")
        print(f"ALERT: {alert.level.upper()}")
        print(f"Metric: {alert.metric}")
        print(f"Value: {alert.value:.4f}")
        print(f"Threshold: {alert.threshold:.4f}")
        print(f"Time: {alert.timestamp}")
        print(f"Action Required: {alert.action_required}")
        print(f"{'='*60}\n")
    
    def _send_email_notification(self, alert: Alert):
        """ì´ë©”ì¼ ì•Œë¦¼ (êµ¬í˜„ ì˜ˆì‹œ)"""
        # ì‹¤ì œ êµ¬í˜„ ì‹œ SMTP ì„¤ì • í•„ìš”
        pass
    
    def _send_slack_notification(self, alert: Alert):
        """Slack ì•Œë¦¼ (êµ¬í˜„ ì˜ˆì‹œ)"""
        # ì‹¤ì œ êµ¬í˜„ ì‹œ Slack Webhook URL í•„ìš”
        pass
    
    def log_trade(self, trade: Dict[str, Any]):
        """ê±°ë˜ ë¡œê¹… ë° ë¹„ìš© ì¶”ì """
        self.realtime_buffer['trades'].append(trade)
        
        # ë¹„ìš© ì¶”ì 
        if 'costs' in trade:
            costs = trade['costs']
            if 'transaction_cost' in costs:
                self.cost_tracker['transaction_costs'].append(costs['transaction_cost'])
            if 'slippage_cost' in costs:
                self.cost_tracker['slippage_costs'].append(costs['slippage_cost'])
            if 'market_impact_cost' in costs:
                self.cost_tracker['market_impact_costs'].append(costs['market_impact_cost'])
            
            total_cost = sum(costs.values())
            self.cost_tracker['total_costs'] += total_cost
    
    def update_realtime(self, portfolio_value: float, returns: float, positions: np.ndarray):
        """ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        timestamp = datetime.datetime.now()
        
        self.realtime_buffer['timestamps'].append(timestamp)
        self.realtime_buffer['portfolio_value'].append(portfolio_value)
        self.realtime_buffer['returns'].append(returns)
        self.realtime_buffer['positions'].append(positions.copy())
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ê³„ì‚°
        if len(self.realtime_buffer['returns']) > 20:
            recent_returns = list(self.realtime_buffer['returns'])[-20:]
            realtime_metrics = {
                'realtime_sharpe': np.mean(recent_returns) / (np.std(recent_returns) + 1e-8) * np.sqrt(252),
                'realtime_volatility': np.std(recent_returns) * np.sqrt(252),
                'realtime_drawdown': self._calculate_realtime_drawdown()
            }
            
            # ì‹¤ì‹œê°„ ì•Œë¦¼ ì²´í¬
            self._check_alerts(realtime_metrics, len(self.history))
    
    def _calculate_realtime_drawdown(self) -> float:
        """ì‹¤ì‹œê°„ ë“œë¡œë‹¤ìš´ ê³„ì‚°"""
        if len(self.realtime_buffer['portfolio_value']) < 2:
            return 0
        
        values = np.array(list(self.realtime_buffer['portfolio_value']))
        running_max = np.maximum.accumulate(values)
        drawdown = (values - running_max) / running_max
        return float(np.min(drawdown))
    
    def get_cost_analysis(self) -> Dict[str, Any]:
        """ê±°ë˜ ë¹„ìš© ë¶„ì„"""
        if not self.cost_tracker['transaction_costs']:
            return {'total_costs': 0, 'cost_breakdown': {}}
        
        return {
            'total_costs': self.cost_tracker['total_costs'],
            'cost_breakdown': {
                'transaction_costs': sum(self.cost_tracker['transaction_costs']),
                'slippage_costs': sum(self.cost_tracker['slippage_costs']),
                'market_impact_costs': sum(self.cost_tracker['market_impact_costs'])
            },
            'avg_costs': {
                'transaction': np.mean(self.cost_tracker['transaction_costs']),
                'slippage': np.mean(self.cost_tracker['slippage_costs']),
                'market_impact': np.mean(self.cost_tracker['market_impact_costs'])
            },
            'cost_per_trade': self.cost_tracker['total_costs'] / len(self.realtime_buffer['trades']) if self.realtime_buffer['trades'] else 0
        }
    
    def _start_dashboard(self):
        """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì‹œì‘"""
        def run_dashboard():
            # Dash ëŒ€ì‹œë³´ë“œ êµ¬í˜„ (ì„ íƒì )
            self.logger.info(f"ëŒ€ì‹œë³´ë“œ ì‹œì‘: http://localhost:{self.dashboard_port}")
            # ì‹¤ì œ êµ¬í˜„ ì‹œ Dash/Plotly ì‚¬ìš©
        
        dashboard_thread = threading.Thread(target=run_dashboard, daemon=True)
        dashboard_thread.start()
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ë°˜í™˜"""
        return {
            'portfolio_value': list(self.realtime_buffer['portfolio_value']),
            'returns': list(self.realtime_buffer['returns']),
            'positions': list(self.realtime_buffer['positions']),
            'timestamps': [t.isoformat() for t in self.realtime_buffer['timestamps']],
            'recent_alerts': [vars(a) for a in list(self.alerts)[-10:]],
            'cost_analysis': self.get_cost_analysis(),
            'stability_report': self.get_stability_report()
        }
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.use_tensorboard:
            self.writer.close()
        if self.use_wandb:
            self.wandb.finish()
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        final_report = {
            'total_alerts': len(self.alerts),
            'alert_breakdown': self._get_alert_breakdown(),
            'cost_analysis': self.get_cost_analysis(),
            'final_metrics': self.history[-1] if self.history else {}
        }
        
        report_path = Path('logs') / f"monitor_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(final_report, f, indent=2, default=str)
        
        self.logger.info(f"ìµœì¢… ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    def _get_alert_breakdown(self) -> Dict[str, int]:
        """ì•Œë¦¼ ë¶„ë¥˜ë³„ ì§‘ê³„"""
        breakdown = {'info': 0, 'warning': 0, 'critical': 0, 'emergency': 0}
        for alert in self.alerts:
            breakdown[alert.level] = breakdown.get(alert.level, 0) + 1
        return breakdown