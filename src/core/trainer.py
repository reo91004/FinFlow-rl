# src/core/trainer.py

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from collections import defaultdict, deque
import json
import time
import random
from pathlib import Path
from tqdm import tqdm
import pandas as pd
import yaml

from src.core.env import PortfolioEnv
from src.core.replay import PrioritizedReplayBuffer
from src.core.offline_dataset import OfflineDataset
from src.core.iql import IQLAgent
# from src.core.sac import DistributionalSAC  # ì‚¬ìš© ì‹œ import
from src.agents.b_cell import BCell
from src.agents.t_cell import TCell
from src.agents.memory import MemoryCell
from src.agents.gating import GatingNetwork
from src.analysis.monitor import PerformanceMonitor
from src.analysis.visualization import plot_equity_curve, plot_drawdown, plot_portfolio_weights
from src.utils.monitoring import StabilityMonitor
from src.utils.logger import FinFlowLogger, get_session_directory
from src.utils.seed import set_seed, get_device_info

@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • - YAML íŒŒì¼ ê¸°ë°˜"""
    
    # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    config_path: str = "configs/default.yaml"
    override_params: Optional[Dict] = None
    
    # YAMLì—ì„œ ë¡œë“œë  ì†ì„±ë“¤ (ì´ˆê¸°ê°’ì€ None)
    env_config: Dict = field(default_factory=dict)
    data_config: Dict = field(default_factory=dict)
    train_config: Dict = field(default_factory=dict)
    
    # ê°œë³„ ì†ì„±ë“¤ (YAMLì—ì„œ ë¡œë“œë¨)
    offline_episodes: int = 500
    offline_steps: int = 200000
    offline_batch_size: int = 512
    offline_eval_interval: int = 10000
    
    iql_epochs: int = 100
    iql_batch_size: int = 256
    iql_lr: float = 3e-4
    iql_expectile: float = 0.7
    iql_temperature: float = 3.0
    
    sac_episodes: int = 1000
    sac_batch_size: int = 256
    sac_lr: float = 3e-4
    sac_gamma: float = 0.99
    sac_tau: float = 0.005
    sac_alpha: float = 0.2
    sac_cql_weight: float = 1.0
    
    memory_capacity: int = 50000
    memory_k_neighbors: int = 5
    
    eval_interval: int = 10
    checkpoint_interval: int = 50
    log_interval: int = 1
    
    device: str = "auto"
    seed: int = 42
    
    data_path: str = "data/processed"
    checkpoint_dir: str = "checkpoints"
    
    target_sharpe: float = 1.5
    target_cvar: float = -0.02
    
    patience: int = 50
    min_improvement: float = 0.01
    
    monitoring_config: Optional[Dict] = None
    
    def __post_init__(self):
        """YAML íŒŒì¼ì—ì„œ ì„¤ì • ìë™ ë¡œë“œ"""
        # YAML íŒŒì¼ ë¡œë“œ
        if Path(self.config_path).exists():
            with open(self.config_path, 'r') as f:
                yaml_config = yaml.safe_load(f)
            
            # ì„¤ì • ë§¤í•‘
            self._load_from_yaml(yaml_config)
        
        # CLI ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        if self.override_params:
            self._apply_overrides(self.override_params)
            
        # ë¡œê±° ìƒì„± (ì„¤ì • ë¡œë“œ í›„)
        self.logger = FinFlowLogger("TrainingConfig")
        self.logger.info(f"ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.config_path}")
        self.logger.info(f"  ì˜¤í”„ë¼ì¸ ì—í”¼ì†Œë“œ: {self.offline_episodes}")
        self.logger.info(f"  IQL ì—í­: {self.iql_epochs}")
        self.logger.info(f"  SAC ì—í”¼ì†Œë“œ: {self.sac_episodes}")
    
    def _load_from_yaml(self, config: Dict):
        """YAML ì„¤ì •ì„ ì†ì„±ìœ¼ë¡œ ë³€í™˜"""
        # í™˜ê²½ ì„¤ì •
        env = config.get('env', {})
        self.env_config = {
            'initial_balance': env.get('initial_capital', 1000000),
            'turnover_cost': env.get('turnover_cost', 0.001),
            'slip_coeff': env.get('slip_coeff', 0.0005),
            'max_weight': env.get('max_weight', 0.2),
            'min_weight': env.get('min_weight', 0.0),
            'window_size': config.get('features', {}).get('window', 30),
            'max_weight_change': env.get('max_turnover', 0.5)
        }
        
        # ë°ì´í„° ì„¤ì •
        data = config.get('data', {})
        self.data_config = {
            'tickers': data.get('symbols'),
            'symbols': data.get('symbols'),  # í˜¸í™˜ì„±ì„ ìœ„í•´ ë‘˜ ë‹¤ ì €ì¥
            'start': data.get('start', '2008-01-01'),
            'end': data.get('end', '2020-12-31'),
            'test_start': data.get('test_start', '2021-01-01'),
            'test_end': data.get('test_end', '2024-12-31'),
            'cache_dir': data.get('cache_dir', 'data/cache'),
            'interval': data.get('interval', '1d'),
            'auto_download': True,
            'use_cache': True
        }
        
        # í•™ìŠµ ì„¤ì •
        train = config.get('train', {})
        self.offline_episodes = train.get('offline_episodes', 500)
        self.offline_steps = train.get('offline_steps', 200000)
        self.offline_batch_size = train.get('offline_batch_size', 512)
        self.offline_eval_interval = train.get('offline_eval_interval', 10000)
        
        self.train_config = {
            'offline_episodes': self.offline_episodes,
            'offline_steps': self.offline_steps,
            'offline_batch_size': self.offline_batch_size,
            'offline_eval_interval': self.offline_eval_interval
        }
        
        # IQL ì„¤ì •
        bcell = config.get('bcell', {})
        self.iql_expectile = bcell.get('iql_expectile', 0.7)
        self.iql_temperature = bcell.get('iql_temperature', 3.0)
        self.iql_lr = bcell.get('critic_lr', 3e-4)
        self.iql_batch_size = train.get('offline_batch_size', 256)
        self.iql_epochs = train.get('offline_steps', 200000) // 1000  # stepsë¥¼ epochsë¡œ ë³€í™˜
        
        # SAC ì„¤ì •
        self.sac_lr = bcell.get('actor_lr', 3e-4)
        self.sac_gamma = bcell.get('gamma', 0.99)
        self.sac_tau = bcell.get('tau', 0.005)
        self.sac_alpha = bcell.get('alpha_init', 0.2)
        self.sac_cql_weight = bcell.get('cql_alpha_start', 0.01)
        self.sac_batch_size = train.get('online_batch_size', 256)
        self.sac_episodes = train.get('online_steps', 300000) // 300  # stepsë¥¼ episodesë¡œ ë³€í™˜
        
        # Memory ì„¤ì •
        memory = config.get('memory', {})
        self.memory_capacity = train.get('buffer_size', 100000)
        self.memory_k_neighbors = memory.get('k_neighbors', 5)
        
        # í‰ê°€ ì„¤ì •
        self.eval_interval = train.get('eval_interval', 5000) // 500  # stepsë¥¼ episodesë¡œ ë³€í™˜
        self.checkpoint_interval = train.get('save_interval', 20000) // 400
        self.log_interval = train.get('log_interval', 100) // 100
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        system = config.get('system', {})
        self.device = system.get('device', config.get('device', 'auto'))
        self.seed = system.get('seed', config.get('seed', 42))
        self.data_path = system.get('data_path', 'data/processed')
        self.checkpoint_dir = system.get('checkpoint_dir', 'checkpoints')
        
        # ëª©í‘œ ì§€í‘œ
        objectives = config.get('objectives', {})
        self.target_sharpe = objectives.get('sharpe_target', 1.5)
        self.target_cvar = objectives.get('cvar_target', -0.02)
        
        # ì¡°ê¸° ì¢…ë£Œ
        self.patience = train.get('early_stop_patience', 50000) // 1000
        self.min_improvement = train.get('early_stop_min_delta', 0.001)
        
        # ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self.monitoring_config = config.get('monitoring', {})
        
        # ì „ì²´ config ì €ì¥
        self._raw_config = config
    
    def _apply_overrides(self, overrides: Dict):
        """CLI ì¸ìë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ"""
        for key, value in overrides.items():
            if value is not None:
                if hasattr(self, key):
                    setattr(self, key, value)
                    if hasattr(self, 'logger'):
                        self.logger.info(f"ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ: {key} = {value}")
    
    def get(self, key: str, default: Any = None) -> Any:
        """ì„¤ì • ê°’ ê°€ì ¸ì˜¤ê¸° (dict-like ì¸í„°í˜ì´ìŠ¤)"""
        return getattr(self, key, default)
    
    def to_dict(self) -> Dict:
        """ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {k: v for k, v in self.__dict__.items() 
                if not k.startswith('_') and k != 'logger'}


class FinFlowTrainer:
    """
    FinFlow í†µí•© í•™ìŠµ ê´€ë¦¬ì
    
    IQL ì‚¬ì „í•™ìŠµ â†’ SAC ë¯¸ì„¸ì¡°ì • íŒŒì´í”„ë¼ì¸
    T-Cell, B-Cell, Memory, Gating í†µí•© ê´€ë¦¬
    """
    
    def __init__(self, config: TrainingConfig):
        """
        Args:
            config: í•™ìŠµ ì„¤ì •
        """
        self.config = config
        self.logger = FinFlowLogger("Trainer")
        
        # Set seed for reproducibility
        set_seed(config.seed)
        
        # Device setup
        if config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(config.device)
        
        self.logger.info(f"ë””ë°”ì´ìŠ¤ ì„¤ì •: {get_device_info(self.device)}")
        
        # Create directories
        self.session_dir = Path(get_session_directory())
        self.log_dir = str(self.session_dir)
        self.run_dir = self.session_dir
        self.checkpoint_dir = self.session_dir / "models"
        self.checkpoint_dir.mkdir(exist_ok=True)
        (self.run_dir / "alerts").mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self._initialize_components()
        
        # Training state
        self.global_step = 0
        self.episode = 0
        self.best_sharpe = -float('inf')
        self.patience_counter = 0

        # ë¬´ì²´ê²° ê°ì§€ë¥¼ ìœ„í•œ ì¹´ìš´í„°
        self.zero_return_count = 0
        self.zero_return_threshold = 50  # K=50 ì—°ì† ë¬´ì²´ê²° ì‹œ ê²½ê³ 
        
        # Metrics tracking
        self.metrics_history = []
        
        # ì•ŒëŒ ì‹œê°í™” ì¿¨ë‹¤ìš´ ì„¤ì •
        self.last_visualization_step = 0
        self.visualization_cooldown = 1000  # ìµœì†Œ 1000 step ê°„ê²©

        # ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„í™” (ì´ˆê¸° í•™ìŠµ ì•ˆì •í™”)
        if hasattr(self, 'monitoring'):
            self.monitoring.rollback_on_divergence = False  # ì´ˆê¸°ì—ëŠ” rollback ë¹„í™œì„±í™”
            self.monitoring.intervention_threshold = 6.0  # ê¸°ì¤€ ì™„í™”

        self.logger.info("FinFlow Trainer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        from src.data.loader import DataLoader
        from src.data.features import FeatureExtractor
        # import pandas as pd  # ì‚¬ìš© ì‹œ import
        
        self.logger.info("ì‹¤ì œ ì‹œì¥ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        # DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        data_loader = DataLoader(cache_dir="data/cache")
        
        # ì„¤ì •ì—ì„œ í‹°ì»¤ ê°€ì ¸ì˜¤ê¸° (configì—ì„œ ì½ê¸°)
        tickers = self.config.data_config.get('tickers')
        
        # configì—ì„œ ë‚ ì§œ ì½ê¸°
        config_data = self.config.data_config
        market_data = data_loader.get_market_data(
            symbols=tickers,  # ëª¨ë“  í‹°ì»¤ ì‚¬ìš©
            train_start=config_data.get('start', '2008-01-01'),
            train_end=config_data.get('end', '2020-12-31'),
            test_start=config_data.get('test_start', '2021-01-01'),
            test_end=config_data.get('test_end', '2024-12-31')
        )
        
        # í•™ìŠµ ë°ì´í„° ì„ íƒ
        price_data = market_data['train_data']
        
        if price_data.empty:
            raise ValueError("ì‹œì¥ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        self.logger.info(f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(price_data)} ì¼, {len(price_data.columns)} ìì‚°")
        
        # íŠ¹ì„± ì¶”ì¶œê¸° (config ì „ë‹¬)
        feature_config = config_data.get('features', {}) if 'features' in locals() else {}
        self.feature_extractor = FeatureExtractor(
            window=self.config.env_config.get('window_size', 30),
            feature_config=feature_config
        )
        
        # Environment ìƒì„±
        self.env = PortfolioEnv(
            price_data=price_data,
            feature_extractor=self.feature_extractor,
            initial_capital=self.config.env_config.get('initial_capital', 1000000),
            turnover_cost=self.config.env_config.get('turnover_cost', 0.001),
            slip_coeff=self.config.env_config.get('slip_coeff', 0.0005),
            no_trade_band=self.config.env_config.get('no_trade_band', 0.002),
            max_leverage=self.config.env_config.get('max_leverage', 1.0),
            max_turnover=self.config.env_config.get('max_turnover', 0.5)
        )
        
        # Get dimensions
        obs = self.env.reset()[0]
        self.state_dim = len(obs)
        self.action_dim = self.env.action_space.shape[0]
        
        self.logger.info(f"í™˜ê²½ ì´ˆê¸°í™”: state_dim={self.state_dim}, action_dim={self.action_dim}")
        
        # T-Cell (Crisis Detection) - config ê¸°ë°˜
        feature_config = self.config.data_config.get('features', {}) if hasattr(self.config, 'data_config') else {}
        self.t_cell = TCell(
            feature_dim=None,  # configì—ì„œ ìë™ ê³„ì‚°
            contamination=0.1,
            n_estimators=100,
            window_size=30,
            feature_config=feature_config
        )
        
        # Memory Cell
        self.memory_cell = MemoryCell(
            capacity=self.config.memory_capacity,
            embedding_dim=32,
            k_neighbors=self.config.memory_k_neighbors
        )
        
        # Gating Network
        self.gating_network = GatingNetwork(
            state_dim=self.state_dim,
            hidden_dim=256,
            num_experts=5
        ).to(self.device)
        
        # B-Cell (Main Agent) - ê¸°ë³¸ ì „ëµìœ¼ë¡œ ì´ˆê¸°í™”
        bcell_config = {
            'actor_hidden': [256, 256],
            'critic_hidden': [256, 256],
            'actor_lr': self.config.sac_lr,
            'critic_lr': self.config.sac_lr,
            'gamma': self.config.sac_gamma,
            'tau': self.config.sac_tau,
            'alpha': self.config.sac_alpha,
            'cql_weight': self.config.sac_cql_weight,
            'n_quantiles': 32
        }
        
        # ì—¬ëŸ¬ B-Cell ì „ëµ ì´ˆê¸°í™”
        self.b_cells = {}
        for specialization in ['volatility', 'correlation', 'momentum', 'defensive', 'growth']:
            self.b_cells[specialization] = BCell(
                specialization=specialization,
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                config=bcell_config,
                device=self.device
            )
        
        # ê¸°ë³¸ B-Cell ì„ íƒ
        self.b_cell = self.b_cells['momentum']
        
        # IQL Agent for pretraining
        self.iql_agent = IQLAgent(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            hidden_dim=256,
            expectile=self.config.iql_expectile,
            temperature=self.config.iql_temperature,
            device=self.device
        )
        
        # Stability Monitor ì´ˆê¸°í™”
        stability_config = {
            'window_size': 100,
            'n_sigma': 3.0,
            'intervention_threshold': 6.0,  # ê°€ì´ë“œ ìš”êµ¬ê°’ìœ¼ë¡œ ìƒí–¥
            'rollback_enabled': True,
            'q_value_max': 100.0,
            'q_value_min': -100.0,
            'entropy_min': 0.1,
            'gradient_max': 10.0,
            'concentration_max': 0.5,
            'turnover_max': 0.5
        }
        self.stability_monitor = StabilityMonitor(stability_config)
        self.logger.info("StabilityMonitor ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Replay Buffer
        # ì´ˆê¸°ì—ëŠ” PER off (uniform sampling)ë¡œ ì‹œì‘
        self.use_per = False  # ì´ˆê¸° PER off
        self.per_activation_step = 10000  # 10k ìŠ¤í… í›„ PER í™œì„±í™”
        self.replay_buffer = PrioritizedReplayBuffer(
            capacity=self.config.memory_capacity,
            alpha=0.0 if not self.use_per else 0.6,  # ì´ˆê¸° alpha=0 (uniform)
            beta=0.4
        )
        
        # Performance Monitor
        monitoring_config = self.config.monitoring_config or {}
        self.performance_monitor = PerformanceMonitor(
            log_dir=self.log_dir,
            use_wandb=monitoring_config.get('use_wandb', False),
            use_tensorboard=monitoring_config.get('use_tensorboard', True),
            wandb_config=monitoring_config
        )
        
        self.logger.info("ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def train(self):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("=" * 50)
        self.logger.info("FinFlow í•™ìŠµ ì‹œì‘")
        self.logger.info("=" * 50)
        
        # Phase 1: IQL Pretraining
        self.logger.info("\n[Phase 1] IQL ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ")
        if not self._check_offline_data():
            self.logger.info("ì˜¤í”„ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            self._prepare_offline_data()
        self._pretrain_iql()
        
        # Phase 2: Online SAC Fine-tuning
        self.logger.info("\n[Phase 2] SAC ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì •")
        self._train_sac()
        
        # Phase 3: Final Evaluation
        self.logger.info("\n[Phase 3] ìµœì¢… í‰ê°€")
        final_metrics = self._evaluate()
        
        # Save final model
        self._save_checkpoint("final")
        
        # Generate report
        self._generate_report(final_metrics)
        
        self.logger.info("=" * 50)
        self.logger.info("í•™ìŠµ ì™„ë£Œ!")
        self.logger.info(f"ìµœì¢… Sharpe Ratio: {final_metrics.get('sharpe_ratio', 0):.3f}")
        self.logger.info(f"ìµœì¢… CVaR(5%): {final_metrics.get('cvar_5', 0):.3f}")
        self.logger.info("=" * 50)
    
    def _check_offline_data(self) -> bool:
        """ì˜¤í”„ë¼ì¸ ë°ì´í„° ì¡´ì¬ í™•ì¸"""
        data_path = Path(self.config.data_path)
        if not self.config.data_config.get('use_cache', True):
            return False
        return data_path.exists() and len(list(data_path.glob("*.npz"))) > 0
    
    def _prepare_offline_data(self):
        """ì˜¤í”„ë¼ì¸ ë°ì´í„° ì¤€ë¹„ - OfflineDataset.collect_from_env() ì‚¬ìš©"""
        data_path = Path(self.config.data_path)
        data_path.mkdir(parents=True, exist_ok=True)
        
        # í˜„ì¬ í™˜ê²½ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        if hasattr(self, 'env') and self.env is not None:
            self.logger.info("í™˜ê²½ì—ì„œ ì˜¤í”„ë¼ì¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            
            # OfflineDataset ìƒì„± ë° ë°ì´í„° ìˆ˜ì§‘
            # configì—ì„œ ì—í”¼ì†Œë“œ ìˆ˜ ê°€ì ¸ì˜¤ê¸° (YAML ì„¤ì • ì‚¬ìš©)
            n_episodes = self.config.offline_episodes
            self.logger.info(f"{n_episodes}ê°œ ì—í”¼ì†Œë“œë¡œ ì˜¤í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘")
            
            dataset = OfflineDataset()
            dataset.collect_from_env(
                env=self.env,
                n_episodes=n_episodes,
                diversity_bonus=True,
                verbose=True
            )
            
            # ë°ì´í„°ì…‹ ì €ì¥
            save_path = data_path / 'offline_data.npz'
            dataset.save(save_path)
            self.logger.info(f"ì˜¤í”„ë¼ì¸ ë°ì´í„°ì…‹ ì €ì¥: {save_path}")
            return
        
        # í™˜ê²½ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜
        raise ValueError(
            "í™˜ê²½ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            "trainerë¥¼ ìƒì„±í•  ë•Œ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        )
    
    
    def _pretrain_iql(self):
        """IQL ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ"""
        # Prepare data if not exists
        if not self._check_offline_data():
            self.logger.info("ì˜¤í”„ë¼ì¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            self._prepare_offline_data()
        
        # Load offline dataset
        dataset = OfflineDataset(self.config.data_path)
        
        self.logger.info(f"ì˜¤í”„ë¼ì¸ ë°ì´í„°ì…‹ ë¡œë“œ: {len(dataset)} samples")
        self.logger.info("=" * 50)
        self.logger.info("IQL ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ ì‹œì‘")
        self.logger.info("=" * 50)
        
        # Training loop with progress bar
        from tqdm import tqdm
        # YAML ì„¤ì • ì‚¬ìš©
        pbar = tqdm(range(self.config.iql_epochs), desc="IQL Pretraining", unit="epoch")
        
        for epoch in pbar:
            epoch_losses = []
            
            # Mini-batch training
            for _ in range(len(dataset) // self.config.iql_batch_size):
                # YAML ì„¤ì • ì‚¬ìš©
                batch = dataset.sample_batch(self.config.iql_batch_size)
                
                # Convert to tensors
                states = torch.FloatTensor(batch['states']).to(self.device)
                actions = torch.FloatTensor(batch['actions']).to(self.device)
                rewards = torch.FloatTensor(batch['rewards']).to(self.device)
                next_states = torch.FloatTensor(batch['next_states']).to(self.device)
                dones = torch.FloatTensor(batch['dones']).to(self.device)
                
                # IQL update
                losses = self.iql_agent.update(
                    states, actions, rewards, next_states, dones
                )
                
                epoch_losses.append(losses)
                self.global_step += 1
            
            # Calculate average losses for this epoch
            if epoch_losses:
                avg_losses = {
                    k: np.mean([l[k] for l in epoch_losses])
                    for k in epoch_losses[0].keys()
                }
                
                # Update progress bar with metrics
                pbar.set_postfix({
                    'V_Loss': f"{avg_losses.get('value_loss', 0):.4f}",
                    'Q_Loss': f"{avg_losses.get('q_loss', 0):.4f}",
                    'Actor_Loss': f"{avg_losses.get('actor_loss', 0):.4f}"
                })
                
                # Log epoch metrics
                if (epoch + 1) % self.config.log_interval == 0:
                    self.logger.info(
                        f"IQL Epoch {epoch+1}/{self.config.iql_epochs} | "
                        f"V Loss: {avg_losses['value_loss']:.6f} | "
                        f"Q Loss: {avg_losses['q_loss']:.6f} | "
                        f"Actor Loss: {avg_losses['actor_loss']:.6f}"
                    )
                    
                    # IQL í•™ìŠµ ì§„ë‹¨ ì •ë³´
                    self.logger.debug(f"Value gradient norm: {avg_losses.get('value_grad_norm', 0):.6f}")
                    self.logger.debug(f"Q gradient norm: {avg_losses.get('q_grad_norm', 0):.6f}")
                    self.logger.debug(f"Actor gradient norm: {avg_losses.get('actor_grad_norm', 0):.6f}")
                    
                    self.logger.log_metrics(avg_losses, self.global_step)
        
        # Transfer knowledge to B-Cell
        self._transfer_iql_to_bcell()
        self.logger.info("IQL ì‚¬ì „í•™ìŠµ ì™„ë£Œ ë° ì§€ì‹ ì „ì´ ì™„ë£Œ")
    
    def _transfer_iql_to_bcell(self):
        """IQLì—ì„œ B-Cellë¡œ ì™„ì „í•œ ì§€ì‹ ì „ì´"""
        
        # 1. Actor ë„¤íŠ¸ì›Œí¬ ì „ì´ (ì •ì±…)
        self.b_cell.actor.load_state_dict(
            self.iql_agent.actor.state_dict()
        )
        self.logger.info("Actor ë„¤íŠ¸ì›Œí¬ ì „ì´ ì™„ë£Œ")
        
        # 2. Value networkë¥¼ Critic ì´ˆê¸°í™”ì— í™œìš©
        with torch.no_grad():
            # IQLì˜ value functionì„ SACì˜ baselineìœ¼ë¡œ ì‚¬ìš©
            if hasattr(self.iql_agent, 'value'):
                self.b_cell.value_baseline = self.iql_agent.value
                
            # Q-network ê°€ì¤‘ì¹˜ë¥¼ Critic ì´ˆê¸°í™”ì— í™œìš©
            # ì£¼ì˜: IQLì€ ë‹¨ì¼ Qê°’, SACëŠ” Quantile ë¶„í¬ ì‚¬ìš©
            if hasattr(self.b_cell, 'critic'):
                # í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ë§Œ ë³µì‚¬
                self._transfer_compatible_layers(
                    source=self.iql_agent.q1,
                    target=self.b_cell.critic.q1,
                    layer_mapping={
                        'fc1': 'fc1',  # ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” ë™ì¼
                        'fc2': 'fc2',  # ë‘ ë²ˆì§¸ ë ˆì´ì–´ë„ í˜¸í™˜
                        # fc3ëŠ” ì¶œë ¥ ì°¨ì›ì´ ë‹¤ë¥´ë¯€ë¡œ ì œì™¸
                    }
                )
                self._transfer_compatible_layers(
                    source=self.iql_agent.q2,
                    target=self.b_cell.critic.q2,
                    layer_mapping={
                        'fc1': 'fc1',
                        'fc2': 'fc2',
                    }
                )
        
        # 3. IQL í•™ìŠµ í†µê³„ ì „ì´
        self.b_cell.initial_stats = {
            'iql_final_value': self._compute_iql_average_value(),
            'iql_final_q': self._compute_iql_average_q(),
            'iql_training_steps': self.iql_agent.training_steps if hasattr(self.iql_agent, 'training_steps') else 0
        }
        
        # 4. Temperature (alpha) ì´ˆê¸°í™”
        # IQLì˜ advantage ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ SACì˜ ì—”íŠ¸ë¡œí”¼ ëª©í‘œ ì„¤ì •
        advantages = self._compute_iql_advantages()
        if advantages is not None:
            initial_entropy = -np.mean(advantages) * 0.1  # íœ´ë¦¬ìŠ¤í‹±
            self.b_cell.target_entropy = initial_entropy
        
        # 5. ëª¨ë“  B-Cell ì „ëµì— ì „ì´
        for bcell_name, bcell in self.b_cells.items():
            if bcell != self.b_cell:  # ê¸°ë³¸ B-Cellì€ ì´ë¯¸ ì „ì´ë¨
                bcell.actor.load_state_dict(self.iql_agent.actor.state_dict())
                if hasattr(bcell, 'critic'):
                    self._transfer_compatible_layers(
                        source=self.iql_agent.q1,
                        target=bcell.critic.q1,
                        layer_mapping={'fc1': 'fc1', 'fc2': 'fc2'}
                    )
                    self._transfer_compatible_layers(
                        source=self.iql_agent.q2,
                        target=bcell.critic.q2,
                        layer_mapping={'fc1': 'fc1', 'fc2': 'fc2'}
                    )
                self.logger.debug(f"B-Cell [{bcell_name}] ì§€ì‹ ì „ì´ ì™„ë£Œ")
        
        self.logger.info(f"ì§€ì‹ ì „ì´ ì™„ë£Œ: Value baseline={self.b_cell.initial_stats.get('iql_final_value', 0):.3f}")
    
    def _transfer_compatible_layers(self, source, target, layer_mapping):
        """í˜¸í™˜ ê°€ëŠ¥í•œ ë ˆì´ì–´ë§Œ ì„ íƒì  ì „ì´"""
        source_dict = source.state_dict()
        target_dict = target.state_dict()
        
        for src_name, tgt_name in layer_mapping.items():
            src_key_w = f"{src_name}.weight"
            src_key_b = f"{src_name}.bias"
            tgt_key_w = f"{tgt_name}.weight"
            tgt_key_b = f"{tgt_name}.bias"
            
            if src_key_w in source_dict and tgt_key_w in target_dict:
                if source_dict[src_key_w].shape == target_dict[tgt_key_w].shape:
                    target_dict[tgt_key_w] = source_dict[src_key_w].clone()
                    target_dict[tgt_key_b] = source_dict[src_key_b].clone()
                    self.logger.debug(f"ë ˆì´ì–´ ì „ì´: {src_name} â†’ {tgt_name}")
                else:
                    self.logger.debug(f"ë ˆì´ì–´ í¬ê¸° ë¶ˆì¼ì¹˜: {src_name} {source_dict[src_key_w].shape} â†’ {tgt_name} {target_dict[tgt_key_w].shape}")
        
        target.load_state_dict(target_dict)
    
    def _compute_iql_average_value(self):
        """IQLì˜ í‰ê·  value ê³„ì‚°"""
        if not hasattr(self.iql_agent, 'value'):
            return 0.0
        
        # ìƒ˜í”Œ ìƒíƒœë“¤ì— ëŒ€í•œ í‰ê·  value ê³„ì‚°
        with torch.no_grad():
            if len(self.replay_buffer) > 100:
                transitions, _, _ = self.replay_buffer.sample(100)
                states = torch.FloatTensor([t.state for t in transitions]).to(self.device)
                values = self.iql_agent.value(states)
                return values.mean().item()
        return 0.0
    
    def _compute_iql_average_q(self):
        """IQLì˜ í‰ê·  Qê°’ ê³„ì‚°"""
        if not hasattr(self.iql_agent, 'q1'):
            return 0.0
        
        with torch.no_grad():
            if len(self.replay_buffer) > 100:
                transitions, _, _ = self.replay_buffer.sample(100)
                states = torch.FloatTensor([t.state for t in transitions]).to(self.device)
                actions = torch.FloatTensor([t.action for t in transitions]).to(self.device)
                q1_values = self.iql_agent.q1(states, actions)
                q2_values = self.iql_agent.q2(states, actions)
                return torch.min(q1_values, q2_values).mean().item()
        return 0.0
    
    def _compute_iql_advantages(self):
        """IQLì˜ advantage ë¶„í¬ ê³„ì‚°"""
        if not hasattr(self.iql_agent, 'value') or not hasattr(self.iql_agent, 'q1'):
            return None
        
        with torch.no_grad():
            if len(self.replay_buffer) > 100:
                transitions, _, _ = self.replay_buffer.sample(100)
                states = torch.FloatTensor([t.state for t in transitions]).to(self.device)
                actions = torch.FloatTensor([t.action for t in transitions]).to(self.device)
                
                values = self.iql_agent.value(states)
                q1_values = self.iql_agent.q1(states, actions)
                q2_values = self.iql_agent.q2(states, actions)
                q_values = torch.min(q1_values, q2_values)
                
                advantages = q_values - values
                return advantages.cpu().numpy()
        return None
    
    def _train_sac(self):
        """SAC ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì •"""
        self.logger.info("=" * 50)
        self.logger.info("SAC ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì • ì‹œì‘")
        self.logger.info("=" * 50)
        
        # ëˆ„ë½ëœ ì†ì„± ì´ˆê¸°í™”
        self.all_costs = []
        self.last_action = np.zeros(self.env.n_assets)
        
        episode_rewards = []
        episode_sharpes = []
        episode_cvars = []
        
        # T-Cell prefit ê°•ì œ ìˆ˜í–‰
        if hasattr(self, 't_cell') and not self.t_cell.is_fitted:
            # ì´ˆê¸° íŠ¹ì„± ìœˆë„ìš° ì¤€ë¹„
            initial_features = []
            for i in range(self.t_cell.window_size):
                feat = self.feature_extractor.extract_features(
                    self.env.price_data,
                    current_idx=self.feature_extractor.window + i
                )
                initial_features.append(feat)
            initial_features = np.array(initial_features)
            self.t_cell.prefit(initial_features)
            self.logger.info("T-Cell prefit ì™„ë£Œ")

        from tqdm import tqdm
        pbar = tqdm(range(self.config.sac_episodes), desc="SAC Training", unit="episode")

        for episode in pbar:
            self.episode = episode
            episode_reward = 0
            episode_steps = 0
            self.episode_returns = []  # ì—í”¼ì†Œë“œ ìˆ˜ìµë¥  ì¶”ì 
            self.episode_actions = []  # ì—í”¼ì†Œë“œ ì•¡ì…˜ ì¶”ì 
            
            # Reset environment
            state, _ = self.env.reset()
            done = False
            
            # Episode loop
            max_episode_steps = min(self.env.max_steps, len(self.env.price_data) - 2)
            last_progress_checkpoint = 0

            while not done:
                # Crisis detection
                crisis_info = self.t_cell.detect_crisis(self.env.get_market_data())
                crisis_level = crisis_info['overall_crisis']
                
                # Memory guidance
                memory_guidance = self.memory_cell.get_memory_guidance(
                    state, crisis_level
                )
                
                # Gating decision
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                gating_decision = self.gating_network(
                    state_tensor, memory_guidance, crisis_level
                )
                
                # Select action using B-Cell
                action = self.b_cell.select_action(
                    state_tensor,
                    bcell_type=gating_decision.selected_bcell,
                    deterministic=False
                )
                
                # Store action for tracking
                self.last_action = action.copy()
                
                # Environment step
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                
                # ì—í”¼ì†Œë“œ ë°ì´í„° ì¶”ì 
                portfolio_return = info.get('portfolio_return', 0)
                if portfolio_return == 0:  # fallback
                    portfolio_return = reward / self.config.env_config['initial_balance']
                self.episode_returns.append(portfolio_return)

                # ì—í”¼ì†Œë“œ 10% ì§„í–‰ ì‹œì ë§ˆë‹¤ í†µê³„ ì¶œë ¥ (ê³¼ë„í•œ ë¡œê·¸ ë°©ì§€)
                progress_percentage = (episode_steps / max_episode_steps) * 100 if max_episode_steps > 0 else 0
                current_checkpoint = int(progress_percentage // 10) * 10  # 10, 20, 30, ...

                if current_checkpoint > last_progress_checkpoint and current_checkpoint > 0:
                    last_progress_checkpoint = current_checkpoint

                    # ìµœê·¼ ìˆ˜ìµë¥  í†µê³„ ê³„ì‚°
                    window_size = max(10, episode_steps // 10)  # ìµœì†Œ 10 ìŠ¤í…, ì•„ë‹ˆë©´ ì „ì²´ì˜ 10%
                    recent_returns = self.episode_returns[-window_size:] if len(self.episode_returns) >= window_size else self.episode_returns
                    cumulative_return = np.prod(1 + np.array(self.episode_returns)) - 1

                    self.logger.debug(
                        f"ì§„í–‰ë¥  {current_checkpoint}% (Step {episode_steps}/{max_episode_steps}) | "
                        f"ìµœê·¼ {len(recent_returns)}ìŠ¤í… í†µê³„: "
                        f"í‰ê· ={np.mean(recent_returns)*100:.2f}%, "
                        f"í‘œì¤€í¸ì°¨={np.std(recent_returns)*100:.2f}%, "
                        f"ìµœëŒ€={np.max(recent_returns)*100:.2f}%, "
                        f"ìµœì†Œ={np.min(recent_returns)*100:.2f}% | "
                        f"ëˆ„ì ìˆ˜ìµë¥ ={(cumulative_return)*100:.2f}%"
                    )

                self.episode_actions.append(action.copy())
                
                # ê±°ë˜ ë¹„ìš© ì¶”ì 
                transaction_cost = info.get('transaction_cost', 0)
                self.all_costs.append(transaction_cost)
                
                # CVaR í˜ë„í‹° ì ìš© (ìµœê·¼ ìˆ˜ìµë¥  ê¸°ë°˜)
                if len(self.episode_returns) >= 20:
                    recent_returns = np.array(self.episode_returns[-20:])
                    cvar_alpha = 0.95  # í•˜ìœ„ 5%
                    var_idx = int(len(recent_returns) * (1 - cvar_alpha))
                    if var_idx > 0:
                        sorted_returns = np.sort(recent_returns)
                        cvar = np.mean(sorted_returns[:var_idx])
                        cvar_target = -0.02  # -2% ëª©í‘œ
                        cvar_penalty = max(0, cvar_target - cvar) * 10.0  # ê°•í•œ í˜ë„í‹°
                        reward = reward - cvar_penalty * 0.1  # ë³´ìƒì— CVaR í˜ë„í‹° ë°˜ì˜

                # Store experience
                from src.core.replay import Transition
                transition = Transition(
                    state=state,
                    action=action,
                    reward=reward,
                    next_state=next_state,
                    done=done
                )
                self.replay_buffer.push(transition)
                
                # Store in memory cell
                self.memory_cell.store(
                    state, action, reward, crisis_level,
                    gating_decision.selected_bcell,
                    {'episode': episode, 'step': episode_steps}
                )
                
                # ë¬´ì²´ê²° ê°ì§€ ë¡œì§
                if abs(portfolio_return) < 1e-12:
                    self.zero_return_count += 1
                else:
                    self.zero_return_count = 0

                if self.zero_return_count >= self.zero_return_threshold:
                    self.logger.error(f"{self.zero_return_threshold}íšŒ ì—°ì† ë¬´ê±°ë˜ ê°ì§€!")
                    self._diagnose_no_trade()
                    assert False, f"ë¬´ê±°ë˜ ë£¨í”„ ê°ì§€: {self.zero_return_threshold}íšŒ ì—°ì† 0 ìˆ˜ìµ"

                # PER í™œì„±í™” ì²´í¬
                if not self.use_per and self.global_step >= self.per_activation_step:
                    self.use_per = True
                    self.replay_buffer.alpha = 0.6  # PER í™œì„±í™”
                    self.logger.info(f"PER í™œì„±í™” (step {self.global_step})")

                # Update B-Cell (ìµœì†Œ ë²„í¼ ì‚¬ì´ì¦ˆ ì™„í™”: 256)
                min_buffer_size = 256  # ê¸°ì¡´ 1000ì—ì„œ ì™„í™”
                if len(self.replay_buffer) > min_buffer_size:
                    transitions, indices, weights = self.replay_buffer.sample(self.config.sac_batch_size)
                    
                    # Convert transitions to batch format
                    states = torch.FloatTensor([t.state for t in transitions]).to(self.device)
                    actions = torch.FloatTensor([t.action for t in transitions]).to(self.device)
                    rewards = torch.FloatTensor([t.reward for t in transitions]).to(self.device)
                    next_states = torch.FloatTensor([t.next_state for t in transitions]).to(self.device)
                    dones = torch.FloatTensor([t.done for t in transitions]).to(self.device)
                    
                    # ë°°ì¹˜ ìƒì„± - í…ì„œ ê·¸ëŒ€ë¡œ ìœ ì§€
                    batch = {
                        'states': states,  # ì´ë¯¸ í…ì„œ
                        'actions': actions,  # ì´ë¯¸ í…ì„œ
                        'rewards': rewards,  # ì´ë¯¸ í…ì„œ
                        'next_states': next_states,  # ì´ë¯¸ í…ì„œ
                        'dones': dones,  # ì´ë¯¸ í…ì„œ
                        'weights': torch.FloatTensor(weights).to(self.device),
                        'indices': indices
                    }
                    
                    losses = self.b_cell.update(batch)
                    
                    # Monitor stability
                    stability_metrics = {
                        'q_value': losses.get('q_value', 0.0),
                        'entropy': losses.get('entropy', 1.0),
                        'loss': losses.get('critic_loss', 0.0),
                        'reward': reward,
                        'gradient_norm': losses.get('grad_norm', 0.0),
                        'learning_rate': self.config.sac_lr,
                        'cql_alpha': losses.get('cql_alpha', 0.0),
                        'portfolio_concentration': np.max(action),
                        'turnover': np.linalg.norm(action - self.last_action)
                    }
                    
                    # ì•¡ì…˜ ì—…ë°ì´íŠ¸
                    self.last_action = action.copy()
                    
                    # Push metrics to stability monitor
                    self.stability_monitor.push(stability_metrics)
                    
                    # Check for intervention
                    alerts = self.stability_monitor.check()
                    if alerts['severity'] in ('warning', 'critical'):
                        self.logger.warning(f"{alerts['severity'].upper()} stability alert: {alerts['issues']}")
                        
                        # ì¦‰ì‹œ ê°œì…
                        self.stability_monitor.intervene(self)
                        
                        # ì•ŒëŒ ìŠ¤ëƒ…ìƒ· ì‹œê°í™” ì €ì¥ (criticalë§Œ + ì¿¨ë‹¤ìš´ ì²´í¬)
                        should_visualize = (
                            alerts['severity'] == 'critical' and  # criticalë§Œ
                            self.global_step - self.last_visualization_step >= self.visualization_cooldown
                        )
                        
                        if should_visualize:
                            self.last_visualization_step = self.global_step
                            alert_timestamp = f"{self.global_step}"
                            
                            # Equity curve ì €ì¥ (ì—í”¼ì†Œë“œ ìˆ˜ìµë¥ ë¡œë¶€í„° ìƒì„±)
                            if hasattr(self, 'episode_returns'):
                                equity_curve = np.cumprod(1 + np.array(self.episode_returns))
                                plot_equity_curve(
                                    equity_curve,
                                    save_path=self.run_dir / "alerts" / f"equity_{alert_timestamp}.png"
                                )
                                plot_drawdown(
                                    equity_curve,
                                    save_path=self.run_dir / "alerts" / f"dd_{alert_timestamp}.png"
                                )
                            
                            # Portfolio weights ì €ì¥
                            if hasattr(self, 'episode_actions') and len(self.episode_actions) > 0:
                                asset_names = [f"Asset_{i}" for i in range(len(action))]
                                latest_weights = self.episode_actions[-1]
                                plot_portfolio_weights(
                                    latest_weights,
                                    asset_names,
                                    save_path=self.run_dir / "alerts" / f"weights_{alert_timestamp}.png"
                                )
                            
                            self.logger.info(f"Critical ì•ŒëŒ ì‹œê°í™” ì €ì¥: {self.run_dir / 'alerts'}")
                    
                    # Update priorities
                    td_errors = losses.get('td_error', None)
                    if td_errors is not None:
                        indices = batch.get('indices', None)
                        if indices is not None:
                            self.replay_buffer.update_priorities(
                                indices, td_errors.cpu().numpy()
                            )
                
                # Update gating network performance
                self.gating_network.update_performance(
                    gating_decision.selected_bcell,
                    reward,
                    {'crisis_level': crisis_level}
                )
                
                # Accumulate
                episode_reward += reward
                episode_steps += 1
                state = next_state
                self.global_step += 1
            
            episode_rewards.append(episode_reward)
            
            # Calculate episode metrics
            if len(self.episode_returns) > 0:
                episode_sharpe = self._calculate_sharpe(self.episode_returns)
                episode_cvar = self._calculate_cvar(self.episode_returns)
                episode_calmar = self._calculate_calmar(self.episode_returns)
                episode_sortino = self._calculate_sortino(self.episode_returns)
                episode_sharpes.append(episode_sharpe)
                episode_cvars.append(episode_cvar)
                
                # Calculate portfolio metrics
                returns_array = np.array(self.episode_returns)
                portfolio_value = self.config.env_config['initial_balance'] * np.prod(1 + returns_array)
                total_return = np.prod(1 + returns_array) - 1
                volatility = np.std(returns_array) * np.sqrt(252)
                
                # ìµœëŒ€ ë‚™í­ ê³„ì‚°
                equity_curve = np.cumprod(1 + returns_array)
                running_max = np.maximum.accumulate(equity_curve)
                drawdown = (equity_curve - running_max) / running_max
                max_drawdown = np.min(drawdown)
                
                # íšŒì „ìœ¨ ê³„ì‚° (ì•¡ì…˜ ë³€í™”ëŸ‰)
                if hasattr(self, 'episode_actions') and len(self.episode_actions) > 1:
                    turnovers = [np.sum(np.abs(self.episode_actions[i] - self.episode_actions[i-1])) 
                                for i in range(1, len(self.episode_actions))]
                    avg_turnover = np.mean(turnovers) if turnovers else 0
                else:
                    avg_turnover = 0
                
                # Update progress bar
                pbar.set_postfix({
                    'Return': f"{total_return:.2%}",
                    'Sharpe': f"{episode_sharpe:.2f}",
                    'Calmar': f"{episode_calmar:.2f}",
                    'Value': f"{portfolio_value/1e6:.2f}M",
                    'Steps': episode_steps
                })
                
                # ë§¤ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ìƒì„¸ ì„±ê³¼ ì¶œë ¥
                self.logger.info("=" * 60)
                self.logger.info(f"Episode {episode+1}/{self.config.sac_episodes} ì™„ë£Œ")
                self.logger.info("-" * 60)
                self.logger.info(f"ğŸ“Š ìˆ˜ìµë¥ : {total_return:.6%} | í¬íŠ¸í´ë¦¬ì˜¤: ${portfolio_value:,.2f}")
                self.logger.info(f"ğŸ“ˆ Sharpe: {episode_sharpe:.3f} | Calmar: {episode_calmar:.3f} | Sortino: {episode_sortino:.3f}")
                self.logger.info(f"ğŸ“‰ CVaR(5%): {episode_cvar:.6f} | MaxDD: {max_drawdown:.6%} | Vol: {volatility:.4%}")
                self.logger.info(f"ğŸ”„ Turnover: {avg_turnover:.4%} | Steps: {episode_steps} | Reward: {episode_reward:.6f}")
                
                # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
                self.logger.debug(f"Raw portfolio value: {portfolio_value}")
                self.logger.debug(f"Transaction costs: {np.mean(self.all_costs[-episode_steps:]) if hasattr(self, 'all_costs') and len(self.all_costs) > 0 else 0:.6f}")
                self.logger.debug(f"Action std: {np.std(action) if 'action' in locals() else 0:.6f}")
                self.logger.info("=" * 60)
            
            # 10 ì—í”¼ì†Œë“œë§ˆë‹¤ í†µê³„ ìš”ì•½
            if (episode + 1) % 10 == 0 and len(episode_rewards) >= 10:
                # ìµœê·¼ 10 ì—í”¼ì†Œë“œ í†µê³„
                recent_returns = []
                for i in range(max(0, episode - 9), episode + 1):
                    if i < len(episode_sharpes):
                        recent_returns.append(episode_sharpes[i])
                
                self.logger.info("\n" + "="*60)
                self.logger.info("ğŸ“Š ìµœê·¼ 10 ì—í”¼ì†Œë“œ í†µê³„:")
                self.logger.info(f"  í‰ê·  Sharpe: {np.mean(episode_sharpes[-10:]):.3f}")
                self.logger.info(f"  í‰ê·  ë³´ìƒ: {np.mean(episode_rewards[-10:]):.4f}")
                self.logger.info(f"  ìµœê³  ë³´ìƒ: {np.max(episode_rewards[-10:]):.4f}")
                self.logger.info(f"  ìµœì € ë³´ìƒ: {np.min(episode_rewards[-10:]):.4f}")
                self.logger.info(f"  í‰ê·  CVaR: {np.mean(episode_cvars[-10:]) if episode_cvars else 0:.3f}")
                self.logger.info("=" * 60 + "\n")
            
            # Evaluation
            if (episode + 1) % self.config.eval_interval == 0:
                eval_metrics = self._evaluate()
                self._check_early_stopping(eval_metrics)
            
            # Checkpoint
            if (episode + 1) % self.config.checkpoint_interval == 0:
                self._save_checkpoint(f"episode_{episode+1}")
    
    def _diagnose_no_trade(self):
        """ë¬´ê±°ë˜ ìƒí™© ì§„ë‹¨"""
        self.logger.error("\n" + "="*60)
        self.logger.error("ğŸ” ë¬´ê±°ë˜ ì§„ë‹¨ ìŠ¤ëƒ…ìƒ·")
        self.logger.error("="*60)

        # í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ
        self.logger.error(f"Portfolio value: {self.env.portfolio_value}")
        self.logger.error(f"Cash: {self.env.cash}")
        self.logger.error(f"Weights: {self.env.weights}")
        self.logger.error(f"Holdings: {getattr(self.env, 'holdings', 'N/A')}")

        # ê±°ë˜ ì œì•½
        self.logger.error(f"No-trade band: {self.env.no_trade_band}")
        self.logger.error(f"Max turnover: {self.env.max_turnover}")
        self.logger.error(f"Min trade size: {getattr(self.env, 'min_trade_size', 1)}")

        # ìµœê·¼ ì•¡ì…˜
        if hasattr(self, 'last_action'):
            self.logger.error(f"Last action: {self.last_action}")
            self.logger.error(f"Action L1 norm: {np.sum(np.abs(self.last_action))}")
            self.logger.error(f"Action change from weights: {np.sum(np.abs(self.last_action - self.env.weights))}")

        # ë²„í¼ ìƒíƒœ
        self.logger.error(f"Replay buffer size: {len(self.replay_buffer)}")
        self.logger.error(f"Zero return count: {self.zero_return_count}")
        self.logger.error("="*60 + "\n")

    def _evaluate(self) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        self.logger.info("í‰ê°€ ì‹œì‘...")
        
        eval_rewards = []
        eval_returns = []
        eval_actions = []
        
        for _ in range(10):  # 10 episodes evaluation
            episode_reward = 0
            episode_returns = []
            
            state, _ = self.env.reset()
            done = False
            
            while not done:
                # Deterministic action
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                # Get crisis and memory guidance
                crisis_info = self.t_cell.detect_crisis(self.env.get_market_data())
                memory_guidance = self.memory_cell.get_memory_guidance(
                    state, crisis_info['overall_crisis']
                )
                
                # Gating decision
                gating_decision = self.gating_network(
                    state_tensor, memory_guidance, crisis_info['overall_crisis']
                )
                
                # Select action
                action = self.b_cell.select_action(
                    state_tensor,
                    bcell_type=gating_decision.selected_bcell,
                    deterministic=True
                )
                
                eval_actions.append(action)
                
                # Step
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                episode_reward += reward
                episode_returns.append(reward)
                state = next_state
            
            eval_rewards.append(episode_reward)
            eval_returns.extend(episode_returns)
        
        # Calculate metrics
        returns_array = np.array(eval_returns)
        
        # Sharpe Ratio
        if len(returns_array) > 1:
            sharpe = np.mean(returns_array) / (np.std(returns_array) + 1e-8) * np.sqrt(252)
        else:
            sharpe = 0
        
        # CVaR (5%)
        sorted_returns = np.sort(returns_array)
        cvar_5 = np.mean(sorted_returns[:max(1, len(sorted_returns) // 20)])
        
        # Max Drawdown
        cumulative = np.cumprod(1 + returns_array)
        running_max = np.maximum.accumulate(cumulative)
        drawdown = (cumulative - running_max) / running_max
        max_drawdown = np.min(drawdown)
        
        metrics = {
            'avg_reward': np.mean(eval_rewards),
            'sharpe_ratio': sharpe,
            'cvar_5': cvar_5,
            'max_drawdown': max_drawdown,
            'avg_return': np.mean(returns_array),
            'return_std': np.std(returns_array)
        }
        
        # Log metrics
        self.logger.info(
            f"í‰ê°€ ê²°ê³¼: Sharpe={sharpe:.3f}, CVaR={cvar_5:.3f}, "
            f"MaxDD={max_drawdown:.3f}, AvgReward={metrics['avg_reward']:.2f}"
        )
        
        self.logger.log_metrics(metrics, self.global_step)
        self.metrics_history.append(metrics)
        
        return metrics
    
    def _calculate_sharpe(self, returns: List[float]) -> float:
        """ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚°"""
        if len(returns) < 20:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            return 0.0
        returns_array = np.array(returns)
        if np.std(returns_array) < 1e-8:
            return 0.0
        sharpe = np.mean(returns_array) / np.std(returns_array) * np.sqrt(252)
        # ë””ë²„ê·¸ ë¡œê·¸
        self.logger.debug(f"Sharpe calculation: mean={np.mean(returns_array):.6f}, std={np.std(returns_array):.6f}, sharpe={sharpe:.3f}")
        return sharpe
    
    def _calculate_cvar(self, returns: List[float], alpha: float = 0.05) -> float:
        """CVaR ê³„ì‚°"""
        if len(returns) < 20:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            return 0.0
        returns_array = np.array(returns)
        sorted_returns = np.sort(returns_array)
        n_tail = max(1, int(len(sorted_returns) * alpha))
        cvar = np.mean(sorted_returns[:n_tail])
        self.logger.debug(f"CVaR calculation: n_tail={n_tail}, cvar={cvar:.6f}")
        return cvar
    
    def _calculate_calmar(self, returns: List[float]) -> float:
        """ì¹¼ë§ˆ ë¹„ìœ¨ ê³„ì‚° (ì—°ê°„ ìˆ˜ìµë¥  / ìµœëŒ€ ë‚™í­)"""
        if len(returns) < 20:
            return 0.0
        equity_curve = np.cumprod(1 + np.array(returns))
        
        # ì—°í™˜ì‚° ìˆ˜ìµë¥ 
        total_return = equity_curve[-1] - 1
        annual_return = (1 + total_return) ** (252 / len(returns)) - 1
        
        # ìµœëŒ€ ë‚™í­ ê³„ì‚°
        running_max = np.maximum.accumulate(equity_curve)
        drawdown = (equity_curve - running_max) / running_max
        max_dd = abs(np.min(drawdown))
        
        if max_dd < 1e-8:
            return 0.0
        return annual_return / max_dd
    
    def _calculate_sortino(self, returns: List[float], target_return: float = 0.0) -> float:
        """ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨ ê³„ì‚° (í•˜ë°© ë³€ë™ì„±ë§Œ ê³ ë ¤)"""
        if len(returns) < 20:
            return 0.0
        returns_array = np.array(returns)
        excess_returns = returns_array - target_return
        
        # í•˜ë°© ë³€ë™ì„±
        downside_returns = excess_returns[excess_returns < 0]
        if len(downside_returns) == 0:
            return 0.0
        
        downside_std = np.sqrt(np.mean(downside_returns ** 2))
        if downside_std < 1e-8:
            return 0.0
        
        return np.mean(excess_returns) / downside_std * np.sqrt(252)
    
    def _check_early_stopping(self, metrics: Dict[str, float]):
        """ì¡°ê¸° ì¢…ë£Œ í™•ì¸"""
        sharpe = metrics.get('sharpe_ratio', 0)
        
        # Check if target met
        if sharpe >= self.config.target_sharpe and \
           metrics.get('cvar_5', 0) >= self.config.target_cvar:
            self.logger.info("ëª©í‘œ ë‹¬ì„±! í•™ìŠµ ì¢…ë£Œ.")
            self._save_checkpoint("target_achieved")
            return True
        
        # Check improvement
        if sharpe > self.best_sharpe + self.config.min_improvement:
            self.best_sharpe = sharpe
            self.patience_counter = 0
            self._save_checkpoint("best")
            self.logger.info(f"ìƒˆë¡œìš´ ìµœê³  Sharpe: {sharpe:.3f}")
        else:
            self.patience_counter += 1
            
        # Check patience
        if self.patience_counter >= self.config.patience:
            self.logger.info(f"ì¡°ê¸° ì¢…ë£Œ (patience={self.config.patience})")
            return True
        
        return False
    
    def _save_checkpoint(self, tag: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        import numpy as np
        import copy
        from datetime import datetime

        # memory_cell ë°ì´í„°ë¥¼ ì™„ì „íˆ í…ì„œë¡œ ë³€í™˜
        memory_data = []
        for m in list(self.memory_cell.memories):
            memory_item = {}
            for key, value in m.items():
                if isinstance(value, np.ndarray):
                    memory_item[key] = torch.tensor(value, dtype=torch.float32)
                elif isinstance(value, (list, tuple)) and len(value) > 0 and isinstance(value[0], (int, float)):
                    memory_item[key] = torch.tensor(value, dtype=torch.float32)
                elif isinstance(value, (int, float, bool)):
                    memory_item[key] = value
                elif isinstance(value, str):
                    memory_item[key] = value
                elif value is None:
                    memory_item[key] = value
                else:
                    # ë‹¤ë¥¸ íƒ€ì…ì€ í…ì„œë¡œ ë³€í™˜ ì‹œë„
                    try:
                        memory_item[key] = torch.tensor(value, dtype=torch.float32)
                    except:
                        memory_item[key] = value
            memory_data.append(memory_item)

        # memory_statsë„ ì•ˆì „í•˜ê²Œ ë³€í™˜
        memory_stats = {}
        if hasattr(self.memory_cell, 'memory_stats'):
            for key, value in self.memory_cell.memory_stats.items():
                if isinstance(value, np.ndarray):
                    memory_stats[key] = torch.tensor(value, dtype=torch.float32)
                elif isinstance(value, (np.integer, np.floating)):
                    memory_stats[key] = float(value)
                else:
                    memory_stats[key] = value

        # ì‹¤ì œ device ë¬¸ìì—´ ì €ì¥
        device_str = str(self.device)
        if device_str == 'auto' or 'auto' in str(device_str):
            if torch.cuda.is_available():
                device_str = f'cuda:{torch.cuda.current_device()}'
            else:
                device_str = 'cpu'
            self.logger.info(f"device='auto'ë¥¼ '{device_str}'ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥")

        # configë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        def make_serializable(obj):
            """ê°ì²´ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
            if isinstance(obj, dict):
                return {k: make_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [make_serializable(v) for v in obj]
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, (int, float, bool, str)) or obj is None:
                return obj
            else:
                return str(obj)

        # config ì „ì²´ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        config_dict = {}
        for key in dir(self.config):
            if not key.startswith('_') and key != 'logger':
                value = getattr(self.config, key)
                if not callable(value):
                    config_dict[key] = value
        serializable_config = make_serializable(config_dict)

        # metricsë„ ì•ˆì „í•˜ê²Œ ë³€í™˜
        safe_metrics = {}
        if self.metrics_history:
            for key, value in self.metrics_history[-1].items():
                if isinstance(value, (np.ndarray, torch.Tensor)):
                    safe_metrics[key] = float(value.item() if hasattr(value, 'item') else value)
                elif isinstance(value, (np.integer, np.floating)):
                    safe_metrics[key] = float(value)
                else:
                    safe_metrics[key] = value

        # B-Cell state_dictë¥¼ ì•ˆì „í•˜ê²Œ ë³€í™˜
        b_cell_state = self.b_cell.state_dict()
        safe_b_cell = {}

        # state_dictë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        for key in ['actor', 'critic_q1', 'critic_q2']:
            if key in b_cell_state:
                safe_b_cell[key] = b_cell_state[key]

        # log_alpha numpy arrayë¥¼ í…ì„œë¡œ ë³€í™˜
        if 'log_alpha' in b_cell_state:
            if isinstance(b_cell_state['log_alpha'], np.ndarray):
                safe_b_cell['log_alpha'] = torch.tensor(b_cell_state['log_alpha'], dtype=torch.float32)
            else:
                safe_b_cell['log_alpha'] = b_cell_state['log_alpha']

        # ë©”íƒ€ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        for key in ['specialization', 'training_step', 'performance_score']:
            if key in b_cell_state:
                safe_b_cell[key] = b_cell_state[key]

        # Optimizer states ì €ì¥
        optimizer_states = {
            'actor_optimizer': self.b_cell.actor_optimizer.state_dict(),
            'critic_optimizer': self.b_cell.critic_optimizer.state_dict(),
            'alpha_optimizer': self.b_cell.alpha_optimizer.state_dict()
        }

        # Specialized B-Cells ì €ì¥ (ìˆëŠ” ê²½ìš°)
        specialized_b_cells = {}
        if hasattr(self, 'b_cells'):
            for name, bcell in self.b_cells.items():
                bcell_state = bcell.state_dict()
                safe_bcell = {}
                for key in ['actor', 'critic_q1', 'critic_q2']:
                    if key in bcell_state:
                        safe_bcell[key] = bcell_state[key]
                if 'log_alpha' in bcell_state:
                    if isinstance(bcell_state['log_alpha'], np.ndarray):
                        safe_bcell['log_alpha'] = torch.tensor(bcell_state['log_alpha'], dtype=torch.float32)
                    else:
                        safe_bcell['log_alpha'] = bcell_state['log_alpha']
                for key in ['specialization', 'training_step', 'performance_score']:
                    if key in bcell_state:
                        safe_bcell[key] = bcell_state[key]
                specialized_b_cells[name] = safe_bcell

        # Replay Buffer ì €ì¥ (í¬ê¸° ì œí•œ)
        replay_buffer_data = None
        if hasattr(self, 'replay_buffer') and len(self.replay_buffer) > 0:
            # ìµœê·¼ 10000ê°œ ìƒ˜í”Œë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ ê³ ë ¤)
            max_samples = min(10000, len(self.replay_buffer))
            buffer_samples = []
            for i in range(max_samples):
                buffer_samples.append(self.replay_buffer.buffer[i])
            replay_buffer_data = {
                'buffer': buffer_samples,
                'priorities': self.replay_buffer.priorities[:max_samples].copy() if hasattr(self.replay_buffer, 'priorities') else None,
                'size': len(self.replay_buffer),
                'ptr': self.replay_buffer.ptr if hasattr(self.replay_buffer, 'ptr') else 0
            }

        # Random states ì €ì¥
        random_states = {
            'torch': torch.get_rng_state(),
            'numpy': np.random.get_state(),
            'python': random.getstate() if 'random' in globals() else None
        }

        # Gating Network performance history ì €ì¥
        gating_history = None
        if hasattr(self.gating_network, 'performance_history'):
            gating_history = {
                'performance_history': dict(self.gating_network.performance_history),
                'selection_count': dict(self.gating_network.selection_count) if hasattr(self.gating_network, 'selection_count') else {}
            }

        # T-Cell crisis history ì €ì¥
        tcell_history = None
        if hasattr(self.t_cell, 'crisis_history'):
            tcell_history = {
                'crisis_history': list(self.t_cell.crisis_history) if hasattr(self.t_cell, 'crisis_history') else [],
                'detection_stats': self.t_cell.detection_stats if hasattr(self.t_cell, 'detection_stats') else {}
            }

        # StabilityMonitor state ì €ì¥
        stability_state = None
        if hasattr(self, 'stability_monitor'):
            stability_state = {
                'alert_counts': self.stability_monitor.alert_counts if hasattr(self.stability_monitor, 'alert_counts') else {},
                'interventions': self.stability_monitor.interventions if hasattr(self.stability_monitor, 'interventions') else [],
                'report': self.stability_monitor.get_report()
            }

        checkpoint = {
            'episode': int(self.episode),
            'global_step': int(self.global_step),
            'b_cell': safe_b_cell,
            'gating_network': self.gating_network.state_dict(),
            'memory_cell': {
                'memories': memory_data,
                'stats': memory_stats
            },
            't_cell': self.t_cell.get_state(),
            'metrics': safe_metrics,
            'config': serializable_config,
            'device': device_str,
            'best_sharpe': float(self.best_sharpe),
            'timestamp': datetime.now().isoformat(),
            'stability_report': self.stability_monitor.get_report() if hasattr(self, 'stability_monitor') else None,
            # ìƒˆë¡œ ì¶”ê°€ëœ í•­ëª©ë“¤
            'optimizer_states': optimizer_states,
            'specialized_b_cells': specialized_b_cells if specialized_b_cells else None,
            'replay_buffer': replay_buffer_data,
            'random_states': random_states,
            'gating_history': gating_history,
            'tcell_history': tcell_history,
            'stability_state': stability_state,
            'version': '2.0'  # ë²„ì „ ì •ë³´ ì¶”ê°€
        }

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        path = self.checkpoint_dir / f"checkpoint_{tag}.pt"
        torch.save(checkpoint, path)

        # Notify StabilityMonitor about checkpoint
        self.stability_monitor.save_checkpoint(str(path))

        self.logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")
    
    def load_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (í˜¸í™˜ì„± ê²€ì‚¬ í¬í•¨)"""
        checkpoint = torch.load(path, map_location=self.device, weights_only=False)
        
        # ë©”íƒ€ë°ì´í„° í™•ì¸
        metadata = checkpoint.get('metadata', {})
        if metadata:
            self.logger.info("ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„°:")
            self.logger.info(f"  - íƒ€ì…: {metadata.get('checkpoint_type', 'unknown')}")
            self.logger.info(f"  - íƒ€ì„ìŠ¤íƒ¬í”„: {metadata.get('timestamp', 'N/A')}")
            self.logger.info(f"  - State ì°¨ì›: {metadata.get('state_dim', 'N/A')}")
            self.logger.info(f"  - Action ì°¨ì›: {metadata.get('action_dim', 'N/A')}")
            self.logger.info(f"  - ìì‚° ìˆ˜: {metadata.get('n_assets', 'N/A')}")
            self.logger.info(f"  - í•™ìŠµ ëª¨ë“œ: {metadata.get('training_mode', 'N/A')}")
            
            # í˜¸í™˜ì„± ê²€ì‚¬
            if 'state_dim' in metadata and metadata['state_dim'] != self.state_dim:
                self.logger.warning(
                    f"State ì°¨ì› ë¶ˆì¼ì¹˜: ì²´í¬í¬ì¸íŠ¸={metadata['state_dim']}, í˜„ì¬={self.state_dim}"
                )
                self.logger.warning("ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            if 'action_dim' in metadata and metadata['action_dim'] != self.action_dim:
                self.logger.warning(
                    f"Action ì°¨ì› ë¶ˆì¼ì¹˜: ì²´í¬í¬ì¸íŠ¸={metadata['action_dim']}, í˜„ì¬={self.action_dim}"
                )
                self.logger.warning("ìì‚° ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                raise ValueError("ì²´í¬í¬ì¸íŠ¸ì™€ í˜„ì¬ í™˜ê²½ì˜ ìì‚° ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # IQL ì²´í¬í¬ì¸íŠ¸ì¸ì§€ full ì²´í¬í¬ì¸íŠ¸ì¸ì§€ í™•ì¸
        checkpoint_type = metadata.get('checkpoint_type', None)
        if checkpoint_type == 'iql':
            is_iql_checkpoint = True
        elif checkpoint_type == 'full':
            is_iql_checkpoint = False
        else:
            # ë ˆê±°ì‹œ ì²´í¬í¬ì¸íŠ¸ (ë©”íƒ€ë°ì´í„° ì—†ìŒ) - íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ íŒë‹¨
            is_iql_checkpoint = 'actor' in checkpoint and 'episode' not in checkpoint
        
        if is_iql_checkpoint:
            # IQL ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            self.logger.info("IQL ì²´í¬í¬ì¸íŠ¸ ê°ì§€ - IQL ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œí•©ë‹ˆë‹¤")
            
            # episodeì™€ global_stepì€ 0ìœ¼ë¡œ ì´ˆê¸°í™”
            self.episode = 0
            self.global_step = checkpoint.get('training_steps', 0)
            
            # ëª¨ë“  B-Cellì— IQL ê°€ì¤‘ì¹˜ ë¡œë“œ
            for bcell_name, bcell in self.b_cells.items():
                if hasattr(bcell, 'load_iql_checkpoint'):
                    bcell.load_iql_checkpoint(checkpoint)
                    self.logger.info(f"B-Cell [{bcell_name}]ì— IQL ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    # load_iql_checkpointê°€ ì—†ìœ¼ë©´ ì§ì ‘ actorë§Œ ë¡œë“œ
                    if 'actor' in checkpoint:
                        bcell.actor.load_state_dict(checkpoint['actor'])
                        self.logger.info(f"B-Cell [{bcell_name}]ì— IQL actor ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            
            # ê¸°ë³¸ B-Cellë„ ì—…ë°ì´íŠ¸
            if hasattr(self.b_cell, 'load_iql_checkpoint'):
                self.b_cell.load_iql_checkpoint(checkpoint)
            
            self.logger.info(f"IQL ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {path}")
            self.logger.info("SAC íŒŒì¸íŠœë‹ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤")
            
        else:
            # Full ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            self.episode = checkpoint['episode']
            self.global_step = checkpoint['global_step']

            self.b_cell.load_state_dict(checkpoint['b_cell'])
            self.gating_network.load_state_dict(checkpoint['gating_network'])

            # Load optimizer states (ìƒˆë¡œ ì¶”ê°€)
            if 'optimizer_states' in checkpoint:
                self.b_cell.actor_optimizer.load_state_dict(checkpoint['optimizer_states']['actor_optimizer'])
                self.b_cell.critic_optimizer.load_state_dict(checkpoint['optimizer_states']['critic_optimizer'])
                self.b_cell.alpha_optimizer.load_state_dict(checkpoint['optimizer_states']['alpha_optimizer'])
                self.logger.info("Optimizer states ë¡œë“œ ì™„ë£Œ")

            # Load specialized B-Cells (ìƒˆë¡œ ì¶”ê°€)
            if 'specialized_b_cells' in checkpoint and checkpoint['specialized_b_cells']:
                for name, bcell_state in checkpoint['specialized_b_cells'].items():
                    if name in self.b_cells:
                        self.b_cells[name].load_state_dict(bcell_state)
                        self.logger.info(f"Specialized B-Cell [{name}] ë¡œë“œ ì™„ë£Œ")

            # Load replay buffer (ìƒˆë¡œ ì¶”ê°€)
            if 'replay_buffer' in checkpoint and checkpoint['replay_buffer']:
                buffer_data = checkpoint['replay_buffer']
                self.replay_buffer.buffer = buffer_data['buffer']
                if 'priorities' in buffer_data and buffer_data['priorities'] is not None:
                    self.replay_buffer.priorities = buffer_data['priorities']
                self.replay_buffer.ptr = buffer_data.get('ptr', 0)
                self.logger.info(f"Replay buffer ë¡œë“œ ì™„ë£Œ (size: {len(buffer_data['buffer'])})")

            # Load random states (ìƒˆë¡œ ì¶”ê°€)
            if 'random_states' in checkpoint:
                torch.set_rng_state(checkpoint['random_states']['torch'])
                np.random.set_state(checkpoint['random_states']['numpy'])
                if checkpoint['random_states']['python']:
                    random.setstate(checkpoint['random_states']['python'])
                self.logger.info("Random states ë³µì› ì™„ë£Œ")

            # Load Gating Network history (ìƒˆë¡œ ì¶”ê°€)
            if 'gating_history' in checkpoint and checkpoint['gating_history']:
                if hasattr(self.gating_network, 'performance_history'):
                    self.gating_network.performance_history = defaultdict(list, checkpoint['gating_history']['performance_history'])
                    self.gating_network.selection_count = defaultdict(int, checkpoint['gating_history']['selection_count'])
                    self.logger.info("Gating Network history ë¡œë“œ ì™„ë£Œ")

            # Load T-Cell history (ìƒˆë¡œ ì¶”ê°€)
            if 'tcell_history' in checkpoint and checkpoint['tcell_history']:
                if hasattr(self.t_cell, 'crisis_history'):
                    self.t_cell.crisis_history = deque(checkpoint['tcell_history']['crisis_history'], maxlen=100)
                    self.t_cell.detection_stats = checkpoint['tcell_history'].get('detection_stats', {})
                    self.logger.info("T-Cell history ë¡œë“œ ì™„ë£Œ")

            # Load StabilityMonitor state (ìƒˆë¡œ ì¶”ê°€)
            if 'stability_state' in checkpoint and checkpoint['stability_state']:
                if hasattr(self, 'stability_monitor'):
                    self.stability_monitor.alert_counts = checkpoint['stability_state'].get('alert_counts', {})
                    self.stability_monitor.interventions = checkpoint['stability_state'].get('interventions', [])
                    self.logger.info("StabilityMonitor state ë¡œë“œ ì™„ë£Œ")

            # Load memory cell
            if 'memory_cell' in checkpoint:
                memory_data = checkpoint['memory_cell']
                self.memory_cell.memories = memory_data['memories']
                self.memory_cell.memory_stats = memory_data['stats']

            # Load T-Cell state
            if 't_cell' in checkpoint:
                self.t_cell.load_state(checkpoint['t_cell'])

            # ë²„ì „ ì •ë³´ í™•ì¸
            version = checkpoint.get('version', '1.0')
            self.logger.info(f"Full ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {path} (version: {version})")
    
    def _generate_report(self, final_metrics: Dict[str, float]):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'config': self.config.__dict__,
            'final_metrics': final_metrics,
            'training_history': self.metrics_history,
            'component_stats': {
                'gating': self.gating_network.get_statistics(),
                'memory': self.memory_cell.get_statistics(),
                't_cell': self.t_cell.get_statistics(),
                'b_cell': self.b_cell.get_statistics()
            }
        }
        
        report_path = self.session_dir / "reports" / "training_report.json"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        self.logger.info(f"í•™ìŠµ ë³´ê³ ì„œ ì €ì¥: {report_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # Create config
    config = TrainingConfig()
    
    # Override with command line args if needed
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--iql-epochs', type=int, default=100)
    parser.add_argument('--sac-episodes', type=int, default=1000)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--device', type=str, default='auto')
    args = parser.parse_args()
    
    config.iql_epochs = args.iql_epochs
    config.sac_episodes = args.sac_episodes
    config.seed = args.seed
    config.device = args.device
    
    # Create trainer
    trainer = FinFlowTrainer(config)
    
    # Train
    trainer.train()


if __name__ == "__main__":
    main()