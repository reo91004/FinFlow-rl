# Training Guide

FinFlow-RL í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìƒì„¸ ê°€ì´ë“œ

## ëª©ì°¨
- [ê°œìš”](#ê°œìš”)
- [í•™ìŠµ íŒŒì´í”„ë¼ì¸](#í•™ìŠµ-íŒŒì´í”„ë¼ì¸)
- [Phase 1: IQL ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ](#phase-1-iql-ì˜¤í”„ë¼ì¸-ì‚¬ì „í•™ìŠµ)
- [Phase 2: B-Cell ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì •](#phase-2-b-cell-ì˜¨ë¼ì¸-ë¯¸ì„¸ì¡°ì •)
- [í•™ìŠµ ëª¨ë‹ˆí„°ë§](#í•™ìŠµ-ëª¨ë‹ˆí„°ë§)
- [í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹](#í•˜ì´í¼íŒŒë¼ë¯¸í„°-íŠœë‹)
- [ì²´í¬í¬ì¸íŒ…](#ì²´í¬í¬ì¸íŒ…)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)

---

## ê°œìš”

FinFlow-RLì€ 2ë‹¨ê³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œë‹¤:

1. **IQL ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ**: ê³¼ê±° ë°ì´í„°ë¡œ ì•ˆì •ì ì¸ ê°€ì¹˜ í•¨ìˆ˜ í•™ìŠµ
2. **B-Cell ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì •**: Distributional SAC + CQLë¡œ ì‹¤ì‹œê°„ ì ì‘

ì´ ì ‘ê·¼ë²•ì˜ ì¥ì :
- Cold-start ë¬¸ì œ í•´ê²°
- ìƒ˜í”Œ íš¨ìœ¨ì„± í–¥ìƒ
- ì•ˆì •ì ì¸ í•™ìŠµ ì‹œì‘ì 

## í•™ìŠµ íŒŒì´í”„ë¼ì¸

### ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

#### ë°©ë²• 1: main.py ì‚¬ìš© (ê¶Œì¥)
```bash
python main.py --mode train \
    --config configs/default.yaml \
    --iql-epochs 100 \
    --sac-episodes 1000
```

#### ë°©ë²• 2: scripts/train.py ì‚¬ìš©
```bash
python scripts/train.py \
    --config configs/default.yaml \
    --mode full  # full = IQL + SAC
```

### íŒŒì´í”„ë¼ì¸ íë¦„ë„

```
ë°ì´í„° ìˆ˜ì§‘ â†’ IQL ì‚¬ì „í•™ìŠµ â†’ B-Cell ì´ˆê¸°í™” â†’ ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì • â†’ ëª¨ë¸ ì €ì¥
     â†“            â†“              â†“               â†“              â†“
  ì˜¤í”„ë¼ì¸     ê°€ì¹˜í•¨ìˆ˜      ì •ì±… ì „ì´      ì‹¤ì‹œê°„ ì ì‘    SafeTensors
   ë°ì´í„°       í•™ìŠµ                         + CQL
```

---

## Phase 1: IQL ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ

### 1.1 ì˜¤í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘

#### ìë™ ìˆ˜ì§‘ (ê¸°ë³¸)
```python
# trainer.py ë‚´ë¶€ì—ì„œ ìë™ ì²˜ë¦¬
trainer = FinFlowTrainer(config)
trainer.collect_offline_data()  # ìë™ìœ¼ë¡œ ìºì‹±ë¨
```

#### ìˆ˜ë™ ìˆ˜ì§‘
```bash
# ë³„ë„ë¡œ ì˜¤í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘
python scripts/pretrain_iql.py \
    --collect-episodes 1000 \
    --save-path data/offline/dataset.npz
```

### 1.2 IQL ì•Œê³ ë¦¬ì¦˜

IQL(Implicit Q-Learning)ì˜ í•µì‹¬ ê°œë…:

```python
# ê°€ì¹˜ í•¨ìˆ˜ í•™ìŠµ (Expectile Regression)
def value_loss(v, q, expectile=0.7):
    diff = q - v
    weight = torch.where(diff > 0, expectile, 1 - expectile)
    return (weight * diff**2).mean()

# Q í•¨ìˆ˜ í•™ìŠµ (TD Learning)
def q_loss(q, r, next_v, gamma=0.99):
    target = r + gamma * next_v
    return F.mse_loss(q, target.detach())

# ì •ì±… í•™ìŠµ (Advantage Weighted)
def policy_loss(log_prob, q, v, temperature=3.0):
    advantage = q - v
    weight = torch.exp(advantage / temperature)
    return -(weight.detach() * log_prob).mean()
```

### 1.3 í•™ìŠµ ì„¤ì •

#### ì¤‘ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°
```yaml
# configs/default.yaml
train:
  offline_episodes: 500        # ë°ì´í„° ìˆ˜ì§‘ ì—í”¼ì†Œë“œ
  offline_training_epochs: 50  # IQL í•™ìŠµ ì—í­
  offline_batch_size: 512      # ë°°ì¹˜ í¬ê¸°

bcell:
  iql_expectile: 0.7           # ê¸°ëŒ“ê°’ íšŒê·€ íŒŒë¼ë¯¸í„°
  iql_temperature: 3.0         # ì •ì±… ì˜¨ë„
```

### 1.4 ì‹¤í–‰ ì˜ˆì‹œ

```python
from src.core.trainer import FinFlowTrainer

# IQLë§Œ í•™ìŠµ
trainer = FinFlowTrainer(config)
iql_results = trainer.train_iql()

print(f"IQL V-Loss: {iql_results['v_loss']:.4f}")
print(f"IQL Q-Loss: {iql_results['q_loss']:.4f}")
print(f"IQL Policy Loss: {iql_results['policy_loss']:.4f}")
```

---

## Phase 2: B-Cell ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì •

### 2.1 B-Cell ì•„í‚¤í…ì²˜

B-Cellì€ 5ê°œì˜ ì „ë¬¸ ì „ëµì„ ê°€ì§„ Multi-Expert System:

1. **Volatility Expert**: ë³€ë™ì„± ê´€ë¦¬
2. **Correlation Expert**: ìƒê´€ê´€ê³„ ìµœì í™”
3. **Momentum Expert**: ì¶”ì„¸ ì¶”ì¢…
4. **Defensive Expert**: ë°©ì–´ì  í¬ì§€ì…”ë‹
5. **Growth Expert**: ì„±ì¥ ì¶”êµ¬

### 2.2 Distributional SAC

ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë¶„í¬ì  ê°•í™”í•™ìŠµ:

```python
# Quantile Critic ë„¤íŠ¸ì›Œí¬
class QuantileCritic(nn.Module):
    def __init__(self, state_dim, action_dim, n_quantiles=32):
        self.n_quantiles = n_quantiles
        self.network = nn.Sequential(
            nn.Linear(state_dim + action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, n_quantiles)
        )

    def forward(self, state, action):
        # Returns: [batch_size, n_quantiles]
        return self.network(torch.cat([state, action], dim=-1))
```

### 2.3 CQL ì •ê·œí™”

Conservative Q-Learningìœ¼ë¡œ ê³¼ëŒ€í‰ê°€ ë°©ì§€:

```python
def cql_loss(q_values, batch_actions, cql_alpha=5.0):
    # í˜„ì¬ ì •ì±…ì˜ Qê°’
    current_q = q_values.gather(-1, batch_actions)

    # ëª¨ë“  í–‰ë™ì˜ logsumexp
    logsumexp = torch.logsumexp(q_values, dim=-1)

    # CQL í˜ë„í‹°
    cql_penalty = (logsumexp - current_q).mean()

    return cql_alpha * cql_penalty
```

### 2.4 í•™ìŠµ ë£¨í”„

```python
# ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì •
for episode in range(sac_episodes):
    state = env.reset()
    episode_return = 0

    for step in range(max_steps):
        # í–‰ë™ ì„ íƒ
        action = bcell.act(state)

        # í™˜ê²½ ìƒí˜¸ì‘ìš©
        next_state, reward, done, info = env.step(action)

        # ë²„í¼ì— ì €ì¥
        buffer.push(state, action, reward, next_state, done)

        # B-Cell ì—…ë°ì´íŠ¸
        if len(buffer) > min_buffer_size:
            batch = buffer.sample(batch_size)
            losses = bcell.update_sac(batch)

        state = next_state
        episode_return += reward

        if done:
            break

    logger.log_metrics({
        'episode': episode,
        'return': episode_return,
        'sharpe': info['sharpe']
    })
```

---

## í•™ìŠµ ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì 

```python
# src/utils/monitoring.py
monitor = StabilityMonitor(config)

# ì•ˆì •ì„± ì²´í¬
monitor.check_q_values(q_values)  # Qê°’ í­ë°œ ê°ì§€
monitor.check_entropy(entropy)     # ì—”íŠ¸ë¡œí”¼ ê¸‰ë½ ê°ì§€
monitor.check_rewards(rewards)     # ë³´ìƒ ì´ìƒì¹˜ ê°ì§€
```

### TensorBoard ì—°ë™

```bash
# TensorBoard ì‹¤í–‰
tensorboard --logdir logs/

# ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
http://localhost:6006
```

### ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ

| ì§€í‘œ | ì •ìƒ ë²”ìœ„ | ê²½ê³  ì„ê³„ê°’ |
|-----|----------|------------|
| Q-value | -10 ~ 10 | > 100 |
| Entropy | > 0.1 | < 0.01 |
| Actor Loss | < 10 | > 50 |
| Critic Loss | < 5 | > 20 |
| Sharpe Ratio | > 0 | < -0.5 |

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### ìë™ íŠœë‹ (Optuna)

```bash
python src/core/tuning.py \
    --config configs/default.yaml \
    --n-trials 100 \
    --objective sharpe
```

### ì¤‘ìš”ë„ë³„ íŒŒë¼ë¯¸í„°

#### ğŸ”´ ë§¤ìš° ì¤‘ìš”
- `alpha_init`: SAC ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (0.75 ê¶Œì¥)
- `cql_alpha`: CQL ì •ê·œí™” ê°•ë„ (5.0-10.0)
- `lr`: í•™ìŠµë¥  (3e-4)
- `batch_size`: ë°°ì¹˜ í¬ê¸° (256-512)

#### ğŸŸ¡ ì¤‘ìš”
- `iql_expectile`: IQL ê¸°ëŒ“ê°’ (0.7)
- `iql_temperature`: IQL ì˜¨ë„ (3.0)
- `gamma`: í• ì¸ìœ¨ (0.99)
- `tau`: íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (0.005)

#### ğŸŸ¢ ëœ ì¤‘ìš”
- `n_quantiles`: ë¶„ìœ„ìˆ˜ ê°œìˆ˜ (32)
- `hidden_dim`: ì€ë‹‰ì¸µ ì°¨ì› (256)
- `memory_capacity`: ë©”ëª¨ë¦¬ ìš©ëŸ‰ (50000)

### ì‹œì¥ ìƒí™©ë³„ ê¶Œì¥ê°’

#### ë³€ë™ì„± ë†’ì€ ì‹œì¥
```yaml
bcell:
  alpha_init: 1.0      # ë†’ì€ íƒí—˜
  cql_alpha: 10.0      # ê°•í•œ ë³´ìˆ˜ì„±

objectives:
  lambda_cvar: 10.0    # CVaR ì¤‘ì‹œ
  lambda_turn: 0.5     # íšŒì „ìœ¨ í˜ë„í‹° ì¦ê°€
```

#### ì•ˆì •ì ì¸ ì‹œì¥
```yaml
bcell:
  alpha_init: 0.5      # ë‚®ì€ íƒí—˜
  cql_alpha: 5.0       # ì¤‘ê°„ ë³´ìˆ˜ì„±

objectives:
  lambda_cvar: 5.0     # CVaR ì™„í™”
  lambda_turn: 0.1     # íšŒì „ìœ¨ í—ˆìš©
```

---

## ì²´í¬í¬ì¸íŒ…

### SafeTensors í˜•ì‹

```python
# ëª¨ë¸ ì €ì¥
bcell.save("checkpoint.safetensors")

# ë©”íƒ€ë°ì´í„° í¬í•¨ ì €ì¥
save_checkpoint({
    'model': bcell.state_dict(),
    'optimizer': optimizer.state_dict(),
    'episode': episode,
    'metrics': metrics
}, "checkpoint_full.pt")
```

### ì²´í¬í¬ì¸íŠ¸ ì „ëµ

```yaml
train:
  checkpoint_interval: 100    # ì—í”¼ì†Œë“œë§ˆë‹¤
  save_best: true            # ìµœê³  ì„±ëŠ¥ ì €ì¥
  save_latest: true          # ìµœì‹  ì €ì¥
  max_checkpoints: 5         # ìµœëŒ€ ë³´ê´€ ìˆ˜
```

### ì¬ê°œ ë°©ë²•

```bash
# íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
python main.py --mode train \
    --resume logs/20250122_120000/models/checkpoint_best.pt
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: í•™ìŠµ ë¶ˆì•ˆì •

**ì¦ìƒ**: ì†ì‹¤ì´ ë°œì‚°í•˜ê±°ë‚˜ ì§„ë™

**í•´ê²°ì±…**:
```yaml
# í•™ìŠµë¥  ê°ì†Œ
bcell:
  actor_lr: 1e-4  # 3e-4 â†’ 1e-4
  critic_lr: 1e-4

# CQL ê°•í™”
  cql_alpha: 10.0  # 5.0 â†’ 10.0

# ë°°ì¹˜ í¬ê¸° ì¦ê°€
train:
  online_batch_size: 512  # 256 â†’ 512
```

### ë¬¸ì œ 2: ê³¼ì í•©

**ì¦ìƒ**: í•™ìŠµ ì„±ëŠ¥ì€ ì¢‹ìœ¼ë‚˜ í‰ê°€ ì„±ëŠ¥ ë‚˜ì¨

**í•´ê²°ì±…**:
```python
# ë“œë¡­ì•„ì›ƒ ì¶”ê°€
class PolicyNetwork(nn.Module):
    def __init__(self, dropout=0.1):
        self.dropout = nn.Dropout(dropout)

# ë°ì´í„° ì¦ê°•
augmented_state = state + torch.randn_like(state) * 0.01
```

### ë¬¸ì œ 3: ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: CUDA out of memory

**í•´ê²°ì±…**:
```bash
# ë°°ì¹˜ í¬ê¸° ê°ì†Œ
python main.py --batch-size 64

# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
python main.py --gradient-checkpointing

# Mixed Precision Training
python main.py --mixed-precision
```

---

## ì„±ëŠ¥ ìµœì í™”

### GPU ê°€ì†

```python
# Multi-GPU í•™ìŠµ
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)

# Mixed Precision
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    loss = model(batch)
scaler.scale(loss).backward()
```

### ë°ì´í„° ë¡œë”© ìµœì í™”

```python
# ë¹„ë™ê¸° ë°ì´í„° ë¡œë”©
dataloader = DataLoader(
    dataset,
    batch_size=256,
    num_workers=4,      # ë©€í‹°í”„ë¡œì„¸ì‹±
    pin_memory=True,    # GPU ì „ì†¡ ê°€ì†
    prefetch_factor=2   # í”„ë¦¬í˜ì¹˜
)
```

### í”„ë¡œíŒŒì¼ë§

```bash
# PyTorch í”„ë¡œíŒŒì¼ëŸ¬
python -m torch.utils.bottleneck main.py --mode train

# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
python -m memory_profiler main.py --mode train
```

---

## ì‹¤ì „ íŒ

### 1. ë‹¨ê³„ë³„ ì ‘ê·¼
```bash
# Step 1: ì‘ì€ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
python main.py --mode demo

# Step 2: ì˜¤í”„ë¼ì¸ í•™ìŠµë§Œ (IQL/TD3BC)
python scripts/train.py --mode offline

# Step 3: ì „ì²´ íŒŒì´í”„ë¼ì¸
python scripts/train.py --mode full
```

### 2. ë¡œê·¸ ë¶„ì„
```python
# ë©”íŠ¸ë¦­ ì‹œê°í™”
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_json('logs/*/metrics.jsonl', lines=True)
df[['sharpe', 'cvar']].plot()
plt.show()
```

### 3. ì•™ìƒë¸” í•™ìŠµ
```python
# ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ í›„ ì•™ìƒë¸”
models = [train_model(seed=i) for i in range(5)]
ensemble_action = np.mean([m.act(state) for m in models], axis=0)
```

---

## ë‹¤ìŒ ë‹¨ê³„

í•™ìŠµì´ ì™„ë£Œë˜ë©´:
1. [EVALUATION.md](EVALUATION.md) - ëª¨ë¸ í‰ê°€ ë°©ë²•
2. [XAI.md](XAI.md) - ì˜ì‚¬ê²°ì • ì„¤ëª… ë°©ë²•
3. [CONFIGURATION.md](CONFIGURATION.md) - ê³ ê¸‰ ì„¤ì • ì˜µì…˜

---

*Last Updated: 2025-01-22*