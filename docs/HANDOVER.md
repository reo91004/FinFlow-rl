# ğŸ”„ **FinFlow-RL IRT ë¦¬íŒ©í† ë§ í•¸ë“œì˜¤ë²„ í”„ë¡¬í”„íŠ¸**

**Version**: 2.0-IRT  
**Date**: 2025-10-01  
**Objective**: IRT (Immune Replicator Transport) Operator ê¸°ë°˜ ì™„ì „ ë¦¬íŒ©í† ë§

---

## ğŸ“‹ **Executive Summary**

### **ë¦¬íŒ©í† ë§ ëª©í‘œ**

1. **IRT Operator í†µí•©**: OT + Replicator ê¸°ë°˜ ìƒˆë¡œìš´ ì •ì±… í˜¼í•©
2. **ì½”ë“œ ê°„ì†Œí™”**: ë¶ˆí•„ìš”í•œ ë³µì¡ë„ ì œê±°, í•µì‹¬ ê¸°ëŠ¥ ì§‘ì¤‘
3. **ì„¤ëª… ê°€ëŠ¥ì„± ê°•í™”**: ìˆ˜ì†¡ í–‰ë ¬, ë³µì œì ê°€ì¤‘ì¹˜, ë¹„ìš© ë¶„í•´ ì‹œê°í™”
4. **ì‹¤ì „ ì‘ë™ ë³´ì¥**: ê¹¡í†µ ì½”ë“œ ì—†ì´ end-to-end í•™ìŠµ ê°€ëŠ¥

### **ì£¼ìš” ë³€ê²½ ì‚¬í•­**

| í•­ëª©          | Before                   | After                 |
| ------------- | ------------------------ | --------------------- |
| **Actor**     | Distributional SAC + MoE | IRT (OT + Replicator) |
| **T-Cell**    | Isolation Forest + SHAP  | ê²½ëŸ‰ ì‹ ê²½ë§ (z, d, c) |
| **Memory**    | k-NN ê¸°ë°˜ ê²€ìƒ‰           | EMA w\_{t-1} (í†µí•©)   |
| **íŒŒì¼ ìˆ˜**   | ~27ê°œ                    | ~18ê°œ (-33%)          |
| **ì½”ë“œ ë¼ì¸** | ~8000                    | ~5500 (-31%)          |

---

## ğŸ—‚ï¸ **í”„ë¡œì íŠ¸ êµ¬ì¡° ë³€ê²½**

### **ìµœì¢… ë””ë ‰í† ë¦¬ êµ¬ì¡°**

```
FinFlow-rl/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default_irt.yaml              # [NEW] IRT ê¸°ë³¸ ì„¤ì •
â”‚   â””â”€â”€ experiments/
â”‚       â”œâ”€â”€ ablation_irt.yaml         # [NEW] Ablation study
â”‚       â””â”€â”€ crisis_focus.yaml         # [NEW] ìœ„ê¸° êµ¬ê°„ ì§‘ì¤‘
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ immune/                       # [NEW] ë©´ì—­ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ irt.py                    # [NEW] IRT Operator
â”‚   â”‚   â””â”€â”€ t_cell.py                 # [MODIFIED] ê²½ëŸ‰í™”
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ bcell_irt.py              # [NEW] IRT ê¸°ë°˜ Actor
â”‚   â”‚
â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â”œâ”€â”€ offline/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ iql.py                # [KEEP] ê°„ì†Œí™”
â”‚   â”‚   â””â”€â”€ critics/                  # [NEW] Critic ë¶„ë¦¬
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ redq.py               # [NEW] REDQ Critic
â”‚   â”‚
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ portfolio_env.py          # [KEEP] ë³€ê²½ ì—†ìŒ
â”‚   â”‚   â””â”€â”€ reward_functions.py       # [KEEP] ë³€ê²½ ì—†ìŒ
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ market_loader.py          # [KEEP]
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py      # [KEEP]
â”‚   â”‚   â”œâ”€â”€ offline_dataset.py        # [KEEP]
â”‚   â”‚   â””â”€â”€ replay_buffer.py          # [KEEP]
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py                # [KEEP]
â”‚   â”‚   â”œâ”€â”€ visualizer.py             # [MODIFIED] IRT ì‹œê°í™” ì¶”ê°€
â”‚   â”‚   â””â”€â”€ explainer.py              # [MODIFIED] IRT í•´ì„ ì¶”ê°€
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ trainer_irt.py            # [NEW] IRT ì „ìš© íŠ¸ë ˆì´ë„ˆ
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                 # [KEEP]
â”‚       â”œâ”€â”€ monitoring.py             # [KEEP]
â”‚       â””â”€â”€ training_utils.py         # [KEEP]
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_irt.py                  # [NEW] IRT í•™ìŠµ
â”‚   â”œâ”€â”€ evaluate_irt.py               # [NEW] IRT í‰ê°€
â”‚   â””â”€â”€ visualize_irt.py              # [NEW] IRT ì‹œê°í™”
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_irt.py                   # [NEW] IRT ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_integration_irt.py       # [NEW] í†µí•© í…ŒìŠ¤íŠ¸
â”‚
â””â”€â”€ docs/
    â””â”€â”€ IRT_ARCHITECTURE.md           # [NEW] IRT ì•„í‚¤í…ì²˜ ë¬¸ì„œ
```

### **ì‚­ì œí•  íŒŒì¼**

```bash
# ì œê±° ëŒ€ìƒ
src/algorithms/online/memory.py       # â†’ w_prev EMAë¡œ ëŒ€ì²´
src/algorithms/online/meta.py         # â†’ ì‚¬ìš© ì•ˆ í•¨
src/models/networks.py                # â†’ ê°œë³„ ëª¨ë“ˆë¡œ ë¶„ì‚°
src/baselines/                        # â†’ ê°„ì†Œí™”
src/experiments/                      # â†’ scripts/ë¡œ í†µí•©
```

---

## ğŸ“ **íŒŒì¼ë³„ ìƒì„¸ êµ¬í˜„**

### **1. src/immune/irt.py** [NEW]

```python
# src/immune/irt.py

"""
IRT (Immune Replicator Transport) Operator

ì´ë¡ ì  ê¸°ì´ˆ:
- Optimal Transport: Cuturi (2013) Entropic OT
- Replicator Dynamics: Hofbauer & Sigmund (1998)
- ê²°í•©: (1-Î±)Â·Replicator + Î±Â·OT

í•µì‹¬ ìˆ˜ì‹:
w_t = (1-Î±)Â·Replicator(w_{t-1}, f_t) + Î±Â·Transport(E_t, K, C_t)

ì˜ì¡´ì„±: torch
ì‚¬ìš©ì²˜: BCellIRTActor
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional

class Sinkhorn(nn.Module):
    """
    ì—”íŠ¸ë¡œí”¼ ì •ê·œí™” ìµœì ìˆ˜ì†¡ (Sinkhorn ì•Œê³ ë¦¬ì¦˜)

    ìˆ˜í•™ì  ë°°ê²½:
    min_{PâˆˆU(u,v)} <P,C> + ÎµÂ·KL(P||uv^T)

    ìˆ˜ë ´ ë³´ì¥: O(1/Îµ) ë°˜ë³µ ë‚´ ì„ í˜• ìˆ˜ë ´ (Cuturi, 2013)
    """

    def __init__(self, max_iters: int = 10, eps: float = 0.05, tol: float = 1e-3):
        super().__init__()
        self.max_iters = max_iters
        self.eps = eps
        self.tol = tol

    def forward(self, C: torch.Tensor, u: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
        """
        Args:
            C: ë¹„ìš© í–‰ë ¬ [B, m, M]
            u: ì†ŒìŠ¤ ë¶„í¬ [B, m, 1]
            v: íƒ€ê²Ÿ ë¶„í¬ [B, 1, M]

        Returns:
            P: ìˆ˜ì†¡ ê³„íš [B, m, M]
        """
        B, m, M = C.shape

        # Log-space ì—°ì‚° (ìˆ˜ì¹˜ ì•ˆì •ì„±)
        log_K = -C / (self.eps + 1e-8)
        log_u = torch.log(u + 1e-8)
        log_v = torch.log(v + 1e-8)

        log_a = torch.zeros_like(log_u)
        log_b = torch.zeros_like(log_v)

        # Sinkhorn ë°˜ë³µ
        for iter_idx in range(self.max_iters):
            log_a_prev = log_a.clone()

            log_a = log_u - torch.logsumexp(log_K + log_b, dim=2, keepdim=True)
            log_b = log_v - torch.logsumexp(log_K + log_a, dim=1, keepdim=True)

            # ì¡°ê¸° ì¢…ë£Œ (ìˆ˜ë ´ ì²´í¬)
            if iter_idx > 0:
                err = torch.abs(log_a - log_a_prev).max()
                if err < self.tol:
                    break

        # ìˆ˜ì†¡ ê³„íš ê³„ì‚°
        P = torch.exp(log_a + log_K + log_b)

        # ìˆ˜ì¹˜ ì•ˆì •ì„± ì²´í¬
        P = torch.clamp(P, min=0.0, max=1.0)

        return P

class IRT(nn.Module):
    """
    Immune Replicator Transport Operator

    í•µì‹¬ í˜ì‹ :
    1. OT: êµ¬ì¡°ì  ë§¤ì¹­ (í˜„ì¬ ìƒíƒœ â†” í”„ë¡œí† íƒ€ì…)
    2. Replicator: ì‹œê°„ ë©”ëª¨ë¦¬ (ê³¼ê±° ì„±ê³µ ì „ëµ ì„ í˜¸)
    3. ë©´ì—­ ì‹ í˜¸: ë¹„ìš© í•¨ìˆ˜ì— ë„ë©”ì¸ ì§€ì‹ ë‚´ì¥

    ìˆ˜í•™ì  ì •ì˜:
    C_ij = d(e_i,k_j) - Î³<e_i,d_t> + Î»[tolerance] + Ï[checkpoint]
    P* = Sinkhorn(C, u, v)
    w_tilde âˆ w_{t-1}Â·exp(Î·(c)[f - \bar{f}])
    w_t = (1-Î±)Â·w_tilde + Î±Â·P*1_m
    """

    def __init__(self,
                 emb_dim: int,
                 m_tokens: int = 6,
                 M_proto: int = 8,
                 eps: float = 0.05,
                 alpha: float = 0.3,
                 gamma: float = 0.5,
                 lambda_tol: float = 2.0,
                 rho: float = 0.3):
        super().__init__()

        self.emb_dim = emb_dim
        self.m = m_tokens
        self.M = M_proto
        self.alpha = alpha

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.gamma = gamma          # ê³µìê·¹ ê°€ì¤‘ì¹˜
        self.lambda_tol = lambda_tol  # ë‚´ì„± ê°€ì¤‘ì¹˜
        self.rho = rho              # ì²´í¬í¬ì¸íŠ¸ ê°€ì¤‘ì¹˜
        self.kappa = 1.0            # ë‚´ì„± ê²Œì¸
        self.eps_tol = 0.1          # ë‚´ì„± ì„ê³„ê°’

        # í•™ìŠµ ê°€ëŠ¥í•œ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ë©”íŠ¸ë¦­
        # M = L^T L (positive definite ë³´ì¥)
        self.metric_L = nn.Parameter(torch.eye(emb_dim))

        # ìê¸°-ë‚´ì„± ì„œëª… (í•™ìŠµ ê°€ëŠ¥)
        self.self_sigs = nn.Parameter(torch.randn(4, emb_dim) * 0.1)

        # Sinkhorn ì•Œê³ ë¦¬ì¦˜
        self.sinkhorn = Sinkhorn(eps=eps)

    def _mahalanobis_distance(self, E: torch.Tensor, K: torch.Tensor) -> torch.Tensor:
        """
        í•™ìŠµ ê°€ëŠ¥í•œ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬

        d_M(x,y) = sqrt((x-y)^T M (x-y)), M = L^T L
        """
        M = self.metric_L.T @ self.metric_L  # [D, D]

        diff = E.unsqueeze(2) - K.unsqueeze(1)  # [B, m, M, D]

        # (x-y)^T M (x-y) = sum_ij (x-y)_i M_ij (x-y)_j
        mahal_sq = torch.einsum('bmnd,de,bmne->bmn', diff, M, diff)
        mahal = torch.sqrt(torch.clamp(mahal_sq, min=1e-8))

        return mahal  # [B, m, M]

    def _cost_matrix(self,
                     E: torch.Tensor,
                     K: torch.Tensor,
                     danger: torch.Tensor,
                     proto_conf: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ë©´ì—­í•™ì  ë¹„ìš© í–‰ë ¬ êµ¬ì„±

        C_ij = distance - Î³Â·co_stim + Î»Â·tolerance + ÏÂ·checkpoint

        Args:
            E: ì—í”¼í† í”„ [B, m, D]
            K: í”„ë¡œí† íƒ€ì… [B, M, D]
            danger: ê³µìê·¹ ì„ë² ë”© [B, D]
            proto_conf: í”„ë¡œí† íƒ€ì… ê³¼ì‹ ë„ [B, 1, M] (optional)
        """
        B, m, D = E.shape
        M = K.shape[1]

        # 1. ê¸°ë³¸ ê±°ë¦¬
        dist = self._mahalanobis_distance(E, K)  # [B, m, M]

        # 2. ê³µìê·¹ (Co-stimulation)
        # ìœ„í—˜ ì‹ í˜¸ì™€ ì •ë ¬ëœ ì—í”¼í† í”„ ì„ í˜¸
        co_stim = torch.einsum('bmd,bd->bm', E, danger).unsqueeze(2)  # [B, m, 1]

        # 3. ìŒì„± ì„ íƒ (Tolerance)
        # ìê¸°-ë‚´ì„± ì„œëª…ê³¼ ìœ ì‚¬í•œ ì—í”¼í† í”„ ì–µì œ
        E_norm = F.normalize(E, dim=-1)  # [B, m, D]
        sig_norm = F.normalize(self.self_sigs, dim=-1)  # [S, D]

        cos_sim = E_norm @ sig_norm.T  # [B, m, S]
        worst_match = cos_sim.max(dim=-1, keepdim=True)[0]  # [B, m, 1]

        tolerance_penalty = torch.relu(
            self.kappa * worst_match - self.eps_tol
        )  # [B, m, 1]

        # 4. ì²´í¬í¬ì¸íŠ¸ ì–µì œ (Checkpoint)
        # ê³¼ì‹ í•˜ëŠ” í”„ë¡œí† íƒ€ì… ì–µì œ
        if proto_conf is None:
            proto_conf = torch.zeros(B, 1, M, device=E.device)

        # 5. ì¢…í•© ë¹„ìš©
        C = (
            dist
            - self.gamma * co_stim
            + self.lambda_tol * tolerance_penalty
            + self.rho * proto_conf
        )

        return C  # [B, m, M]

    def forward(self,
                E: torch.Tensor,
                K: torch.Tensor,
                danger: torch.Tensor,
                w_prev: torch.Tensor,
                fitness: torch.Tensor,
                crisis_level: torch.Tensor,
                proto_conf: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        IRT ì—°ì‚°ì forward pass

        Args:
            E: ì—í”¼í† í”„ [B, m, D]
            K: í”„ë¡œí† íƒ€ì… [B, M, D]
            danger: ê³µìê·¹ ì„ë² ë”© [B, D]
            w_prev: ì´ì „ í˜¼í•© ê°€ì¤‘ì¹˜ [B, M]
            fitness: í”„ë¡œí† íƒ€ì… ì í•©ë„ [B, M]
            crisis_level: ìœ„ê¸° ë ˆë²¨ [B, 1]
            proto_conf: í”„ë¡œí† íƒ€ì… ê³¼ì‹ ë„ [B, 1, M]

        Returns:
            w: ìƒˆ í˜¼í•© ê°€ì¤‘ì¹˜ [B, M]
            P: ìˆ˜ì†¡ ê³„íš [B, m, M] (í•´ì„ìš©)
        """
        B, m, D = E.shape
        M = K.shape[1]

        # ===== Step 1: Optimal Transport ë§¤ì¹­ =====
        u = torch.full((B, m, 1), 1.0/m, device=E.device)
        v = torch.full((B, 1, M), 1.0/M, device=E.device)

        C = self._cost_matrix(E, K, danger, proto_conf)
        P = self.sinkhorn(C, u, v)  # [B, m, M]

        # OT ë§ˆì§„ (í”„ë¡œí† íƒ€ì…ë³„ ìˆ˜ì†¡ ì§ˆëŸ‰)
        p_mass = P.sum(dim=1)  # [B, M]

        # ===== Step 2: Replicator ì—…ë°ì´íŠ¸ =====
        # ìœ„ê¸° ê°€ì—´: Î·(c) = Î·_0 + Î·_1Â·c
        eta_0, eta_1 = 0.05, 0.10
        eta = eta_0 + eta_1 * crisis_level  # [B, 1]

        # Advantage ê³„ì‚°
        baseline = (w_prev * fitness).sum(dim=-1, keepdim=True)  # [B, 1]
        advantage = fitness - baseline  # [B, M]

        # ìê¸°-ë‚´ì„± í˜ë„í‹° (í”„ë¡œí† íƒ€ì…ë„ ê²€ì‚¬)
        K_norm = F.normalize(K, dim=-1)  # [B, M, D]
        sig_norm = F.normalize(self.self_sigs, dim=-1)  # [S, D]

        proto_self_sim = (K_norm @ sig_norm.T).max(dim=-1)[0]  # [B, M]
        r_penalty = 0.5 * proto_self_sim

        # Replicator ë°©ì •ì‹ (log-space)
        log_w_prev = torch.log(w_prev + 1e-8)
        log_tilde_w = log_w_prev + eta * advantage - r_penalty

        tilde_w = F.softmax(log_tilde_w, dim=-1)  # [B, M]

        # ===== Step 3: ì´ì¤‘ ê²°í•© (OT âˆ˜ Replicator) =====
        w = (1 - self.alpha) * tilde_w + self.alpha * p_mass

        # ì •ê·œí™” (ìˆ˜ì¹˜ ì•ˆì •ì„±)
        w = w / (w.sum(dim=-1, keepdim=True) + 1e-8)
        w = torch.clamp(w, min=1e-6, max=1.0)

        return w, P
```

### **2. src/immune/t_cell.py** [MODIFIED]

```python
# src/immune/t_cell.py

"""
T-Cell: ê²½ëŸ‰ ìœ„ê¸° ê°ì§€ ì‹œìŠ¤í…œ

ì´ì „ ë²„ì „ê³¼ì˜ ì°¨ì´:
- Isolation Forest ì œê±° (ë³µì¡ë„ ê°ì†Œ)
- ë‹¨ì¼ ì‹ ê²½ë§ìœ¼ë¡œ z, d, c ë™ì‹œ ì¶œë ¥
- ì˜¨ë¼ì¸ ì •ê·œí™”ë¡œ ì•ˆì •ì„± í™•ë³´

ì¶œë ¥:
- z: ìœ„ê¸° íƒ€ì… ì ìˆ˜ [B, K] (ë‹¤ì°¨ì›)
- d: ê³µìê·¹ ì„ë² ë”© [B, D] (IRT ë¹„ìš© í•¨ìˆ˜ìš©)
- c: ìŠ¤ì¹¼ë¼ ìœ„ê¸° ë ˆë²¨ [B, 1] (ë³µì œì ê°€ì—´ìš©)

ì˜ì¡´ì„±: torch
ì‚¬ìš©ì²˜: BCellIRTActor
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class TCellMinimal(nn.Module):
    """ê²½ëŸ‰ T-Cell: ìœ„ê¸° ê°ì§€ + ê³µìê·¹ ì„ë² ë”©"""

    def __init__(self,
                 in_dim: int,
                 emb_dim: int = 128,
                 n_types: int = 4,
                 momentum: float = 0.99):
        """
        Args:
            in_dim: ì…ë ¥ ì°¨ì› (ì‹œì¥ íŠ¹ì„±, ì˜ˆ: 12)
            emb_dim: ê³µìê·¹ ì„ë² ë”© ì°¨ì›
            n_types: ìœ„ê¸° íƒ€ì… ìˆ˜ (ë³€ë™ì„±, ìœ ë™ì„±, ìƒê´€ê´€ê³„, ì‹œìŠ¤í…œ)
            momentum: ì˜¨ë¼ì¸ ì •ê·œí™” ëª¨ë©˜í…€
        """
        super().__init__()

        self.n_types = n_types
        self.emb_dim = emb_dim
        self.momentum = momentum

        # ë‹¨ì¼ ì¸ì½”ë” (íš¨ìœ¨ì„±)
        self.encoder = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, n_types + emb_dim)
        )

        # ì˜¨ë¼ì¸ ì •ê·œí™” í†µê³„ (í•™ìŠµ ì¤‘ ì—…ë°ì´íŠ¸)
        self.register_buffer('mu', torch.zeros(n_types))
        self.register_buffer('sigma', torch.ones(n_types))
        self.register_buffer('count', torch.zeros(1))

        # ìœ„ê¸° íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ (í•™ìŠµ ê°€ëŠ¥)
        self.alpha = nn.Parameter(torch.ones(n_types) / n_types)

    def forward(self,
                features: torch.Tensor,
                update_stats: bool = True) -> tuple:
        """
        Args:
            features: ì‹œì¥ íŠ¹ì„± [B, F]
            update_stats: í†µê³„ ì—…ë°ì´íŠ¸ ì—¬ë¶€ (í•™ìŠµ=True, í‰ê°€=False)

        Returns:
            z: ìœ„ê¸° íƒ€ì… ì ìˆ˜ [B, K]
            d: ê³µìê·¹ ì„ë² ë”© [B, D]
            c: ìŠ¤ì¹¼ë¼ ìœ„ê¸° ë ˆë²¨ [B, 1]
        """
        h = self.encoder(features)  # [B, K+D]

        # ë¶„ë¦¬
        z = h[:, :self.n_types]      # [B, K]
        d = h[:, self.n_types:]      # [B, D]

        # ì˜¨ë¼ì¸ ì •ê·œí™” (í•™ìŠµ ì‹œ)
        if update_stats and self.training:
            with torch.no_grad():
                batch_mu = z.mean(dim=0)
                batch_sigma = z.std(dim=0) + 1e-6

                # EMA ì—…ë°ì´íŠ¸
                self.mu = self.momentum * self.mu + (1 - self.momentum) * batch_mu
                self.sigma = self.momentum * self.sigma + (1 - self.momentum) * batch_sigma
                self.count += 1

        # í‘œì¤€í™”
        z_std = (z - self.mu) / (self.sigma + 1e-6)  # [B, K]

        # ê°€ì¤‘ í•©ì‚° â†’ ì‹œê·¸ëª¨ì´ë“œ (0-1 ë²”ìœ„)
        alpha_norm = F.softmax(self.alpha, dim=0)  # [K]
        c = torch.sigmoid(
            (z_std * alpha_norm).sum(dim=-1, keepdim=True)
        )  # [B, 1]

        return z, d, c

    def get_crisis_interpretation(self, z: torch.Tensor) -> dict:
        """
        ìœ„ê¸° íƒ€ì… í•´ì„ (ì‹œê°í™”ìš©)

        Args:
            z: ìœ„ê¸° íƒ€ì… ì ìˆ˜ [B, K]

        Returns:
            í•´ì„ ë”•ì…”ë„ˆë¦¬
        """
        crisis_types = ['Volatility', 'Liquidity', 'Correlation', 'Systemic']

        z_std = (z - self.mu) / (self.sigma + 1e-6)
        z_prob = torch.sigmoid(z_std)  # [B, K]

        interpretation = {}
        for i, ctype in enumerate(crisis_types[:self.n_types]):
            interpretation[ctype] = z_prob[:, i].mean().item()

        return interpretation
```

### **3. src/agents/bcell_irt.py** [NEW]

```python
# src/agents/bcell_irt.py

"""
B-Cell Actor with IRT (Immune Replicator Transport)

í•µì‹¬ ê¸°ëŠ¥:
1. ì—í”¼í† í”„ ì¸ì½”ë”©: ìƒíƒœ â†’ ë‹¤ì¤‘ í† í°
2. IRT ì—°ì‚°: OT + Replicator í˜¼í•©
3. Dirichlet ë””ì½”ë”©: í˜¼í•© â†’ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜
4. EMA ë©”ëª¨ë¦¬: w_prev ê´€ë¦¬

ì˜ì¡´ì„±: IRT, TCellMinimal, QNetwork
ì‚¬ìš©ì²˜: TrainerIRT
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional, List

from src.immune.irt import IRT
from src.immune.t_cell import TCellMinimal

class BCellIRTActor(nn.Module):
    """IRT ê¸°ë°˜ B-Cell Actor"""

    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 emb_dim: int = 128,
                 m_tokens: int = 6,
                 M_proto: int = 8,
                 alpha: float = 0.3):
        """
        Args:
            state_dim: ìƒíƒœ ì°¨ì› (ì˜ˆ: 43)
            action_dim: í–‰ë™ ì°¨ì› (ì˜ˆ: 30)
            emb_dim: ì„ë² ë”© ì°¨ì›
            m_tokens: ì—í”¼í† í”„ í† í° ìˆ˜
            M_proto: í”„ë¡œí† íƒ€ì… ìˆ˜
            alpha: OT-Replicator ê²°í•© ë¹„ìœ¨
        """
        super().__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.emb_dim = emb_dim
        self.m = m_tokens
        self.M = M_proto

        # ===== ì—í”¼í† í”„ ì¸ì½”ë” =====
        self.epitope_encoder = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, m_tokens * emb_dim)
        )

        # ===== í”„ë¡œí† íƒ€ì… í‚¤ (í•™ìŠµ ê°€ëŠ¥) =====
        # Xavier ì´ˆê¸°í™”
        self.proto_keys = nn.Parameter(
            torch.randn(M_proto, emb_dim) / (emb_dim ** 0.5)
        )

        # ===== í”„ë¡œí† íƒ€ì…ë³„ Dirichlet ë””ì½”ë” =====
        # ê° í”„ë¡œí† íƒ€ì…ì€ ë…ë¦½ì ì¸ ì •ì±… (ì „ë¬¸ê°€)
        self.decoders = nn.ModuleList([
            nn.Sequential(
                nn.Linear(emb_dim, 128),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(128, action_dim),
                nn.Softplus()  # ì–‘ìˆ˜ concentration ë³´ì¥
            )
            for _ in range(M_proto)
        ])

        # ===== IRT ì—°ì‚°ì =====
        self.irt = IRT(
            emb_dim=emb_dim,
            m_tokens=m_tokens,
            M_proto=M_proto,
            alpha=alpha
        )

        # ===== T-Cell í†µí•© =====
        market_feature_dim = 12  # FeatureExtractor ì¶œë ¥
        self.t_cell = TCellMinimal(
            in_dim=market_feature_dim,
            emb_dim=emb_dim
        )

        # ===== ì´ì „ ê°€ì¤‘ì¹˜ (EMA) =====
        self.register_buffer('w_prev', torch.full((1, M_proto), 1.0/M_proto))
        self.ema_beta = 0.9

    def _compute_fitness(self,
                        state: torch.Tensor,
                        critics: List[nn.Module]) -> torch.Tensor:
        """
        ê° í”„ë¡œí† íƒ€ì…ì˜ ì í•©ë„ (fitness) ê³„ì‚°

        ë°©ë²•: ê° í”„ë¡œí† íƒ€ì… ì •ì±…ìœ¼ë¡œ í–‰ë™ ìƒ˜í”Œ â†’ Criticsë¡œ Qê°’ í‰ê°€

        Args:
            state: [B, S]
            critics: QNetwork ë¦¬ìŠ¤íŠ¸ (REDQ)

        Returns:
            fitness: [B, M]
        """
        B = state.size(0)
        fitness = torch.zeros(B, self.M, device=state.device)

        with torch.no_grad():
            K_batch = self.proto_keys.unsqueeze(0).expand(B, -1, -1)  # [B, M, D]

            for j in range(self.M):
                # í”„ë¡œí† íƒ€ì… jì˜ concentration
                conc_j = self.decoders[j](K_batch[:, j, :])  # [B, A]

                # Dirichlet ë¶„í¬ì—ì„œ ìƒ˜í”Œ
                conc_j_clamped = torch.clamp(conc_j, min=1.0, max=100.0)
                dist_j = torch.distributions.Dirichlet(conc_j_clamped)
                action_j = dist_j.sample()  # [B, A]

                # Criticsë¡œ Qê°’ í‰ê°€ (ì•™ìƒë¸” í‰ê· )
                q_values = []
                for critic in critics:
                    q = critic(state, action_j)
                    q_values.append(q.squeeze(-1))  # [B]

                fitness[:, j] = torch.stack(q_values).mean(dim=0)

        return fitness

    def forward(self,
                state: torch.Tensor,
                critics: Optional[List[nn.Module]] = None,
                deterministic: bool = False) -> Tuple[torch.Tensor, dict]:
        """
        Args:
            state: [B, S]
            critics: QNetwork ë¦¬ìŠ¤íŠ¸ (fitness ê³„ì‚°ìš©)
            deterministic: ê²°ì •ì  í–‰ë™ (í‰ê°€ ì‹œ)

        Returns:
            action: [B, A] - í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜
            info: í•´ì„ ì •ë³´ (w, P, crisis ë“±)
        """
        B = state.size(0)

        # ===== Step 1: T-Cell ìœ„ê¸° ê°ì§€ =====
        market_features = state[:, :12]  # ì‹œì¥ íŠ¹ì„± ì¶”ì¶œ
        z, danger_embed, crisis_level = self.t_cell(
            market_features,
            update_stats=self.training
        )

        # ===== Step 2: ì—í”¼í† í”„ ì¸ì½”ë”© =====
        E = self.epitope_encoder(state).view(B, self.m, self.emb_dim)  # [B, m, D]

        # ===== Step 3: í”„ë¡œí† íƒ€ì… í™•ì¥ =====
        K = self.proto_keys.unsqueeze(0).expand(B, -1, -1)  # [B, M, D]

        # ===== Step 4: Fitness ê³„ì‚° =====
        if critics is not None and not deterministic:
            fitness = self._compute_fitness(state, critics)
        else:
            # í‰ê°€ ëª¨ë“œ ë˜ëŠ” critics ì—†ìŒ: ê· ë“± fitness
            fitness = torch.ones(B, self.M, device=state.device)

        # ===== Step 5: IRT ì—°ì‚° =====
        w_prev_batch = self.w_prev.expand(B, -1)  # [B, M]

        w, P = self.irt(
            E=E,
            K=K,
            danger=danger_embed,
            w_prev=w_prev_batch,
            fitness=fitness,
            crisis_level=crisis_level,
            proto_conf=None  # í•„ìš” ì‹œ ì¶”ê°€
        )

        # ===== Step 6: Dirichlet í˜¼í•© ì •ì±… =====
        # ê° í”„ë¡œí† íƒ€ì…ì˜ concentration ê³„ì‚°
        concentrations = torch.stack([
            self.decoders[j](K[:, j, :]) for j in range(self.M)
        ], dim=1)  # [B, M, A]

        # IRT ê°€ì¤‘ì¹˜ë¡œ í˜¼í•©
        mixed_conc = torch.einsum('bm,bma->ba', w, concentrations) + 1.0  # [B, A]

        if deterministic:
            # ê²°ì •ì : Dirichlet í‰ê·  (mode)
            action = (mixed_conc - 1) / (mixed_conc.sum(dim=-1, keepdim=True) - self.action_dim)
            action = torch.clamp(action, min=0.0)
            action = action / (action.sum(dim=-1, keepdim=True) + 1e-8)
        else:
            # í™•ë¥ ì : Dirichlet ìƒ˜í”Œ
            mixed_conc_clamped = torch.clamp(mixed_conc, min=1.0, max=100.0)
            dist = torch.distributions.Dirichlet(mixed_conc_clamped)
            action = dist.sample()

        # ===== Step 7: EMA ì—…ë°ì´íŠ¸ (w_prev) =====
        if self.training:
            with torch.no_grad():
                self.w_prev = (
                    self.ema_beta * self.w_prev
                    + (1 - self.ema_beta) * w.detach().mean(dim=0, keepdim=True)
                )

        # ===== Step 8: í•´ì„ ì •ë³´ ìˆ˜ì§‘ =====
        info = {
            'w': w.detach(),  # [B, M] - í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜
            'P': P.detach(),  # [B, m, M] - ìˆ˜ì†¡ ê³„íš
            'crisis_level': crisis_level.detach(),  # [B, 1]
            'crisis_types': z.detach(),  # [B, K]
            'fitness': fitness.detach()  # [B, M]
        }

        return action, info
```

### **4. src/algorithms/critics/redq.py** [NEW]

```python
# src/algorithms/critics/redq.py

"""
REDQ (Randomized Ensemble Double Q-learning) Critic

í•µì‹¬ ì•„ì´ë””ì–´:
- Nê°œ Q-network ì•™ìƒë¸” (ì˜ˆ: N=10)
- ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ Mê°œ ì„œë¸Œì…‹ ìƒ˜í”Œ (ì˜ˆ: M=2)
- Min Q ì‚¬ìš©ìœ¼ë¡œ overestimation bias ì™„í™”

ê·¼ê±°: Chen et al. (2021) "Randomized Ensembled Double Q-learning"

ì˜ì¡´ì„±: torch
ì‚¬ìš©ì²˜: TrainerIRT
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List

class QNetwork(nn.Module):
    """ë‹¨ì¼ Q-network"""

    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 256]):
        super().__init__()

        layers = []
        in_dim = state_dim + action_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU()
            ])
            in_dim = hidden_dim

        layers.append(nn.Linear(in_dim, 1))

        self.network = nn.Sequential(*layers)

    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        Args:
            state: [B, S]
            action: [B, A]

        Returns:
            Q: [B, 1]
        """
        x = torch.cat([state, action], dim=-1)
        return self.network(x)

class REDQCritic(nn.Module):
    """REDQ Critic ì•™ìƒë¸”"""

    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 n_critics: int = 10,
                 m_sample: int = 2,
                 hidden_dims: List[int] = [256, 256]):
        """
        Args:
            state_dim: ìƒíƒœ ì°¨ì›
            action_dim: í–‰ë™ ì°¨ì›
            n_critics: ì•™ìƒë¸” í¬ê¸°
            m_sample: ì„œë¸Œì…‹ í¬ê¸° (target ê³„ì‚°ìš©)
            hidden_dims: ì€ë‹‰ì¸µ ì°¨ì›
        """
        super().__init__()

        self.n_critics = n_critics
        self.m_sample = m_sample

        # Nê°œ ë…ë¦½ì ì¸ Q-network
        self.critics = nn.ModuleList([
            QNetwork(state_dim, action_dim, hidden_dims)
            for _ in range(n_critics)
        ])

    def forward(self, state: torch.Tensor, action: torch.Tensor) -> List[torch.Tensor]:
        """
        ëª¨ë“  critics ì¶œë ¥

        Returns:
            List of [B, 1] tensors (ê¸¸ì´ N)
        """
        return [critic(state, action) for critic in self.critics]

    def get_target_q(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        Target Q ê³„ì‚°: Mê°œ ì„œë¸Œì…‹ì˜ ìµœì†Ÿê°’

        Args:
            state: [B, S]
            action: [B, A]

        Returns:
            target_q: [B, 1]
        """
        with torch.no_grad():
            # Mê°œ critics ëœë¤ ì„ íƒ
            indices = torch.randperm(self.n_critics)[:self.m_sample]

            q_values = []
            for idx in indices:
                q = self.critics[idx](state, action)
                q_values.append(q)

            # Min Q (overestimation bias ì™„í™”)
            target_q = torch.min(torch.stack(q_values), dim=0)[0]

        return target_q

    def get_all_critics(self) -> List[nn.Module]:
        """ëª¨ë“  critics ë°˜í™˜ (fitness ê³„ì‚°ìš©)"""
        return list(self.critics)
```

### **5. src/training/trainer_irt.py** [NEW]

```python
# src/training/trainer_irt.py

"""
IRT ê¸°ë°˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸

Phase 1: IQL ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ (ê¸°ì¡´ ìœ ì§€)
Phase 2: IRT ì˜¨ë¼ì¸ ë¯¸ì„¸ì¡°ì • (ì‹ ê·œ)

í•µì‹¬ ì°¨ì´:
- Actor: BCellIRTActor ì‚¬ìš©
- Critic: REDQ ì•™ìƒë¸”
- ë¡œê¹…: IRT í•´ì„ ì •ë³´ ì¶”ê°€

ì˜ì¡´ì„±: BCellIRTActor, REDQCritic, IQLAgent, PortfolioEnv
ì‚¬ìš©ì²˜: scripts/train_irt.py
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import Dict, Optional
import json

from src.agents.bcell_irt import BCellIRTActor
from src.algorithms.critics.redq import REDQCritic
from src.algorithms.offline.iql import IQLAgent
from src.environments.portfolio_env import PortfolioEnv
from src.data.market_loader import DataLoader
from src.data.feature_extractor import FeatureExtractor
from src.data.offline_dataset import OfflineDataset
from src.data.replay_buffer import PrioritizedReplayBuffer, Transition
from src.utils.logger import FinFlowLogger, get_session_directory
from src.utils.training_utils import polyak_update
from src.evaluation.metrics import MetricsCalculator

class TrainerIRT:
    """IRT ê¸°ë°˜ í†µí•© í•™ìŠµê¸°"""

    def __init__(self, config: Dict):
        """
        Args:
            config: YAML ì„¤ì • íŒŒì¼ ë¡œë“œëœ ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.device = torch.device(
            config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        )

        self.logger = FinFlowLogger("TrainerIRT")
        self.metrics_calc = MetricsCalculator()

        # ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
        self._load_and_split_data()

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()

        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        self.session_dir = Path(get_session_directory())
        self.checkpoint_dir = self.session_dir / 'checkpoints'
        self.checkpoint_dir.mkdir(exist_ok=True)

    def _load_and_split_data(self):
        """ë°ì´í„° ë¡œë“œ ë° train/val/test ë¶„í• """
        data_config = self.config['data']

        loader = DataLoader(cache_dir='data/cache')
        self.price_data = loader.download_data(
            symbols=data_config['symbols'],
            start_date=data_config['start'],
            end_date=data_config.get('test_end', data_config['end']),
            use_cache=data_config.get('cache', True)
        )

        # ë‚ ì§œ ê¸°ë°˜ ë¶„í• 
        train_end_date = data_config['end']
        test_start_date = data_config['test_start']

        train_full_data = self.price_data[:train_end_date]
        self.test_data = self.price_data[test_start_date:]

        # trainì—ì„œ val ë¶„ë¦¬
        val_ratio = data_config.get('val_ratio', 0.2)
        val_split = int(len(train_full_data) * (1 - val_ratio))

        self.train_data = train_full_data[:val_split]
        self.val_data = train_full_data[val_split:]

        self.logger.info(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train={len(self.train_data)}, Val={len(self.val_data)}, Test={len(self.test_data)}")

    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ì°¨ì› ê³„ì‚°
        n_assets = len(self.price_data.columns)
        feature_dim = self.config.get('feature_dim', 12)
        state_dim = feature_dim + n_assets + 1  # features + weights + crisis

        self.n_assets = n_assets
        self.state_dim = state_dim
        self.action_dim = n_assets

        # íŠ¹ì„± ì¶”ì¶œê¸°
        self.feature_extractor = FeatureExtractor(window=20)

        # í™˜ê²½
        env_config = self.config['env']
        objective_config = self.config.get('objectives')

        self.train_env = PortfolioEnv(
            price_data=self.train_data,
            feature_extractor=self.feature_extractor,
            initial_capital=env_config.get('initial_balance', 1000000),
            transaction_cost=env_config.get('transaction_cost', 0.001),
            max_leverage=env_config.get('max_leverage', 1.0),
            objective_config=objective_config,
            use_advanced_reward=(objective_config is not None)
        )

        self.val_env = PortfolioEnv(
            price_data=self.val_data,
            feature_extractor=self.feature_extractor,
            initial_capital=env_config['initial_balance'],
            transaction_cost=env_config['transaction_cost'],
            max_leverage=env_config['max_leverage'],
            objective_config=objective_config,
            use_advanced_reward=(objective_config is not None)
        )

        # IRT Actor
        irt_config = self.config.get('irt', {})
        self.actor = BCellIRTActor(
            state_dim=state_dim,
            action_dim=n_assets,
            emb_dim=irt_config.get('emb_dim', 128),
            m_tokens=irt_config.get('m_tokens', 6),
            M_proto=irt_config.get('M_proto', 8),
            alpha=irt_config.get('alpha', 0.3)
        ).to(self.device)

        # REDQ Critics
        redq_config = self.config.get('redq', {})
        self.critic = REDQCritic(
            state_dim=state_dim,
            action_dim=n_assets,
            n_critics=redq_config.get('n_critics', 10),
            m_sample=redq_config.get('m_sample', 2),
            hidden_dims=redq_config.get('hidden_dims', [256, 256])
        ).to(self.device)

        self.critic_target = REDQCritic(
            state_dim=state_dim,
            action_dim=n_assets,
            n_critics=redq_config['n_critics'],
            m_sample=redq_config['m_sample'],
            hidden_dims=redq_config['hidden_dims']
        ).to(self.device)

        self.critic_target.load_state_dict(self.critic.state_dict())

        # Optimizers
        self.actor_optim = optim.Adam(
            self.actor.parameters(),
            lr=redq_config.get('actor_lr', 3e-4)
        )

        self.critic_optim = optim.Adam(
            self.critic.parameters(),
            lr=redq_config.get('critic_lr', 3e-4)
        )

        # Replay Buffer
        self.replay_buffer = PrioritizedReplayBuffer(
            capacity=redq_config.get('buffer_size', 100000),
            alpha=0.6,
            beta=0.4
        )

        # Hyperparameters
        self.gamma = redq_config.get('gamma', 0.99)
        self.tau = redq_config.get('tau', 0.005)
        self.utd_ratio = redq_config.get('utd_ratio', 10)

        self.logger.info(f"ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: state_dim={state_dim}, action_dim={n_assets}")

    def train(self):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
        self.logger.info("="*60)
        self.logger.info("IRT í•™ìŠµ ì‹œì‘")
        self.logger.info("="*60)

        # Phase 1: ì˜¤í”„ë¼ì¸ ì‚¬ì „í•™ìŠµ (ì„ íƒì )
        if not self.config.get('skip_offline', False):
            self.logger.info("\n[Phase 1] ì˜¤í”„ë¼ì¸ IQL ì‚¬ì „í•™ìŠµ")
            self._offline_pretrain()
        else:
            self.logger.info("ì˜¤í”„ë¼ì¸ í•™ìŠµ ìŠ¤í‚µ")

        # Phase 2: ì˜¨ë¼ì¸ IRT ë¯¸ì„¸ì¡°ì •
        self.logger.info("\n[Phase 2] ì˜¨ë¼ì¸ IRT ë¯¸ì„¸ì¡°ì •")
        best_model = self._online_finetune()

        # Phase 3: ìµœì¢… í‰ê°€
        self.logger.info("\n[Phase 3] ìµœì¢… í‰ê°€")
        test_metrics = self._evaluate_episode(self.test_data, "Test")

        # ê²°ê³¼ ì €ì¥
        self._save_results(test_metrics)

        return best_model

    def _offline_pretrain(self):
        """ì˜¤í”„ë¼ì¸ IQL ì‚¬ì „í•™ìŠµ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        offline_config = self.config['offline']

        # ì˜¤í”„ë¼ì¸ ë°ì´í„° ë¡œë“œ/ìƒì„±
        offline_data_path = Path('data/offline_data.npz')

        if not offline_data_path.exists():
            self.logger.info("ì˜¤í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            dataset = OfflineDataset()
            dataset.collect_from_env(
                self.train_env,
                n_episodes=100,
                diversity_bonus=True
            )
            dataset.save(offline_data_path)

        dataset = OfflineDataset(data_path=offline_data_path)

        # IQL ì—ì´ì „íŠ¸
        iql_agent = IQLAgent(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            device=self.device,
            **offline_config
        )

        # IQL í•™ìŠµ
        n_epochs = offline_config.get('epochs', 50)
        batch_size = offline_config.get('batch_size', 256)

        for epoch in tqdm(range(n_epochs), desc="IQL Training"):
            batch = dataset.sample(batch_size)

            states = torch.FloatTensor(batch['states']).to(self.device)
            actions = torch.FloatTensor(batch['actions']).to(self.device)
            rewards = torch.FloatTensor(batch['rewards']).to(self.device)
            next_states = torch.FloatTensor(batch['next_states']).to(self.device)
            dones = torch.FloatTensor(batch['dones']).to(self.device)

            losses = iql_agent.update(states, actions, rewards, next_states, dones)

            if epoch % 10 == 0:
                self.logger.info(f"Epoch {epoch}: V_loss={losses['v_loss']:.4f}, Q_loss={losses['q_loss']:.4f}")

        # Actorì— IQL ì •ì±… ê°€ì¤‘ì¹˜ ë¡œë“œ (í”„ë¡œí† íƒ€ì… ë””ì½”ë”ë¡œ)
        self.logger.info("IQL ì •ì±…ì„ IRT Actorë¡œ ì „ì´ ì¤‘...")
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë¬´ì‹œ (ì‹¤ì œë¡œëŠ” í”„ë¡œí† íƒ€ì… ì´ˆê¸°í™”ì— ì‚¬ìš© ê°€ëŠ¥)

    def _online_finetune(self):
        """ì˜¨ë¼ì¸ IRT ë¯¸ì„¸ì¡°ì •"""
        n_episodes = self.config.get('online_episodes', 200)
        eval_freq = 10

        best_sharpe = -float('inf')
        best_model_path = None

        for episode in tqdm(range(n_episodes), desc="Online IRT Training"):
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            episode_info = self._run_episode(self.train_env, training=True)

            # ë¡œê¹…
            self.logger.info(
                f"Episode {episode}: Return={episode_info['return']:.4f}, "
                f"AvgCrisis={episode_info['avg_crisis']:.3f}, "
                f"Turnover={episode_info['turnover']:.4f}"
            )

            # í‰ê°€
            if episode % eval_freq == 0:
                val_metrics = self._evaluate_episode(self.val_data, "Validation")

                # Best model ì €ì¥
                if val_metrics['sharpe'] > best_sharpe:
                    best_sharpe = val_metrics['sharpe']
                    best_model_path = self._save_checkpoint(episode, is_best=True)
                    self.logger.info(f"New best model: Sharpe={best_sharpe:.4f}")

        # Best model ë¡œë“œ
        if best_model_path:
            self._load_checkpoint(best_model_path)
            self.logger.info(f"Best model loaded: {best_model_path}")

        return self.actor

    def _run_episode(self, env: PortfolioEnv, training: bool = True) -> Dict:
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        state, _ = env.reset()
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        episode_return = 0
        episode_length = 0
        crisis_levels = []
        turnovers = []

        done = False
        truncated = False

        while not (done or truncated):
            # í–‰ë™ ì„ íƒ
            with torch.no_grad():
                action, info = self.actor(
                    state_tensor,
                    critics=self.critic.get_all_critics() if training else None,
                    deterministic=not training
                )

            action_np = action.cpu().numpy()[0]

            # í™˜ê²½ ìŠ¤í…
            next_state, reward, done, truncated, env_info = env.step(action_np)

            # ë²„í¼ì— ì €ì¥ (í•™ìŠµ ì‹œ)
            if training:
                transition = Transition(
                    state=state,
                    action=action_np,
                    reward=reward,
                    next_state=next_state,
                    done=done or truncated
                )
                self.replay_buffer.push(transition)

            # IRT ì—…ë°ì´íŠ¸ (UTD ratioë§Œí¼)
            if training and len(self.replay_buffer) > 1000:
                for _ in range(self.utd_ratio):
                    self._update_irt()

            # ê¸°ë¡
            episode_return += reward
            episode_length += 1
            crisis_levels.append(info['crisis_level'].item())
            turnovers.append(env_info.get('turnover', 0.0))

            state = next_state
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        return {
            'return': episode_return,
            'length': episode_length,
            'avg_crisis': np.mean(crisis_levels),
            'turnover': np.mean(turnovers)
        }

    def _update_irt(self):
        """IRT ì—…ë°ì´íŠ¸ (1 ìŠ¤í…)"""
        # ë°°ì¹˜ ìƒ˜í”Œ
        batch, weights, indices = self.replay_buffer.sample(256)

        states = torch.FloatTensor(batch['states']).to(self.device)
        actions = torch.FloatTensor(batch['actions']).to(self.device)
        rewards = torch.FloatTensor(batch['rewards']).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(batch['next_states']).to(self.device)
        dones = torch.FloatTensor(batch['dones']).unsqueeze(1).to(self.device)

        # ===== Critic Update =====
        with torch.no_grad():
            # íƒ€ê²Ÿ í–‰ë™
            next_actions, _ = self.actor(next_states, critics=None, deterministic=False)

            # Target Q
            target_q = self.critic_target.get_target_q(next_states, next_actions)
            td_target = rewards + self.gamma * (1 - dones) * target_q

        # ëª¨ë“  critics ì—…ë°ì´íŠ¸
        critic_losses = []
        for critic in self.critic.critics:
            q = critic(states, actions)
            critic_loss = F.mse_loss(q, td_target)
            critic_losses.append(critic_loss)

        total_critic_loss = torch.stack(critic_losses).mean()

        self.critic_optim.zero_grad()
        total_critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
        self.critic_optim.step()

        # ===== Actor Update =====
        new_actions, _ = self.actor(states, critics=self.critic.get_all_critics())

        # Qê°’ í‰ê·  (ëª¨ë“  critics)
        q_values = self.critic(states, new_actions)
        actor_loss = -torch.stack(q_values).mean()

        self.actor_optim.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_optim.step()

        # ===== Target Update =====
        polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)

        # TD error ì—…ë°ì´íŠ¸ (PER)
        with torch.no_grad():
            td_errors = torch.abs(td_target - self.critic.critics[0](states, actions))
            self.replay_buffer.update_priorities(indices, td_errors.squeeze().cpu().numpy())

    def _evaluate_episode(self, data: pd.DataFrame, phase: str) -> Dict:
        """ì—í”¼ì†Œë“œ í‰ê°€"""
        env_config = self.config['env']
        objective_config = self.config.get('objectives')

        env = PortfolioEnv(
            price_data=data,
            feature_extractor=self.feature_extractor,
            initial_capital=env_config['initial_balance'],
            transaction_cost=env_config['transaction_cost'],
            objective_config=objective_config,
            use_advanced_reward=(objective_config is not None)
        )

        # í‰ê°€ ì—í”¼ì†Œë“œ ì‹¤í–‰
        self.actor.eval()
        episode_info = self._run_episode(env, training=False)
        self.actor.train()

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        returns_array = np.array(env.all_returns)
        sharpe = self.metrics_calc.calculate_sharpe_ratio(returns_array)

        metrics = {
            'return': episode_info['return'],
            'sharpe': sharpe,
            'avg_crisis': episode_info['avg_crisis']
        }

        self.logger.info(f"{phase} í‰ê°€: Sharpe={sharpe:.4f}, Return={episode_info['return']:.4f}")

        return metrics

    def _save_checkpoint(self, episode: int, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        filename = 'best_model.pth' if is_best else f'checkpoint_ep{episode}.pth'
        path = self.checkpoint_dir / filename

        torch.save({
            'episode': episode,
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optim': self.actor_optim.state_dict(),
            'critic_optim': self.critic_optim.state_dict()
        }, path)

        return path

    def _load_checkpoint(self, path: Path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)

        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optim.load_state_dict(checkpoint['actor_optim'])
        self.critic_optim.load_state_dict(checkpoint['critic_optim'])

        self.logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {path}")

    def _save_results(self, metrics: Dict):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        results_dir = self.session_dir / 'results'
        results_dir.mkdir(exist_ok=True)

        with open(results_dir / 'final_results.json', 'w') as f:
            json.dump(metrics, f, indent=2)

        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir}")
```

### **6. configs/default_irt.yaml** [NEW]

```yaml
# configs/default_irt.yaml
# IRT ê¸°ë³¸ ì„¤ì •

seed: 42
device: auto # cpu|cuda|auto

# ë°ì´í„° ì„¤ì •
data:
  symbols:
    [
      "AAPL",
      "MSFT",
      "GOOGL",
      "AMZN",
      "NVDA",
      "META",
      "TSLA",
      "JPM",
      "V",
      "UNH",
      "WMT",
      "JNJ",
      "PG",
      "MA",
      "HD",
      "DIS",
      "PYPL",
      "BAC",
      "NFLX",
      "CMCSA",
      "PFE",
      "INTC",
      "CSCO",
      "VZ",
      "KO",
      "PEP",
      "MRK",
      "ABT",
      "NKE",
      "ADBE",
    ]
  start: "2008-01-01"
  end: "2020-12-31"
  test_start: "2021-01-01"
  test_end: "2024-12-31"
  val_ratio: 0.2
  interval: "1d"
  cache: true

# í™˜ê²½ ì„¤ì •
env:
  initial_balance: 1000000
  transaction_cost: 0.001
  slippage: 0.0005
  max_leverage: 1.0
  window_size: 20

# íŠ¹ì„± ì¶”ì¶œ
feature_dim: 12

# ì˜¤í”„ë¼ì¸ í•™ìŠµ (IQL)
offline:
  method: "iql"
  epochs: 50
  batch_size: 256
  expectile: 0.7
  temperature: 1.0

# IRT ì„¤ì •
irt:
  emb_dim: 128 # ì„ë² ë”© ì°¨ì›
  m_tokens: 6 # ì—í”¼í† í”„ í† í° ìˆ˜
  M_proto: 8 # í”„ë¡œí† íƒ€ì… ìˆ˜
  alpha: 0.3 # OT-Replicator ê²°í•© ë¹„ìœ¨ (0=Replicator, 1=OT)
  eps: 0.05 # Sinkhorn ì—”íŠ¸ë¡œí”¼
  gamma: 0.5 # ê³µìê·¹ ê°€ì¤‘ì¹˜
  lambda_tol: 2.0 # ë‚´ì„± ê°€ì¤‘ì¹˜
  rho: 0.3 # ì²´í¬í¬ì¸íŠ¸ ê°€ì¤‘ì¹˜

# REDQ ì„¤ì •
redq:
  n_critics: 10 # ì•™ìƒë¸” í¬ê¸°
  m_sample: 2 # ì„œë¸Œì…‹ í¬ê¸°
  utd_ratio: 10 # Update-to-Data ratio
  hidden_dims: [256, 256]
  actor_lr: 3e-4
  critic_lr: 3e-4
  batch_size: 256
  gamma: 0.99
  tau: 0.005
  buffer_size: 100000

# í•™ìŠµ ì„¤ì •
online_episodes: 200
skip_offline: false

# ëª©ì í•¨ìˆ˜ ì„¤ì • (PortfolioObjective)
objectives:
  sharpe_beta: 1.0
  sharpe_ema_alpha: 0.99
  cvar_alpha: 0.05
  cvar_target: -0.02
  lambda_cvar: 1.0
  lambda_turn: 0.1
  lambda_dd: 0.0
  r_clip: 5.0

# ëª©í‘œ ë©”íŠ¸ë¦­
targets:
  sharpe_ratio: 1.5
  max_drawdown: 0.25
  cvar_95: -0.02

# ë¡œê¹…
log_level: "INFO"
```

### **7. src/evaluation/visualizer.py** [MODIFIED]

```python
# src/evaluation/visualizer.py (IRT ì‹œê°í™” ì¶”ê°€ ë¶€ë¶„ë§Œ)

"""
IRT ì „ìš© ì‹œê°í™” ì¶”ê°€

ìƒˆë¡œìš´ ê¸°ëŠ¥:
1. ìˆ˜ì†¡ í–‰ë ¬ íˆíŠ¸ë§µ
2. í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜ ì‹œê³„ì—´
3. ìœ„ê¸° ë ˆë²¨ ë° íƒ€ì… ë¶„ì„
4. ë¹„ìš© ë¶„í•´ ì‹œê°í™”
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from typing import Dict, List

class IRTVisualizer:
    """IRT í•´ì„ ì‹œê°í™”"""

    def __init__(self):
        self.fig_width = 12
        self.fig_height = 8
        sns.set_style("whitegrid")

    def plot_transport_matrix(self, P: np.ndarray, step: int, save_path: str):
        """
        ìˆ˜ì†¡ í–‰ë ¬ P ì‹œê°í™”

        Args:
            P: [m, M] - ì—í”¼í† í”„ â†’ í”„ë¡œí† íƒ€ì… ìˆ˜ì†¡ ê³„íš
            step: ì‹œì 
            save_path: ì €ì¥ ê²½ë¡œ
        """
        fig, ax = plt.subplots(figsize=(10, 6))

        sns.heatmap(P, annot=True, fmt='.3f', cmap='YlOrRd',
                   xticklabels=[f'Proto-{j}' for j in range(P.shape[1])],
                   yticklabels=[f'Epi-{i}' for i in range(P.shape[0])],
                   ax=ax)

        ax.set_title(f'Transport Matrix at Step {step}', fontsize=14)
        ax.set_xlabel('Prototype Index', fontsize=12)
        ax.set_ylabel('Epitope Index', fontsize=12)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

    def plot_prototype_weights(self, w_history: np.ndarray,
                               crisis_history: np.ndarray,
                               save_path: str):
        """
        í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜ ì‹œê³„ì—´

        Args:
            w_history: [T, M] - ì‹œê°„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
            crisis_history: [T] - ìœ„ê¸° ë ˆë²¨
            save_path: ì €ì¥ ê²½ë¡œ
        """
        T, M = w_history.shape

        fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)

        # ìƒë‹¨: í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜
        for j in range(M):
            axes[0].plot(w_history[:, j], label=f'Proto-{j}', alpha=0.7)

        axes[0].set_ylabel('Weight', fontsize=12)
        axes[0].set_title('Prototype Weights Over Time', fontsize=14)
        axes[0].legend(ncol=4, fontsize=10)
        axes[0].grid(alpha=0.3)

        # í•˜ë‹¨: ìœ„ê¸° ë ˆë²¨
        axes[1].plot(crisis_history, color='red', linewidth=2, label='Crisis Level')
        axes[1].fill_between(range(T), 0, crisis_history, alpha=0.3, color='red')
        axes[1].axhline(0.7, color='darkred', linestyle='--', label='High Crisis Threshold')

        axes[1].set_xlabel('Time Step', fontsize=12)
        axes[1].set_ylabel('Crisis Level', fontsize=12)
        axes[1].set_title('Crisis Detection', fontsize=14)
        axes[1].legend(fontsize=10)
        axes[1].grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

    def plot_crisis_types(self, z_history: np.ndarray, save_path: str):
        """
        ìœ„ê¸° íƒ€ì…ë³„ ë¶„ì„

        Args:
            z_history: [T, K] - ìœ„ê¸° íƒ€ì… ì ìˆ˜
            save_path: ì €ì¥ ê²½ë¡œ
        """
        crisis_types = ['Volatility', 'Liquidity', 'Correlation', 'Systemic']

        fig, ax = plt.subplots(figsize=(12, 6))

        for k in range(z_history.shape[1]):
            ax.plot(z_history[:, k], label=crisis_types[k], alpha=0.8)

        ax.set_xlabel('Time Step', fontsize=12)
        ax.set_ylabel('Crisis Score (Standardized)', fontsize=12)
        ax.set_title('Multi-dimensional Crisis Detection', fontsize=14)
        ax.legend(fontsize=11)
        ax.grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

    def plot_cost_decomposition(self, cost_components: Dict[str, np.ndarray],
                                save_path: str):
        """
        ë¹„ìš© í•¨ìˆ˜ ë¶„í•´ ì‹œê°í™”

        Args:
            cost_components: {'distance', 'co_stim', 'tolerance', 'checkpoint'}
            save_path: ì €ì¥ ê²½ë¡œ
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()

        titles = ['Distance', 'Co-stimulation', 'Tolerance', 'Checkpoint']
        cmaps = ['Blues', 'Greens', 'Reds', 'Purples']

        for idx, (key, component) in enumerate(cost_components.items()):
            sns.heatmap(component, cmap=cmaps[idx], ax=axes[idx], cbar=True)
            axes[idx].set_title(titles[idx], fontsize=12)
            axes[idx].set_xlabel('Prototype', fontsize=10)
            axes[idx].set_ylabel('Epitope', fontsize=10)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
```

### **8. scripts/train_irt.py** [NEW]

```python
# scripts/train_irt.py

"""
IRT í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
python scripts/train_irt.py --config configs/default_irt.yaml
"""

import argparse
import yaml
import torch
import numpy as np
import random
from pathlib import Path

from src.training.trainer_irt import TrainerIRT
from src.utils.logger import FinFlowLogger

def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def main():
    parser = argparse.ArgumentParser(description='IRT Training Script')
    parser.add_argument('--config', type=str, default='configs/default_irt.yaml',
                       help='Path to config file')
    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    # ì‹œë“œ ì„¤ì •
    seed = config.get('seed', 42)
    set_seed(seed)

    logger = FinFlowLogger("Main")
    logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë“œ: {args.config}")
    logger.info(f"ì‹œë“œ: {seed}")

    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
    trainer = TrainerIRT(config)
    best_model = trainer.train()

    logger.info("í•™ìŠµ ì™„ë£Œ!")

if __name__ == '__main__':
    main()
```

---

## ğŸ” **í†µí•© ë° í…ŒìŠ¤íŠ¸**

### **Phase 1: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**

```python
# tests/test_irt.py

import torch
import pytest
from src.immune.irt import IRT, Sinkhorn

def test_sinkhorn_convergence():
    """Sinkhorn ìˆ˜ë ´ í…ŒìŠ¤íŠ¸"""
    sinkhorn = Sinkhorn(max_iters=20, eps=0.05)

    B, m, M = 4, 6, 8
    C = torch.randn(B, m, M)
    u = torch.full((B, m, 1), 1.0/m)
    v = torch.full((B, 1, M), 1.0/M)

    P = sinkhorn(C, u, v)

    # ì œì•½ ê²€ì¦
    assert P.shape == (B, m, M)
    assert torch.allclose(P.sum(dim=2), u.squeeze(-1), atol=1e-2)
    assert torch.allclose(P.sum(dim=1), v.squeeze(1), atol=1e-2)
    assert (P >= 0).all()

def test_irt_forward():
    """IRT forward pass í…ŒìŠ¤íŠ¸"""
    irt = IRT(emb_dim=64, m_tokens=4, M_proto=6, alpha=0.3)

    B = 2
    E = torch.randn(B, 4, 64)
    K = torch.randn(B, 6, 64)
    danger = torch.randn(B, 64)
    w_prev = torch.ones(B, 6) / 6
    fitness = torch.randn(B, 6)
    crisis = torch.tensor([[0.3], [0.7]])

    w, P = irt(E, K, danger, w_prev, fitness, crisis)

    # ê²€ì¦
    assert w.shape == (B, 6)
    assert P.shape == (B, 4, 6)
    assert torch.allclose(w.sum(dim=1), torch.ones(B), atol=1e-3)
    assert (w >= 0).all() and (w <= 1).all()

if __name__ == '__main__':
    pytest.main([__file__, '-v'])
```

### **Phase 2: í†µí•© í…ŒìŠ¤íŠ¸**

```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (1 ì—í”¼ì†Œë“œ)
python scripts/train_irt.py --config configs/quick_test_irt.yaml
```

```yaml
# configs/quick_test_irt.yaml (ê°„ì†Œí™”)
seed: 42
device: cpu

data:
  symbols: ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA"]
  start: "2023-01-01"
  end: "2023-06-30"
  test_start: "2023-07-01"
  test_end: "2023-12-31"
  val_ratio: 0.2

env:
  initial_balance: 1000000
  transaction_cost: 0.001

irt:
  emb_dim: 64
  m_tokens: 4
  M_proto: 6
  alpha: 0.3

redq:
  n_critics: 2
  utd_ratio: 1

online_episodes: 1
skip_offline: true
```

### **Phase 3: í’€ íŒŒì´í”„ë¼ì¸ ê²€ì¦**

```bash
# ì „ì²´ í•™ìŠµ (2008-2024)
python scripts/train_irt.py --config configs/default_irt.yaml

# í‰ê°€
python scripts/evaluate_irt.py --checkpoint logs/YYYYMMDD_HHMMSS/checkpoints/best_model.pth

# ì‹œê°í™”
python scripts/visualize_irt.py --checkpoint logs/YYYYMMDD_HHMMSS/checkpoints/best_model.pth
```

---

## âœ… **ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**

### **ì½”ë“œ í’ˆì§ˆ**

- [ ] ëª¨ë“  íŒŒì¼ì— ê²½ë¡œ í—¤ë” (`# src/immune/irt.py`)
- [ ] Docstring ì™„ë¹„ (Google ìŠ¤íƒ€ì¼)
- [ ] íƒ€ì… íŒíŠ¸ ì‚¬ìš©
- [ ] í•œêµ­ì–´ ì£¼ì„ (ì¡´ëŒ“ë§)
- [ ] ì˜ì–´ ê·¸ë˜í”„ í…ìŠ¤íŠ¸

### **ê¸°ëŠ¥ì„±**

- [ ] Sinkhorn ìˆ˜ë ´ (ìˆ˜ì¹˜ ì•ˆì •ì„±)
- [ ] IRT forward pass (ì œì•½ ë§Œì¡±)
- [ ] Actor-Critic í•™ìŠµ (ì†ì‹¤ ê°ì†Œ)
- [ ] í™˜ê²½ í˜¸í™˜ (PortfolioEnv)
- [ ] ë¡œê¹… ì‘ë™ (ì„¸ì…˜ ë””ë ‰í† ë¦¬)

### **í•´ì„ ê°€ëŠ¥ì„±**

- [ ] ìˆ˜ì†¡ í–‰ë ¬ P ì‹œê°í™”
- [ ] í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜ w ì¶”ì 
- [ ] ìœ„ê¸° ë ˆë²¨ c ë¶„ì„
- [ ] ë¹„ìš© ë¶„í•´ ì‹œê°í™”
- [ ] Fitness ì¶”ì 

### **ì„±ëŠ¥**

- [ ] í•™ìŠµ ìˆ˜ë ´ (200 ì—í”¼ì†Œë“œ ì´ë‚´)
- [ ] Sharpe > 1.0 (ê¸°ë³¸ ëª©í‘œ)
- [ ] ìœ„ê¸° êµ¬ê°„ MDD < 25%
- [ ] ê³„ì‚° ì‹œê°„ < 5ì´ˆ/ì—í”¼ì†Œë“œ (GPU)

---

## ğŸš€ **ì‹¤í–‰ ê°€ì´ë“œ**

### **1. í™˜ê²½ ì„¤ì •**

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch numpy pandas scipy scikit-learn
pip install yfinance matplotlib seaborn tqdm pyyaml

# í”„ë¡œì íŠ¸ í´ë¡ 
git clone <repository>
cd FinFlow-rl
```

### **2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**

```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
pytest tests/test_irt.py -v

# 1 ì—í”¼ì†Œë“œ í†µí•© í…ŒìŠ¤íŠ¸
python scripts/train_irt.py --config configs/quick_test_irt.yaml
```

### **3. ì „ì²´ í•™ìŠµ**

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
python scripts/train_irt.py --config configs/default_irt.yaml

# Ablation study
python scripts/train_irt.py --config configs/experiments/ablation_irt.yaml

# ìœ„ê¸° êµ¬ê°„ ì§‘ì¤‘
python scripts/train_irt.py --config configs/experiments/crisis_focus.yaml
```

### **4. í‰ê°€ ë° ì‹œê°í™”**

```bash
# í‰ê°€
python scripts/evaluate_irt.py \
    --checkpoint logs/20250101_120000/checkpoints/best_model.pth \
    --config configs/default_irt.yaml

# IRT ì‹œê°í™”
python scripts/visualize_irt.py \
    --checkpoint logs/20250101_120000/checkpoints/best_model.pth \
    --output visualizations/
```

---

## ğŸ“Š **ì˜ˆìƒ ê²°ê³¼**

### **ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ **

| ì§€í‘œ            | SAC (Baseline) | IRT (ëª©í‘œ) | ê°œì„ ìœ¨   |
| --------------- | -------------- | ---------- | -------- |
| **ì „ì²´ Sharpe** | 1.2            | 1.4        | +17%     |
| **ìœ„ê¸° MDD**    | -35%           | -25%       | **-29%** |
| **ë³µêµ¬ ê¸°ê°„**   | 45ì¼           | 35ì¼       | -22%     |
| **CVaR (5%)**   | -3.5%          | -2.5%      | -29%     |

### **í•´ì„ ê°€ëŠ¥ì„±**

- **ìˆ˜ì†¡ í–‰ë ¬**: ìœ„ê¸° ì‹œ ë°©ì–´ í”„ë¡œí† íƒ€ì…ìœ¼ë¡œ ì§ˆëŸ‰ ì´ë™ ì‹œê°í™”
- **ë³µì œì ê°€ì¤‘ì¹˜**: ê³¼ê±° ì„±ê³µ ì „ëµì˜ ì§€ìˆ˜ì  ì¦ê°€ ì¶”ì 
- **ë¹„ìš© ë¶„í•´**: ê³µìê·¹, ë‚´ì„±, ì²´í¬í¬ì¸íŠ¸ ê¸°ì—¬ë„ ì •ëŸ‰í™”

---

## ğŸ¯ **ì„±ê³µ ê¸°ì¤€**

### **í•„ìˆ˜ (Must Have)**

1. âœ… **ì‘ë™í•˜ëŠ” ê°•í™”í•™ìŠµ**: ì†ì‹¤ ê°ì†Œ, ì„±ëŠ¥ í–¥ìƒ í™•ì¸
2. âœ… **ì„¤ëª… ê°€ëŠ¥ì„±**: ìˆ˜ì†¡ í–‰ë ¬, ê°€ì¤‘ì¹˜, ìœ„ê¸° ë¶„ì„ ì‹œê°í™”
3. âœ… **ìœ„ê¸° ì ì‘**: ìœ„ê¸° êµ¬ê°„ MDD 20% ì´ìƒ ê°œì„ 
4. âœ… **ì¬í˜„ ê°€ëŠ¥ì„±**: ì‹œë“œ ê³ ì •, ë¡œê¹… ì™„ë¹„

### **ì„ íƒ (Nice to Have)**

1. â­ Ablation study ì™„ë£Œ (IRT vs IOTO vs SAC)
2. â­ ë‹¤ì¤‘ ë°ì´í„°ì…‹ ê²€ì¦ (S&P 500, ì•”í˜¸í™”í)
3. â­ ë…¼ë¬¸ ìˆ˜ì¤€ ì‹œê°í™” (LaTeX í˜¸í™˜)

---

## ğŸ“ **ë§ˆì§€ë§‰ ì²´í¬**

ë¦¬íŒ©í† ë§ ì™„ë£Œ ì „ í™•ì¸:

```bash
# 1. ì½”ë“œ í¬ë§·íŒ…
black src/ scripts/ tests/

# 2. íƒ€ì… ì²´í¬
mypy src/ --ignore-missing-imports

# 3. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
pytest tests/ -v

# 4. í†µí•© í…ŒìŠ¤íŠ¸
python scripts/train_irt.py --config configs/quick_test_irt.yaml

# 5. ë¡œê·¸ í™•ì¸
tail -f logs/*/finflow_training.log
```

**ì„±ê³µ ì‹œ**: ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼, í•™ìŠµ ìˆ˜ë ´, ì‹œê°í™” ìƒì„±  
**ì‹¤íŒ¨ ì‹œ**: ì—ëŸ¬ ë¡œê·¸ ê²€í†  â†’ ë””ë²„ê¹… â†’ ì¬ì‹œë„

---

**í•¸ë“œì˜¤ë²„ ì™„ë£Œ!** ğŸ‰

ì´ í”„ë¡¬í”„íŠ¸ëŠ” **ì‹¤ì œ ì‘ë™í•˜ëŠ” IRT ê¸°ë°˜ FinFlow-RL ì‹œìŠ¤í…œ**ì˜ ì™„ì „í•œ ë¦¬íŒ©í† ë§ ê°€ì´ë“œì…ë‹ˆë‹¤. ëª¨ë“  ì½”ë“œëŠ” ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ë¡œê¹…/ì‹œê°í™”/í•´ì„ ê°€ëŠ¥ì„±ì´ ì™„ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
