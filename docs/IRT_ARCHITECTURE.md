# IRT Architecture Documentation

## Overview

IRT (Immune Replicator Transport) is a novel policy mixing operator that combines Optimal Transport with Replicator Dynamics for crisis-adaptive portfolio management.

## Mathematical Foundation

### Core Equation
```
w_t = (1-Î±)Â·Replicator(w_{t-1}, f_t) + Î±Â·Transport(E_t, K, C_t)
```

Where:
- `w_t`: Portfolio weights at time t
- `Î±`: Mixing coefficient (0=pure replicator, 1=pure OT)
- `E_t`: Epitope embeddings (current state features)
- `K`: Prototype keys (expert strategies)
- `C_t`: Cost matrix with immunological biases

### Cost Matrix Design
```
C_ij = d(e_i,k_j) - Î³<e_i,d_t> + Î»[tolerance] + Ï[checkpoint]
```

Components:
- **Distance**: Mahalanobis distance between epitopes and prototypes
- **Co-stimulation**: Alignment with danger signals (-Î³ term)
- **Tolerance**: Suppression of self-similar patterns (Î» term)
- **Checkpoint**: Overconfidence penalty (Ï term)

## Architecture Components

### 1. IRT Operator (`src/immune/irt.py`)
- **Sinkhorn Algorithm**: Entropic optimal transport solver
- **Mahalanobis Metric**: Learnable distance metric
- **Replicator Dynamics**: Time memory via w_{t-1}
- **Debug Output**: Returns `(w, P, debug_info)` where debug_info contains:
  - `w_rep`: Replicator-only weights [B, M]
  - `w_ot`: OT-only weights [B, M]
  - `cost_matrix`: Immunological costs [B, m, M]
  - `eta`: Crisis-adaptive learning rate [B, 1]

### 2. T-Cell (`src/immune/t_cell.py`)
- **Crisis Detection**: Multi-dimensional crisis types
- **Danger Embedding**: Co-stimulation signals
- **Crisis Level**: Scalar crisis intensity

### 3. B-Cell Actor (`src/agents/bcell_irt.py`)
- **Epitope Encoder**: State â†’ Multiple tokens
- **Prototype Decoders**: M independent Dirichlet policies
- **IRT Integration**: Weight mixing via OT+Replicator

### 4. REDQ Critics (`src/algorithms/critics/redq.py`)
- **Ensemble**: N Q-networks (default N=10)
- **Min-Q Target**: Conservative value estimation
- **High UTD Ratio**: Sample-efficient learning

## Training Pipeline

### Phase 1: Offline Pretraining (Optional)
- IQL (Implicit Q-Learning) with expectile regression
- Conservative value learning from historical data

### Phase 2: Online IRT Fine-tuning
- REDQ critics for stable Q-learning
- IRT actor for adaptive policy mixing
- Crisis-aware exploration/exploitation

## Key Innovations

### 1. Time Memory
Unlike attention/MoE, IRT maintains explicit time memory through w_{t-1}, preventing collapse to softmax in single-token limit.

### 2. Immunological Inductive Bias
Cost function incorporates domain knowledge:
- Co-stimulation for crisis signals
- Tolerance for failure avoidance
- Checkpoint for overconfidence suppression

### 3. Crisis Adaptation
Dynamic learning rate Î·(c) = Î·_0 + Î·_1Â·c automatically increases adaptation speed during crises.

## Performance Targets

| Metric | Baseline (SAC) | IRT Target | Improvement |
|--------|---------------|-----------|-------------|
| Sharpe Ratio | 1.2 | 1.4+ | +17% |
| Crisis MDD | -35% | -25% | -29% |
| Recovery Time | 45 days | 35 days | -22% |
| CVaR (5%) | -3.5% | -2.5% | -29% |

## Configuration

### Default Settings (`configs/default_irt.yaml`)
```yaml
irt:
  # Basic Structure
  emb_dim: 128       # Embedding dimension
  m_tokens: 6        # Number of epitope tokens
  M_proto: 8         # Number of prototypes
  alpha: 0.3         # OT-Replicator mixing ratio

  # Sinkhorn Algorithm
  eps: 0.10          # Sinkhorn entropy (updated: 0.05 â†’ 0.10)
  max_iters: 10      # Maximum iterations
  tol: 0.001         # Convergence threshold

  # Cost Function Weights
  gamma: 0.5         # Co-stimulation weight
  lambda_tol: 2.0    # Tolerance weight
  rho: 0.3           # Checkpoint weight

  # Crisis Heating (Replicator)
  eta_0: 0.05        # Base learning rate
  eta_1: 0.15        # Crisis increase (updated: 0.10 â†’ 0.15)

  # Self-Tolerance
  kappa: 1.0         # Tolerance gain
  eps_tol: 0.1       # Tolerance threshold
  n_self_sigs: 4     # Number of self signatures

  # EMA Memory
  ema_beta: 0.9      # Exponential moving average coefficient

objectives:
  lambda_turn: 0.01  # Turnover penalty (updated: 0.1 â†’ 0.01)
  lambda_cvar: 1.0   # CVaR constraint weight
  lambda_dd: 0.0     # Drawdown penalty
```

### Ablation Studies
- `Î±=0`: Pure Replicator (temporal consistency)
- `Î±=0.3`: Balanced (default)
- `Î±=1`: Pure OT (structural matching)

### Hyperparameter Tuning Guide

#### Problem: No-Trade Loop (ë¬´ê±°ë˜ ë£¨í”„)

**ì¦ìƒ**:
- Episode ì „ì²´ì—ì„œ turnover â‰ˆ 0
- í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜ê°€ ê· ë“± ë¶„í¬ ìœ ì§€ (1/M)
- ê· ë“± ë°°ë¶„(1/N) ì •ì±… ë°˜ë³µ

**ì§„ë‹¨**:
1. **í™˜ê²½ ë ˆë²¨**: Turnover penaltyê°€ ê±°ë˜ ì–µì œ
   - `lambda_turn`ì´ ì¼ì¼ ìˆ˜ìµë¥ (Â±1%)ê³¼ ë¹„ìŠ·í•œ ìŠ¤ì¼€ì¼
2. **IRT ë ˆë²¨**: Exploration ë©”ì»¤ë‹ˆì¦˜ ì–µì œ
   - Sinkhorn `eps` ë„ˆë¬´ ë‚®ìŒ â†’ deterministic OT
   - Dirichlet concentration ë„ˆë¬´ ë†’ìŒ â†’ deterministic policy

**í•´ê²° ë°©ë²•** (REFACTORING.md ì² í•™: ê¸°ì¡´ ë©”ì»¤ë‹ˆì¦˜ í™œìš©):

| íŒŒë¼ë¯¸í„° | ë³€ê²½ ì „ | ë³€ê²½ í›„ | ê·¼ê±° |
|---------|---------|---------|------|
| `lambda_turn` | 0.1 | 0.01 | ì¼ì¼ ìˆ˜ìµë¥  ìŠ¤ì¼€ì¼ ì •í•© |
| `eps` | 0.05 | 0.10 | Cuturi (2013) ê¶Œì¥ ë²”ìœ„ |
| `eta_1` | 0.10 | 0.15 | ë¹ ë¥¸ ìœ„ê¸° ì ì‘ (ìµœëŒ€ 0.20) |
| Dirichlet `min` | 1.0 | 0.5 | Î±<1 sparse, ë†’ì€ ì—”íŠ¸ë¡œí”¼ |
| Dirichlet `max` | 100 | 50 | Over-confidence ë°©ì§€ |

**ì´ë¡ ì  ê·¼ê±°**:

1. **Sinkhorn Entropy** (REFACTORING.md:151):
   ```math
   min_{PâˆˆU(u,v)} <P,C> + ÎµÂ·KL(P||uv^T)
   ```
   - Îµâ†‘ â†’ ìˆ˜ì†¡ ê³„íš Pê°€ ê· ë“± ë¶„ì‚° â†’ exploration ì¦ê°€

2. **Dirichlet Exploration**:
   - Î±_k < 1: Sparse ì„ í˜¸ (ë†’ì€ ì—”íŠ¸ë¡œí”¼)
   - Î±_k â†’ âˆ: Deterministic (ì—”íŠ¸ë¡œí”¼ 0)
   - min=0.5: ì•ˆì „í•œ exploration ë²”ìœ„

3. **Turnover Penalty ìŠ¤ì¼€ì¼**:
   ```
   ë³€ê²½ ì „: 10% turnover â†’ penalty 0.01 (ìˆ˜ìµë¥  Â±1%ì™€ ë™ì¼ â†’ ê±°ë˜ ì–µì œ)
   ë³€ê²½ í›„: 10% turnover â†’ penalty 0.001 (í•©ë¦¬ì  ìˆ˜ì¤€)
   ```

**ë³µì¡ë„**: 0 (ì„¤ì • íŒŒì¼ 3ì¤„ + ì½”ë“œ 1ì¤„)

#### Recommended Ranges

| íŒŒë¼ë¯¸í„° | ìµœì†Œ | ê¸°ë³¸ | ìµœëŒ€ | ìš©ë„ |
|---------|-----|------|------|------|
| `alpha` | 0.1 | 0.3 | 0.5 | OT-Replicator ê· í˜• |
| `eps` | 0.01 | 0.10 | 0.2 | Exploration (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘) |
| `eta_0` | 0.03 | 0.05 | 0.08 | ê¸°ë³¸ ì ì‘ ì†ë„ |
| `eta_1` | 0.05 | 0.15 | 0.20 | ìœ„ê¸° ê°€ì—´ (ë†’ì„ìˆ˜ë¡ ë¹ ë¦„, ë¶ˆì•ˆì • ì£¼ì˜) |
| `m_tokens` | 4 | 6 | 8 | ìƒíƒœ ì •ë³´ ì±„ë„ ìˆ˜ |
| `M_proto` | 6 | 8 | 12 | ì „ëµ ë‹¤ì–‘ì„± (ë„ˆë¬´ ë§ìœ¼ë©´ ê³¼ì í•©) |
| `lambda_turn` | 0.001 | 0.01 | 0.1 | ê±°ë˜ ë¹„ìš© (ë‚®ì„ìˆ˜ë¡ ììœ ë¡œìš´ ê±°ë˜) |

**Warning**: `eta_1 > 0.20`ì€ ë¶ˆì•ˆì • ê°€ëŠ¥ì„± (REFACTORING.md ê²½ê³ )

## Usage

### Training
```bash
python scripts/train_irt.py --config configs/default_irt.yaml
```

### Evaluation
```bash
# Evaluation automatically generates 12 visualizations
python scripts/evaluate_irt.py --checkpoint logs/*/checkpoints/best_model.pth --config configs/default_irt.yaml

# Or via main.py
python main.py --mode evaluate --resume logs/*/checkpoints/best_model.pth
```

**Output Structure**:
```
logs/YYYYMMDD_HHMMSS/evaluation/
â”œâ”€â”€ evaluation_results.json     # Raw data (metrics, returns, weights, IRT decomposition)
â””â”€â”€ visualizations/             # 12 PNG files (auto-generated)
    â”œâ”€â”€ irt_decomposition.png   # [NEW] OT vs Replicator decomposition
    â”œâ”€â”€ tcell_analysis.png      # [NEW] Crisis types & regimes
    â”œâ”€â”€ cost_matrix.png         # [NEW] Immunological costs
    â”œâ”€â”€ stock_analysis.png      # [NEW] Stock-level attribution
    â”œâ”€â”€ attribution_analysis.png # [NEW] Contribution breakdown
    â”œâ”€â”€ performance_timeline.png # [NEW] Rolling metrics
    â”œâ”€â”€ benchmark_comparison.png # [NEW] vs Equal-weight
    â”œâ”€â”€ risk_dashboard.png      # [NEW] VaR/CVaR/Drawdown
    â”œâ”€â”€ portfolio_weights.png
    â”œâ”€â”€ returns.png
    â”œâ”€â”€ crisis_levels.png
    â””â”€â”€ prototype_weights.png
```

### Legacy Visualization (Deprecated)
```bash
# Old standalone script (now integrated into evaluate_irt.py)
python scripts/visualize_irt.py --results logs/*/evaluation/evaluation_results.json
```

## í‰ê°€ ë° ì‹œê°í™”

### ìë™ ì‹œê°í™” íŒŒì´í”„ë¼ì¸

**v2.0.3 (2025-10-03)** ë¶€í„°, `evaluate_irt.py`ëŠ” IRT ì˜ì‚¬ê²°ì •ì— ëŒ€í•œ ì™„ì „í•œ ì„¤ëª… ê°€ëŠ¥ì„±ì„ ì œê³µí•˜ëŠ” **12ê°œì˜ ì¢…í•© ì‹œê°í™”**ë¥¼ ìë™ ìƒì„±í•œë‹¤.

#### ğŸ”¬ IRT ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„ (3ê°œ í”Œë¡¯)

1. **IRT ë¶„í•´** (`irt_decomposition.png`)
   - ëŒ€í‘œ í”„ë¡œí† íƒ€ì…ì— ëŒ€í•œ **w = (1-Î±)Â·w_rep + Î±Â·w_ot** ë¶„í•´
   - ëª¨ë“  í”„ë¡œí† íƒ€ì…ì˜ L2 norm ë¹„êµ
   - ìœ„ê¸° ë ˆë²¨ ëŒ€ë¹„ ìœ„ê¸° ì ì‘í˜• í•™ìŠµë¥  **Î·(c) = Î·â‚€ + Î·â‚Â·c**
   - **ì‚¬ìš© ì‚¬ë¡€**: IRT í˜¼í•© ê²€ì¦, ìœ„ê¸° ì ì‘ í™•ì¸

2. **T-Cell ë¶„ì„** (`tcell_analysis.png`)
   - Crisis type ë¶„í¬ (í‰ê·  + ìƒìœ„ 3ê°œ ì‹œê³„ì—´)
   - Crisis level vs returns ì‚°ì ë„
   - Crisis type ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
   - ìœ„ê¸° êµ¬ê°„ë³„ ì„±ê³¼ (ë‚®ìŒ/ì¤‘ê°„/ë†’ìŒ)
   - **ì‚¬ìš© ì‚¬ë¡€**: ìœ„ê¸° ê°ì§€ ì´í•´, T-Cell ë™ì‘ ê²€ì¦

3. **ë¹„ìš© í–‰ë ¬** (`cost_matrix.png`)
   - í‰ê·  ë©´ì—­í•™ì  ë¹„ìš© í–‰ë ¬ (Î³, Î», Ï íš¨ê³¼)
   - ë¹„ìš© ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
   - Early vs Late episode ë¹„ìš© ì§„í™”
   - **ì‚¬ìš© ì‚¬ë¡€**: ë©´ì—­í•™ì  í¸í–¥ ê²€ì‚¬, OT ë™ì‘ ë””ë²„ê·¸

#### ğŸ’¼ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ (3ê°œ í”Œë¡¯)

4. **ì¢…ëª© ë¶„ì„** (`stock_analysis.png`)
   - í‰ê·  ê°€ì¤‘ì¹˜ ê¸°ì¤€ ìƒìœ„ 10ê°œ ë³´ìœ  ì¢…ëª© (**ì‹¤ì œ ì‹¬ë³¼**: AAPL, MSFT ë“±)
   - ê°€ì¥ ì—­ë™ì ì¸ ìƒìœ„ 10ê°œ ì¢…ëª© (ê°€ì¤‘ì¹˜ ë³€ë™ì„±ì„ í†µí•œ ìœ„ê¸° ë¯¼ê°ë„)
   - **ì‚¬ìš© ì‚¬ë¡€**: í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±, ìœ„ê¸° ë¯¼ê° ìì‚° ì‹ë³„

5. **ê¸°ì—¬ë„ ë¶„ì„** (`attribution_analysis.png`)
   - ì¢…ëª©ë³„ ìˆ˜ìµ ê¸°ì—¬ë„ ë¶„í•´ (ìƒìœ„ 10ê°œ ëˆ„ì , ìƒìœ„ 3ê°œ ì‹œê³„ì—´)
   - í”„ë¡œí† íƒ€ì… í™œìš©ë„ (í‰ê·  ê°€ì¤‘ì¹˜)
   - í”„ë¡œí† íƒ€ì… ì„±ê³¼ ê¸°ì—¬ë„ (ë†’ì€ í™œì„±í™” ì‹œ ìˆ˜ìµë¥ )
   - **ì‚¬ìš© ì‚¬ë¡€**: ìµœê³  ì„±ê³¼ ì¢…ëª© ì‹ë³„, ì „ëµ íš¨ê³¼ì„±

6. **í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜** (`portfolio_weights.png`)
   - ì‹œê°„ì— ë”°ë¥¸ ì „ì²´ ìì‚° ê°€ì¤‘ì¹˜ ìŠ¤íƒ ì°¨íŠ¸
   - **ì‚¬ìš© ì‚¬ë¡€**: ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ì§„í™”

#### ğŸ“ˆ ì„±ê³¼ & ë¦¬ìŠ¤í¬ (4ê°œ í”Œë¡¯)

7. **ì„±ê³¼ íƒ€ì„ë¼ì¸** (`performance_timeline.png`)
   - Rolling Sharpe Ratio (60ì¼ ìœˆë„ìš°, ëª©í‘œ=1.5)
   - Drawdown íƒ€ì„ë¼ì¸ (ëª©í‘œ=-25%)
   - í¬íŠ¸í´ë¦¬ì˜¤ íšŒì „ìœ¨
   - **ì‚¬ìš© ì‚¬ë¡€**: ì‹œê°„ì— ë”°ë¥¸ ì„±ê³¼ í’ˆì§ˆ ì¶”ì 

8. **ë²¤ì¹˜ë§ˆí¬ ë¹„êµ** (`benchmark_comparison.png`)
   - IRT vs Equal-weight ëˆ„ì  ìˆ˜ìµë¥ 
   - Outperformance/Underperformance ì˜ì—­
   - **ì‚¬ìš© ì‚¬ë¡€**: ì•ŒíŒŒ ìƒì„± ê²€ì¦

9. **ë¦¬ìŠ¤í¬ ëŒ€ì‹œë³´ë“œ** (`risk_dashboard.png`) - 2Ã—2 ê²©ì:
   - ìˆ˜ìµë¥  ë¶„í¬ + VaR(5%), CVaR(5%)
   - Drawdown waterfall (Max DD ê°•ì¡°)
   - Risk-return ì‚°ì ë„ (rolling windows)
   - ìœ„ê¸° vs ë¹„ìœ„ê¸° ìˆ˜ìµë¥  (boxplot)
   - **ì‚¬ìš© ì‚¬ë¡€**: ì¢…í•© ë¦¬ìŠ¤í¬ í‰ê°€

10. **ìˆ˜ìµë¥ ** (`returns.png`)
    - ì¼ì¼ ìˆ˜ìµë¥  ì‹œê³„ì—´
    - ëˆ„ì  ìˆ˜ìµë¥ 
    - **ì‚¬ìš© ì‚¬ë¡€**: ê¸°ë³¸ ì„±ê³¼ í™•ì¸

#### ğŸ§¬ IRT ì»´í¬ë„ŒíŠ¸ (2ê°œ í”Œë¡¯)

11. **ìœ„ê¸° ë ˆë²¨** (`crisis_levels.png`)
    - ì„ê³„ê°’(0.3, 0.7)ì„ í¬í•¨í•œ T-Cell ìœ„ê¸° ë ˆë²¨ ê°ì§€
    - **ì‚¬ìš© ì‚¬ë¡€**: ìœ„ê¸° ê°ì§€ íƒ€ì„ë¼ì¸

12. **í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜** (`prototype_weights.png`)
    - ê°œë³„ í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜ (M=8)
    - í”„ë¡œí† íƒ€ì… ë‹¤ì–‘ì„± ì—”íŠ¸ë¡œí”¼
    - **ì‚¬ìš© ì‚¬ë¡€**: ì „ëµ ë‹¤ì–‘ì„± í™•ì¸

### XAI ì˜¤ë²„í—¤ë“œ

**í•™ìŠµ ì‹œ**: ê±°ì˜ 0ì— ê°€ê¹Œìš´ ì˜¤ë²„í—¤ë“œ (<0.1%)
- Debug infoëŠ” ì´ë¯¸ ê³„ì‚°ëœ ì¤‘ê°„ ê°’ì„ ì¬ì‚¬ìš©
- ì¶”ê°€ forward pass ì—†ìŒ

**í‰ê°€ ì‹œ**: 12ê°œ ì‹œê°í™”ì— ì•½ 5-10ì´ˆ
- í‰ê°€ ì™„ë£Œ í›„ 1íšŒì„± ë¹„ìš©
- í‰ê°€ ì •í™•ë„ì— ì˜í–¥ ì—†ìŒ

### ê³ ê¸‰ XAI (ì„ íƒ ì‚¬í•­)

ë” ê¹Šì€ ë¶„ì„ì„ ì›í•œë‹¤ë©´ `src/evaluation/explainer.py` ì‚¬ìš© (ìˆ˜ë™ í†µí•© í•„ìš”):
- **SHAP values**: íŠ¹ì„± ì¤‘ìš”ë„
- **Integrated Gradients**: Attribution ë¶„ì„
- **LIME**: ì§€ì—­ì  í•´ì„ ê°€ëŠ¥ì„±

## í•´ì„ ê°€ëŠ¥ì„± (Interpretability)

IRTëŠ” ë‹¤ìŒì„ í†µí•´ í’ë¶€í•œ í•´ì„ ê°€ëŠ¥ì„±ì„ ì œê³µí•œë‹¤:

1. **ìˆ˜ì†¡ í–‰ë ¬ P**: ì—í”¼í† í”„-í”„ë¡œí† íƒ€ì… ë§¤í•‘ í‘œì‹œ
2. **í”„ë¡œí† íƒ€ì… ê°€ì¤‘ì¹˜ w**: ì „ëµ í˜¼í•© ê³µê°œ
3. **IRT ë¶„í•´**: OT vs Replicator ê¸°ì—¬ë„ ë¶„ë¦¬
4. **ë¹„ìš© í–‰ë ¬**: ë©´ì—­í•™ì  ì˜ì‚¬ê²°ì • ìš”ì¸ ë¶„í•´
5. **ìœ„ê¸° ë¶„ì„**: êµ¬ê°„ ë¶„í•´ë¥¼ í¬í•¨í•œ ë‹¤ì°¨ì› ìœ„ê¸° ê¸°ì—¬ë„
6. **ì¢…ëª© ê¸°ì—¬ë„**: í¬íŠ¸í´ë¦¬ì˜¤ ë ˆë²¨ ê¸°ì—¬ë„ ë¶„ì„

## ì´ë¡ ì  ë³´ì¥

### ì¦ëª…ëœ ì†ì„±
- Sinkhornì€ O(1/Îµ) ë°˜ë³µì—ì„œ ìˆ˜ë ´ (Cuturi, 2013)
- ReplicatorëŠ” ê³ ì • ì í•©ë„ í•˜ì—ì„œ ESSë¡œ ìˆ˜ë ´ (Hofbauer & Sigmund, 1998)
- ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”ì—ì„œ OT í•´ëŠ” ìœ ì¼í•¨

### ë¯¸í•´ê²° ë¬¸ì œ
- OT+Replicator ê²°í•© ì‹œìŠ¤í…œì˜ ì™„ì „í•œ ìˆ˜ë ´ ì¦ëª…
- ìµœì  Î± ì„ íƒ ì´ë¡ 
- N>100 ìì‚°ìœ¼ë¡œì˜ í™•ì¥ì„±

## í•˜ìœ„ í˜¸í™˜ì„± (Backward Compatibility)

**âš ï¸ í˜¸í™˜ì„± ê¹¨ì§ (BREAKING CHANGE) (v2.0.3, 2025-10-03)**

IRT Operator ì‹œê·¸ë‹ˆì²˜ê°€ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½ë˜ì—ˆë‹¤:
```python
# ì´ì „ (v2.0.2)
def forward(...) -> Tuple[torch.Tensor, torch.Tensor]:
    return w, P

# ì‹ ê·œ (v2.0.3)
def forward(...) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
    return w, P, debug_info
```

**ì˜í–¥**:
- **ì´ì „ ì²´í¬í¬ì¸íŠ¸** (v2.0.2 ì´í•˜)ëŠ” `ValueError: not enough values to unpack` ì˜¤ë¥˜ ë°œìƒ
- **í•´ê²° ë°©ë²•**: ìƒˆ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì¬í•™ìŠµ (ê¶Œì¥) ë˜ëŠ” í•˜ìœ„ í˜¸í™˜ unpacking êµ¬í˜„

**ê¶Œì¥ ì¡°ì¹˜**: ë¬´ê±°ë˜ ë£¨í”„ í•´ê²°ì„ ìœ„í•´ ì—…ë°ì´íŠ¸ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°(`lambda_turn=0.01`, `eps=0.10`, `eta_1=0.15`)ë¡œ ì¬í•™ìŠµ.

**í•˜ìœ„ í˜¸í™˜ ë¡œë”©** (ì¬í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°):
```python
# src/agents/bcell_irt.py:193
irt_output = self.irt(...)
if isinstance(irt_output, tuple) and len(irt_output) == 3:
    w, P, irt_debug = irt_output
else:
    w, P = irt_output
    irt_debug = {'w_rep': None, 'w_ot': None, 'cost_matrix': None, 'eta': None}
```

## ì°¸ê³  ë¬¸í—Œ

1. Cuturi (2013) "Sinkhorn Distances: Lightspeed Computation of Optimal Transport"
2. Hofbauer & Sigmund (1998) "Evolutionary Games and Population Dynamics"
3. Chen et al. (2021) "Randomized Ensembled Double Q-Learning"
4. Kostrikov et al. (2021) "Implicit Q-Learning"

## ë¬¸ì˜

ì§ˆë¬¸ì´ë‚˜ ê¸°ì—¬ëŠ” ì €ì¥ì†Œì— issueë¥¼ ì—´ì–´ì£¼ì„¸ìš”.